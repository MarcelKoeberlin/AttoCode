blob
mark :1
data 10331
import numpy as np
import os
import time
import json
from datetime import datetime
from pynput import keyboard
from pylablib.devices import PrincetonInstruments
import gc


# Global stop flag
stop_loop = False

# PATHS ########################################################################
class Paths:
    SPECTRA_FOLDER = r"C:\Users\Moritz\Desktop\Pixis_data\01_XUV_Spectra"


# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 25
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    NUMBER_OF_IMAGES = int(300e3)

    ACQUISITION_TIME_VIOLATION_THRESHOLD_MS = EXP_TIME_MS*1.8


# MAIN FUNCTION ################################################################
def main():
    global stop_loop

    # Start the keyboard listener
    listener = keyboard.Listener(on_press=on_press)
    listener.start()

    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(f"Timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Save camera attributes BEFORE acquisition
    all_attrs_at_start = cam.get_all_attribute_values()

    # Backup original attributes
    original_roi = cam.get_roi()
    original_trigger_det = cam.get_attribute_value("Trigger Determination")
    original_trigger_resp = cam.get_attribute_value("Trigger Response")
    original_shutter_delay = cam.get_attribute_value("Shutter Closing Delay")
    original_delay_res = cam.get_attribute_value("Shutter Delay Resolution")
    original_shutter_mode = cam.get_attribute_value("Shutter Timing Mode")

    # Setup camera for acquisition
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    print(f"[SET] Exposure Time: {cam.get_attribute_value('Exposure Time')} ms")

    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    print_roi(cam)

    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    print(f"[SET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    print(f"[SET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    cam.set_attribute_value("Clean Until Trigger", False)
    print(f"[SET] Clean Until Trigger: {cam.get_attribute_value('Clean Until Trigger')}")

    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    print(f"[SET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

    cam.set_attribute_value("Shutter Closing Delay", 0)
    print(f"[SET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

    time.sleep(0.2)

    # Save attributes after setup
    all_attrs_measurement = cam.get_all_attribute_values()

    # Set up acquisition
    cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES)

    # Structured dtype for mmap
    dtype = np.dtype([
        ("spectrum", np.uint16, Settings.SPECTRA_SHAPE[1]),
        ("timestamp_us", np.uint64)
    ])
    mmap_path = os.path.join(Paths.SPECTRA_FOLDER, f"{timestamp}_XUV_spectra_combined.npy")
    mmap = np.memmap(mmap_path, dtype=dtype, mode="w+", shape=(Settings.NUMBER_OF_IMAGES,))
    mmap[:] = 0
    mmap.flush()

    # Save metadata
    start_time_unix = time.time()
    metadata = {
        "timestamp": timestamp,
        "exp_time_ms": Settings.EXP_TIME_MS,
        "binning": Settings.BINNING,
        "spectra_shape": Settings.SPECTRA_SHAPE,
        "number_of_images_planned": Settings.NUMBER_OF_IMAGES,
        "start_time_unix": start_time_unix,
        "memmap_file": os.path.basename(mmap_path),
        "dtype": {
            "spectrum": "uint16",
            "timestamp_us": "uint64"
        },
        "camera_attributes_at_start": [list(item) for item in all_attrs_at_start.items()],
        "camera_attributes_measurement": [list(item) for item in all_attrs_measurement.items()]
    }
    metadata_path = os.path.join(Paths.SPECTRA_FOLDER, f"{timestamp}_XUV_"
                                                       f"metadata.json")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=4)

    # Start acquisition
    print("Starting acquisition... (Press 'Esc' to stop early)")
    cam.start_acquisition()

    violations = []
    t_prev = time.time()
    i = 0
    # run main acquisition loop ########################################################
    try:
        while i < Settings.NUMBER_OF_IMAGES:
            data = cam.read_oldest_image()

            if data is None:
                time.sleep(Settings.EXP_TIME_MS / 1000 / 20)
                continue

            t_now = time.time()
            timestamp_us = int(t_now * 1e6)
            # Efficient max value extraction
            max_val = data.max()

            mmap[i] = (data.ravel().astype(np.uint16), timestamp_us)

            if i % 100 == 0:
                mmap.flush()
                print(f"Image {i} flushed.")

            dt = t_now - t_prev

            # print status:
            print(rf"Image no {i} acquired in {dt:.4f} s, max value: {max_val}")

            if dt > Settings.ACQUISITION_TIME_VIOLATION_THRESHOLD_MS / 1000:
                print(f"ACQUISITION TIME VIOLATION at {i}: {dt:.4f}s --------------------------------------")
                violations.append(i)

            t_prev = t_now

            if stop_loop:
                print("User interrupted acquisition with 'Esc'.")
                break

            i += 1

    finally:
        # Final flush
        mmap.flush()
        del mmap
        gc.collect()

        # remove zeros from memmap
        print("Cleaning memmap...")
        load_and_clean_memmap(mmap_path, Settings.SPECTRA_SHAPE[1])

        # Stop acquisition
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()

        # Reset original camera settings
        print("\nResetting camera to original settings:")
        cam.set_roi(*original_roi)
        print_roi(cam)

        cam.set_attribute_value("Trigger Determination", original_trigger_det)
        print(f"[RESET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

        cam.set_attribute_value("Trigger Response", original_trigger_resp)
        print(f"[RESET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

        cam.set_attribute_value("Shutter Closing Delay", original_shutter_delay)
        print(f"[RESET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

        cam.set_attribute_value("Shutter Delay Resolution", original_delay_res)
        print(f"[RESET] Shutter Delay Resolution: {cam.get_attribute_value('Shutter Delay Resolution')}")

        cam.set_attribute_value("Shutter Timing Mode", original_shutter_mode)
        print(f"[RESET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

        cam.close()
        print("Camera connection closed.")

        # Finalize metadata
        metadata["images_acquired"] = i
        metadata["violations"] = violations
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=4)

        print(f"Acquisition complete. Images acquired: {i}, Number of Violations: {len(violations)}")
        print(rf"Violations at: {violations}")


# PRINT ROI HELPER ############################################################
def print_roi(cam) -> None:
    roi = cam.get_roi()
    print("ROI settings:")
    print(f"  Horizontal start:  {roi[0]}")
    print(f"  Horizontal end:    {roi[1]}")
    print(f"  Vertical start:    {roi[2]}")
    print(f"  Vertical end:      {roi[3]}")
    print(f"  Horizontal binning:{roi[4]}")
    print(f"  Vertical binning:  {roi[5]}")


# PYNPUT CALLBACK #############################################################
def on_press(key):
    global stop_loop
    try:
        if key == keyboard.Key.esc:
            print("Detected 'Esc' key, breaking the loop.")
            stop_loop = True
            return False  # Stop the listener
    except AttributeError:
        pass

# CLEAR ZEROS FROM MEMMAP #######################################################
def load_and_clean_memmap(file_path: str, spectrum_length: int) -> None:
    """
    Loads the memmap, removes all-zero rows, and overwrites the original file safely via a temp file.

    :param file_path: Path to the .npy file.
    :param spectrum_length: Length of the spectrum.
    """
    import numpy as np
    import os
    import tempfile
    import gc

    print(f"Loading memmap from: {file_path}")

    dtype = np.dtype([
        ("intensities", np.uint16, spectrum_length),
        ("timestamp_us", np.uint64)
    ])

    # Step 1: Open and filter data
    mmap = np.memmap(file_path, dtype=dtype, mode="r")
    nonzero_mask = ~(
        (mmap["timestamp_us"] == 0) &
        (np.all(mmap["intensities"] == 0, axis=1))
    )
    cleaned_data = mmap[nonzero_mask].copy()  # Load into RAM
    print(f"Original rows: {len(mmap)}, Non-zero rows: {len(cleaned_data)}")

    # Step 2: Fully release original mmap (important on Windows)
    if hasattr(mmap, '_mmap'):
        mmap._mmap.close()
    del mmap
    gc.collect()

    # Step 3: Write to a temporary file
    temp_fd, temp_path = tempfile.mkstemp(dir=os.path.dirname(file_path))
    os.close(temp_fd)

    cleaned_mmap = np.memmap(temp_path, dtype=dtype, mode="w+", shape=(len(cleaned_data),))
    cleaned_mmap[:] = cleaned_data
    cleaned_mmap.flush()

    if hasattr(cleaned_mmap, '_mmap'):
        cleaned_mmap._mmap.close()
    del cleaned_mmap
    gc.collect()

    # Step 4: Atomically replace original file
    os.replace(temp_path, file_path)

    print(f"Cleaned memmap saved to: {file_path}")


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()

blob
mark :2
data 14165
from lib2to3.pgen2.token import NUMBER

from pylablib.devices import PrincetonInstruments
import os
from datetime import datetime
import time
import numpy as np
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

# PATHS: ##############################################################
class Paths:
    BACKUP_ALL_ATTRIBUTES_AT_START = r"C:\Users\Moritz\Desktop\Pixis_data\attribute_backups"
    MEASUREMENT_ATTRIBUTES = r"C:\Users\Moritz\Desktop\Pixis_data\measurement_attributes"
    SPECTRA_FOLDER = r"C:\Users\Moritz\Desktop\Pixis_data\01_XUV_Spectra"

# PROGRAM SETTINGS: ####################################################
class Settings:
    EXP_TIME_MS = 28  # Set exposure time, 28ms is currently the max for 20Hz
    # ROI = (300, 800, 100, 350)  # (x_start, x_end, y_start, y_end) - Example ROI inside 1340x400
    BINNING = (1, 400)  # (x_binning, y_binning) - Bin all rows into 1, keeping full width
    SPECTRA_SHAPE = (1, 1340)  # Shape of the spectra array, CHANGE TOGETHER WITH BINNING

    NUMBER_OF_IMAGES = int(3e4)  # Number of images to acquire
    # NUMBER_OF_IMAGES = int(1e2)  # Number of images to acquire

#########################################################################
# MAIN FUNCTION ########################################################
#########################################################################
def main():
    # generate timestamp:
    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(rf"Current timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    # initialize the camera object:
    cam = PrincetonInstruments.PicamCamera('2105050003')

    # print the current pixel format:
    print(rf"Current pixel format: {cam.get_attribute_value('Pixel Format')}")

    # Print the temperatures:
    print(rf"Sensor Temperature Set Point: {cam.get_attribute_value('Sensor Temperature Set Point')} K")
    print(rf"Sensor Temperature Reading: {cam.get_attribute_value('Sensor Temperature Reading')} K")

    # save the list of all attributes to a txt file at the start: --------------------------------
    path = os.path.join(Paths.BACKUP_ALL_ATTRIBUTES_AT_START, timestamp + "_all_attributes_backup.txt")
    with open(path, "w") as file:
        file.write(str(cam.get_all_attribute_values()).removeprefix("Dictionary(").removesuffix(")"))
    print("\n")

    # Print all attributes: -------------------------------------------------------
    print("List of all camera attributes:")
    print(str(cam.get_all_attribute_values()).removeprefix("Dictionary(").removesuffix(
        ")"))  # prints all camera attributes and their values
    print("\n")

    #########################################################################
    #########################################################################
    # IMAGE ACQUISITION ####################################################
    #########################################################################
    #########################################################################
    print("\n")
    print("IMAGE ACQUISITION: ##############################################")
    print("\n")

    #########################################################################
    # BACKUP ORIGINAL SETTINGS #############################################
    #########################################################################

    # get the original ROI settings:
    ROI_ORIGINAL = cam.get_roi()
    # get the original Trigger determination settings
    TRIGGER_DETERMINATION_ORIGINAL = cam.get_attribute_value("Trigger Determination")
    # get the original Trigger Response settings
    TRIGGER_RESPONSE_ORIGINAL = cam.get_attribute_value("Trigger Response")
    # get the original Shutter Closing Delay settings
    SHUTTER_CLOSING_DELAY_ORIGINAL = cam.get_attribute_value("Shutter Closing Delay")
    # get the original Shutter Delay Resolution settings
    SHUTTER_DELAY_RESOLUTION_ORIGINAL = cam.get_attribute_value("Shutter Delay Resolution")
    # get the original Shutter Timing Mode settings
    SHUTTER_TIMING_MODE_ORIGINAL = cam.get_attribute_value("Shutter Timing Mode")

    #######################################################################
    # Set up the camera for acquisition ###################################
    #######################################################################

    # Set the exposure time -----------------------------------------------
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    # print the set exp time:
    # print(rf"Exposure time set to {cam.cav['Exposure Time']} ms")
    print(rf"Exposure time set to {cam.get_attribute_value('Exposure Time')} ms")  # alternatively

    # Set up the ROI: ####################################################
    print("Current settings:")
    print(cam.get_settings())

    # print the roi limits:
    print(f"ROI limits: {cam.get_roi_limits()}")

    # set the ROI to the desired settings:
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    # set the ROI to the desired settings:
    print("Current ROI settings:")
    print_roi(cam)

    # wait 0.2 seconds:
    time.sleep(0.2)

    # Set up the trigger: ####################################################
    # set the trigger determination to 'Positive Polarity':
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")

    # set the trigger response to 'Readout Per Trigger':
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")

    # print the trigger determination:
    print(f"Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")
    # print the trigger response:
    print(f"Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    # wait 0.2 seconds:
    time.sleep(0.2)

    # Set up the shutter: ################################################

    # shutter_closing_delay = cam.get_attribute("Shutter Closing Delay")

    # set the shutter timing mode:
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    # set shutter closing delay:
    cam.set_attribute_value("Shutter Closing Delay", 0)

    # print the shutter timing mode:
    print(f"Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")
    # print the shutter closing delay:
    print(f"Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

    # wait 0.2 seconds:
    time.sleep(0.2)

    # print the calculated fps:
    print(f"Calculated fps: {cam.get_attribute_value('Frame Rate Calculation')}")

    # Save the measurement attributes: ###################################
    path = os.path.join(Paths.MEASUREMENT_ATTRIBUTES, timestamp + "_measurement_attributes.txt")
    with open(path, "w") as file:
        file.write(str(cam.get_all_attribute_values()).removeprefix("Dictionary(").removesuffix(")"))
    print("\n")

    # wait 0.2 seconds:
    time.sleep(0.2)

    # set up the acquisition settings: ####################################
    # cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES + 1)
    cam.setup_acquisition(mode="sequence", nframes=100)

    # set up the memmap:
    memmap_path = os.path.join(Paths.SPECTRA_FOLDER, timestamp + "_spectra_memmap.dat")
    shape = (Settings.NUMBER_OF_IMAGES, Settings.SPECTRA_SHAPE[0], Settings.SPECTRA_SHAPE[1])
    dtype = np.uint16
    mmap = np.memmap(memmap_path, dtype=dtype, mode="w+", shape=shape)
    # initialize the memmap to 0 and flush to disk
    mmap[:] = 0
    # flush to disk:
    mmap.flush()

    # wait 0.2 seconds:
    time.sleep(0.2)

    #############################################################################
    # start the acquisition: ####################################################
    #############################################################################
    # start the timer:
    start_time = time.time()

    print("\n")
    print("Starting acquisition...")
    print("\n")
    cam.start_acquisition()

    image_count = 0
    image_acquired = True
    first_image = True
    while image_count < Settings.NUMBER_OF_IMAGES:
        # get the oldest image:
        data_full = cam.read_oldest_image()

        if data_full is None:
            if image_acquired:
                print("Waiting for image to be available...")
            # Wait for the image to be available:
            time.sleep(round((Settings.EXP_TIME_MS / 1000) / 4, 3))
            image_acquired = False
            continue

        if first_image:
            start_time = time.time()
            first_image = False
            # print_frames_status(cam)

        # add the image to the memmap:
        mmap[image_count] = data_full

        # flush the memmap every 100 images:
        if image_count % 100 == 0 or image_count == Settings.NUMBER_OF_IMAGES - 1:
            mmap.flush()
            print(f"Flushed memmap at image {image_count} ----------------------------------------------")

        image_count += 1
        image_acquired = True

        # stop the timer:
        end_time = time.time()
        print(rf"{image_count} images acquired in {end_time - start_time:.3f} seconds.")
        print(rf"Frequency: {(image_count - 1)/ (end_time - start_time):.3f} Hz")

    #######################################################################
    # Some final steps ####################################################
    #######################################################################

    # stop the acquisition:
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
        print("Acquisition stopped.")

    # clear the acquisition settings:
    cam.clear_acquisition()

    # set back to the original attributes:
    cam.set_roi(*ROI_ORIGINAL)
    cam.set_attribute_value("Trigger Determination", TRIGGER_DETERMINATION_ORIGINAL)
    cam.set_attribute_value("Trigger Response", TRIGGER_RESPONSE_ORIGINAL)
    cam.set_attribute_value("Shutter Closing Delay", SHUTTER_CLOSING_DELAY_ORIGINAL)
    cam.set_attribute_value("Shutter Delay Resolution", SHUTTER_DELAY_RESOLUTION_ORIGINAL)
    cam.set_attribute_value("Shutter Timing Mode", SHUTTER_TIMING_MODE_ORIGINAL)

    # Close the camera ####################################################
    cam.close()
    print("\nCamera connection closed.")

#########################################################################
#########################################################################
# MISC FUNCTIONS #######################################################
#########################################################################
#########################################################################
def save_image(data, filename):
    """
    Save image as a NumPy .npy file.

    :param data: Image array
    :param filename: Filename to save the image
    """
    path = os.path.join(Paths.SPECTRA_FOLDER, filename)
    np.save(path, data)
    print(f"Saved image to {path}")


def print_roi(cam) -> None:
    """
    Print the current ROI settings of the camera in a fancyier way, with documentation:
    Return tuple (hstart, hend, vstart, vend, hbin, vbin). hstart and hend specify horizontal image extent, vstart and vend specify vertical image extent (start is inclusive, stop is exclusive, starting from 0), hbin and vbin specify binning.
    """
    roi = cam.get_roi()
    print(f"ROI settings:")
    print(f"Horizontal start: {roi[0]}")
    print(f"Horizontal end: {roi[1]}")
    print(f"Vertical start: {roi[2]}")
    print(f"Vertical end: {roi[3]}")
    print(f"Horizontal binning: {roi[4]}")
    print(f"Vertical binning: {roi[5]}")
    print("\n")

def print_array_info(arr):
    """
    Prints information about a NumPy array including:
    - The array contents
    - Shape
    - Number of dimensions
    - Number of elements
    - Data type
    - Bytes per element
    - Total bytes
    - Detailed NumPy info

    Raises:
        TypeError: If `arr` is not a numpy.ndarray.
    """

    print("Array:\n", arr, "\n")
    print("Shape:", arr.shape)
    print("Number of dimensions:", arr.ndim)
    print("Number of elements:", arr.size)
    print("Data type (dtype):", arr.dtype)
    print("Bytes per element (itemsize):", arr.itemsize)
    print("Total bytes (nbytes):", arr.nbytes, "\n")

    # # Check if `arr` is a numpy array
    # if not isinstance(arr, np.ndarray):
    #     raise TypeError("Input must be a NumPy array (np.ndarray).")
    # print("Detailed NumPy info on the array:")
    # np.info(arr)


def print_frames_status(cam) -> None:
    """
    Calls cam.get_frames_status() and prints the acquisition and buffer status
    in a fancy format.

    :param cam: Camera object that has a method get_frames_status().
    """
    console = Console()

    # Get the status from the camera
    status = cam.get_frames_status()

    # Unpacking the tuple
    acquired, unread, skipped, buffer_size = status

    # Title panel
    title = Panel.fit("[bold cyan]Acquisition & Buffer Status[/bold cyan]",
                      border_style="cyan", padding=(1, 2))

    # Table for details
    table = Table(box=None)
    table.add_column("Metric", style="bold yellow", justify="right")
    table.add_column("Value", style="bold white", justify="left")

    table.add_row("[cyan]Acquired Frames[/cyan]", f"{acquired}")
    table.add_row("[cyan]Unread Frames[/cyan]", f"{unread}")
    table.add_row("[cyan]Skipped Frames[/cyan]", f"{skipped}")
    table.add_row("[cyan]Buffer Size[/cyan]", f"{buffer_size}")

    console.print(title)
    console.print(table)


# Run main function #############################################################
if __name__ == "__main__":
    main()

blob
mark :3
data 7587
from pylablib.devices import PrincetonInstruments
import os
from datetime import datetime
import time
import numpy as np

# PATHS: ##############################################################
class Paths:
    SPECTRA_FOLDER = r"C:\Users\Moritz\Desktop\Pixis_data\01_XUV_Spectra"

# PROGRAM SETTINGS: ####################################################
class Settings:
    EXP_TIME_MS = 28  # Set exposure time, 28ms is currently the max for 20Hz
    # ROI = (300, 800, 100, 350)  # (x_start, x_end, y_start, y_end) - Example ROI inside 1340x400
    BINNING = (1, 400)  # (x_binning, y_binning) - Bin all rows into 1, keeping full width
    SPECTRA_SHAPE = (1, 1340)  # Shape of the spectra array, CHANGE TOGETHER WITH BINNING

    NUMBER_OF_IMAGES = int(3e4)  # Number of images to acquire

#########################################################################
# MAIN FUNCTION ########################################################
#########################################################################
def main():
    # generate timestamp:
    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(rf"Current timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    # initialize the camera object:
    cam = PrincetonInstruments.PicamCamera('2105050003')

    # print the current pixel format:
    print(rf"Current pixel format: {cam.get_attribute_value('Pixel Format')}")

    # Print the temperatures:
    print(rf"Sensor Temperature Set Point: {cam.get_attribute_value('Sensor Temperature Set Point')} K")
    print(rf"Sensor Temperature Reading: {cam.get_attribute_value('Sensor Temperature Reading')} K")

    #########################################################################
    #########################################################################
    # IMAGE ACQUISITION ####################################################
    #########################################################################
    #########################################################################
    print("\n")
    print("IMAGE ACQUISITION: ##############################################")
    print("\n")

    #######################################################################
    # Set up the camera for acquisition ###################################
    #######################################################################

    # Set the exposure time -----------------------------------------------
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    # print the set exp time:
    # print(rf"Exposure time set to {cam.cav['Exposure Time']} ms")
    print(rf"Exposure time set to {cam.get_attribute_value('Exposure Time')} ms")  # alternatively

    # Set up the ROI: ####################################################
    print("Current settings:")
    print(cam.get_settings())

    # print the roi limits:
    print(f"ROI limits: {cam.get_roi_limits()}")

    # set the ROI to the desired settings:
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])

    # Set up the trigger: ####################################################
    # set the trigger determination to 'Positive Polarity':
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")

    # set the trigger response to 'Readout Per Trigger':
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")

    # print the trigger determination:
    print(f"Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")
    # print the trigger response:
    print(f"Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    # Set up the shutter: ################################################

    # shutter_closing_delay = cam.get_attribute("Shutter Closing Delay")

    # set the shutter timing mode:
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    # set shutter closing delay:
    cam.set_attribute_value("Shutter Closing Delay", 0)

    # print the shutter timing mode:
    print(f"Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")
    # print the shutter closing delay:
    print(f"Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

    # print the calculated fps:
    print(f"Calculated fps: {cam.get_attribute_value('Frame Rate Calculation')}")

    # set up the acquisition settings: ####################################
    # cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES + 1)
    cam.setup_acquisition(mode="sequence", nframes=100)

    # set up the memmap:
    memmap_path = os.path.join(Paths.SPECTRA_FOLDER, timestamp + "_spectra_memmap.dat")
    shape = (Settings.NUMBER_OF_IMAGES, Settings.SPECTRA_SHAPE[0], Settings.SPECTRA_SHAPE[1])
    dtype = np.uint16
    mmap = np.memmap(memmap_path, dtype=dtype, mode="w+", shape=shape)
    # initialize the memmap to 0 and flush to disk
    mmap[:] = 0
    # flush to disk:
    mmap.flush()

    #############################################################################
    # start the acquisition: ####################################################
    #############################################################################
    # start the timer:
    start_time = time.time()

    print("\n")
    print("Starting acquisition...")
    print("\n")
    cam.start_acquisition()

    image_count = 0
    image_acquired = True
    first_image = True
    while image_count < Settings.NUMBER_OF_IMAGES:
        # get the oldest image:
        data_full = cam.read_oldest_image()

        if data_full is None:
            if image_acquired:
                print("Waiting for image to be available...")
            # Wait for the image to be available:
            time.sleep(round((Settings.EXP_TIME_MS / 1000) / 4, 3))
            image_acquired = False
            continue

        if first_image:
            start_time = time.time()
            first_image = False
            # print_frames_status(cam)

        # add the image to the memmap:
        mmap[image_count] = data_full

        # flush the memmap every 100 images:
        if image_count % 100 == 0 or image_count == Settings.NUMBER_OF_IMAGES - 1:
            mmap.flush()
            print(f"Flushed memmap at image {image_count} ----------------------------------------------")

        image_count += 1
        image_acquired = True

        # stop the timer:
        end_time = time.time()
        print(rf"{image_count} images acquired in {end_time - start_time:.3f} seconds.")
        print(rf"Frequency: {(image_count - 1)/ (end_time - start_time):.3f} Hz")

    #######################################################################
    # Some final steps ####################################################
    #######################################################################

    # stop the acquisition:
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
        print("Acquisition stopped.")

    # clear the acquisition settings:
    cam.clear_acquisition()

    # Close the camera ####################################################
    cam.close()
    print("\nCamera connection closed.")


# Run main function #############################################################
if __name__ == "__main__":
    main()

blob
mark :4
data 1940
import numpy as np
import os
import sys
from tkinter import Tk, filedialog

SPECTRUM_LENGTH = 1340
DEFAULT_PATH = r"C:\Users\Moritz\Desktop\Pixis_data\01_XUV_Spectra"


def select_npy_file() -> str:
    """
    Opens a file dialog to select a `.npy` file using tkinter on Windows.
    :return: Path to the selected .npy file or exits if cancelled.
    """
    root = Tk()
    root.withdraw()  # Hide the main tkinter window

    file_path = filedialog.askopenfilename(
        title="Select a .npy memmap file",
        initialdir=DEFAULT_PATH,
        filetypes=[("NumPy memmap files", "*.npy")]
    )

    if not file_path:
        print("File selection cancelled.")
        sys.exit(1)

    if not file_path.endswith(".npy"):
        print("Selected file is not a .npy file.")
        sys.exit(1)

    return file_path


def load_and_clean_memmap(file_path: str) -> None:
    """
    Loads the memmap, removes all-zero rows, and overwrites the original file.
    :param file_path: Path to the .npy file.
    """
    print(f"Loading memmap from: {file_path}")

    dtype = np.dtype([
        ("intensities", np.uint16, SPECTRUM_LENGTH),
        ("timestamp_us", np.uint64)
    ])

    mmap = np.memmap(file_path, dtype=dtype, mode="r")

    nonzero_mask = ~(
        (mmap["timestamp_us"] == 0) &
        (np.all(mmap["intensities"] == 0, axis=1))
    )

    cleaned_data = mmap[nonzero_mask]
    print(f"Original rows: {len(mmap)}, Non-zero rows: {len(cleaned_data)}")

    del mmap  # Unmap before overwriting
    os.remove(file_path)

    cleaned_mmap = np.memmap(file_path, dtype=dtype, mode="w+", shape=(len(cleaned_data),))
    cleaned_mmap[:] = cleaned_data
    cleaned_mmap.flush()
    print(f"Cleaned memmap saved to: {file_path}")


def main():
    file_path = select_npy_file()
    load_and_clean_memmap(file_path)


if __name__ == "__main__":
    main()

blob
mark :5
data 236

# Notes to code:

- Trigger determination: {1: 'Positive Polarity', 2: 'Negative Polarity'}
- Trigger respone: {'No Response': 1, 'Readout Per Trigger': 2}
- Shutter Timing Mode: {'Always Closed': 2, 'Always Open': 3, 'Normal': 1}
blob
mark :6
data 3593
import tkinter as tk
from tkinter import filedialog
import numpy as np
import matplotlib.pyplot as plt
import sys
import os

def select_dat_file() -> str:
    """
    Opens a file dialog for the user to select a .dat memmap file.

    :return: The selected file path or an empty string if no file was selected.
    """
    root = tk.Tk()
    root.withdraw()  # Hide the root window

    file_path = filedialog.askopenfilename(
        title="Select .dat memmap file",
        initialdir="C:\\Users\\Moritz\\Desktop\\Pixis_data\\01_XUV_Spectra",
        filetypes=[("Memmap Files", "*.dat")]
    )

    return file_path

def load_memmap_dat(filename: str, dtype=np.uint16, entry_shape=(1, 1340)):
    """
    Loads a .dat memmap file and determines its shape dynamically.

    :param filename: Path to the .dat file.
    :param dtype: Data type (default: np.uint16).
    :param entry_shape: The shape of one entry.
    :return: Loaded memmap array.
    """
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File not found: {filename}")

    # Compute number of entries dynamically
    file_size = os.path.getsize(filename)  # File size in bytes
    entry_size = np.prod(entry_shape) * np.dtype(dtype).itemsize  # Bytes per entry

    if file_size % entry_size != 0:
        raise ValueError("File size is not a multiple of entry size. Possible corruption.")

    num_entries = file_size // entry_size  # Compute the number of (1, 1340) entries

    # Load the memmap file
    memmap_array = np.memmap(filename, dtype=dtype, mode='r', shape=(num_entries, *entry_shape))

    print(f"Memmap file loaded with shape: {memmap_array.shape}")
    return memmap_array

def main():
    """
    Main function to select a .dat memmap file, load the entire dataset,
    remove rows that are all zeros, compute min/1%/max values, and display it using imshow.
    """
    file_path = select_dat_file()

    if not file_path:
        print("No file selected. Exiting.")
        sys.exit()

    try:
        # Load memmap
        memmap_array = load_memmap_dat(file_path)

        # Reshape to (num_entries, 1340) for visualization
        data = memmap_array[:, 0, :]  # Shape: (num_entries, 1340)

        # Remove rows that are entirely zeros
        non_zero_mask = ~np.all(data == 0, axis=1)  # True for non-zero rows
        filtered_data = data[non_zero_mask]  # Only keep non-zero rows

        if filtered_data.shape[0] == 0:
            print("All rows are zero! Nothing to display.")
            sys.exit()

        # Compute min, 1% percentile, and max values
        data_min = np.min(filtered_data)
        data_1percent = np.percentile(filtered_data, 1)
        data_max = np.max(filtered_data)

        print(f"Filtered shape: {filtered_data.shape} (after removing zero rows)")
        print(f"Min value: {data_min}")
        print(f"1% percentile value: {data_1percent}")
        print(f"Max value: {data_max}")

        # Flip the data horizontally
        flipped_data = np.fliplr(filtered_data)

        # Display the flipped memmap as an image
        plt.imshow(flipped_data, cmap='viridis', aspect='auto', vmin=data_1percent, vmax=data_max)
        plt.colorbar()
        plt.title(f"Memmap Visualization {flipped_data.shape}")
        plt.xlabel("Width (Pixels)")
        plt.ylabel("Entry Index (filtered)")
        plt.show()

    except Exception as e:
        print(f"Error loading memmap file: {e}")
        sys.exit()

if __name__ == "__main__":
    main()
blob
mark :7
data 89
# Pixis_code
API implementation for hardware triggered acquisition with the Pixis 400B

blob
mark :8
data 1480
import numpy as np
import os
import time

data_dir = r"C:\Users\Moritz\Desktop\Pixis_data\data_tests"
spectrum_length = 2048  # Example length
max_spectra = 10000

# Individual .npy file saving
start_time = time.time()
for i in range(max_spectra):
    intensities = np.random.random(spectrum_length)
    np.save(os.path.join(data_dir, f"intensities_{i}.npy"), intensities)
end_time = time.time()
print(f"Saving 10000 individual .npy files took: {end_time - start_time:.2f} sec")

# Using np.memmap (flushed every iteration)
memmap_path = os.path.join(data_dir, "spectra_memmap.npy")
mmap = np.memmap(memmap_path, dtype=np.float32, mode="w+", shape=(max_spectra, spectrum_length))

start_time = time.time()
for i in range(max_spectra):
    intensities = np.random.random(spectrum_length)
    mmap[i] = intensities
    mmap.flush()  # Flushing after every write
end_time = time.time()
print(f"Using np.memmap (flushed every iteration) took: {end_time - start_time:.2f} sec")

# Using np.memmap (batch flushing every 100 iterations)
batch_size = 100
start_time = time.time()
for i in range(max_spectra):
    intensities = np.random.random(spectrum_length)
    mmap[i] = intensities
    if i % batch_size == 0 or i == 999:
        mmap.flush()  # Flush only every batch
end_time = time.time()
print(f"Using np.memmap (batch flushed every 100 iterations) took: {end_time - start_time:.2f} sec")
print(rf"Time per flush: {(end_time - start_time) / (max_spectra / batch_size):.2f} sec")

blob
mark :9
data 10390
from lib2to3.pgen2.token import NUMBER

from pylablib.devices import PrincetonInstruments
import os
from datetime import datetime
import time
import numpy as np
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

# PATHS: ##############################################################
class Paths:
    BACKUP_ALL_ATTRIBUTES_AT_START = r"C:\Users\Moritz\Desktop\Pixis_data\attribute_backups"
    TEST_IMAGES = r"C:\Users\Moritz\Desktop\Pixis_data\test_images"

# PROGRAM SETTINGS: ####################################################
class Settings:
    EXP_TIME_MS = 20  # Set exposure time
    # ROI = (300, 800, 100, 350)  # (x_start, x_end, y_start, y_end) - Example ROI inside 1340x400
    BINNING = (1, 400)  # (x_binning, y_binning) - Bin all rows into 1, keeping full width

    NUMBER_OF_IMAGES = 50  # Number of images to acquire

#########################################################################
# MAIN FUNCTION ########################################################
#########################################################################
def main():
    # generate timestamp:
    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(rf"Current timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    # initialize the camera object:
    cam = PrincetonInstruments.PicamCamera('2105050003')

    # print the current pixel format:
    print(rf"Current pixel format: {cam.get_attribute_value('Pixel Format')}")

    # Print the temperatures:
    print(rf"Sensor Temperature Set Point: {cam.get_attribute_value('Sensor Temperature Set Point')} K")
    print(rf"Sensor Temperature Reading: {cam.get_attribute_value('Sensor Temperature Reading')} K")

    # Set the exposure time -----------------------------------------------
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    # print the set exp time:
    # print(rf"Exposure time set to {cam.cav['Exposure Time']} ms")
    print(rf"Exposure time set to {cam.get_attribute_value('Exposure Time')} ms")  # alternatively

    # save the list of all attributes to a txt file at the start: --------------------------------
    path = os.path.join(Paths.BACKUP_ALL_ATTRIBUTES_AT_START, timestamp + "_all_attributes_backup.txt")
    with open(path, "w") as file:
        file.write(str(cam.get_all_attribute_values()).removeprefix("Dictionary(").removesuffix(")"))
    print("\n")

    # Print all attributes: -------------------------------------------------------
    print("List of all camera attributes:")
    print(str(cam.get_all_attribute_values()).removeprefix("Dictionary(").removesuffix(
        ")"))  # prints all camera attributes and their values
    print("\n")

    #########################################################################
    #########################################################################
    # IMAGE ACQUISITION ####################################################
    #########################################################################
    #########################################################################
    print("\n")
    print("IMAGE ACQUISITION: ##############################################")
    print("\n")

    #########################################################################
    # BACKUP ORIGINAL SETTINGS #############################################
    #########################################################################

    # get the original ROI settings:
    ROI_ORIGINAL = cam.get_roi()
    # # get the oroginal trigger mode:
    # TRIGGER_MODE_ORIGINAL = cam.get_attribute_value("Trigger Source")

    #######################################################################
    # 1. Acquire single full frame (1340 x 400)
    #######################################################################

    # print the current ROI settings:
    print_roi(cam)

    print(cam.get_settings())

    # print the roi limits:
    print(f"ROI limits: {cam.get_roi_limits()}")

    # set the ROI to the desired settings:
    cam.set_roi(hbin=1, vbin=400)
    # set the ROI to the desired settings:
    print_roi(cam)

    # print max calculated fps:
    print(f"Max calculated fps: {cam.get_attribute_value('Frame Rate Calculation')}")

    # wait 0.5 seconds:
    time.sleep(0.2)

    # set up the acquisition settings:
    # cam.setup_acquisition(mode="snap", nframes=Settings.NUMBER_OF_IMAGES + 1)
    cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES + 1)

    # start the timer:
    start_time = time.time()

    print("\n")
    # start the acquisition: ####################################################
    cam.start_acquisition()


    # print(cam.acquisition_in_progress())


    # print the status of the frames:
    # print_frames_status(cam)


    # data_full = cam.snap()  # single image
    # data_full = cam.grab()  # single image
    # check if new image is available:

    # data_full = cam.read_newest_image()
    # save_image(data_full, f"full_frame_{timestamp}.npy")
    # cam.snap()

    # # Acquire the images in a loop:
    # for i in range(Settings.NUMBER_OF_IMAGES + 100):
    #     print(rf"Unread frames: {cam.get_frames_status()[1]}")
    #     data_full = cam.read_newest_image()
    #     save_image(data_full, rf"{timestamp}_full_image_{i}.npy")
    #     print_frames_status(cam)

    image_count = 0
    while image_count < Settings.NUMBER_OF_IMAGES:
        # get the status of the frames:
        acquired_imgs, unread_imgs, skipped_imgs, buffer_size = cam.get_frames_status()
        # wait till images are available:
        while unread_imgs == 0:
            time.sleep(0.001) # wait for the image to be available
            acquired_imgs, unread_imgs, skipped_imgs, buffer_size = cam.get_frames_status()

        print(f"Acquired: {acquired_imgs}, Unread: {unread_imgs}, Skipped: {skipped_imgs}, Buffer size: {buffer_size}")

        data_full = cam.read_oldest_image()
        save_image(data_full, rf"{timestamp}_full_image_{image_count}.npy")
        image_count += 1

        # stop the timer:
        end_time = time.time()
        print(rf"{image_count} images acquired in {end_time - start_time} seconds.")


    # stop the acquisition:
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
        print("Acquisition stopped.")

    # clear the acquisition settings:
    cam.clear_acquisition()

    # set the ROI back to the original settings:
    cam.set_roi(*ROI_ORIGINAL)

    # Close the camera ####################################################
    cam.close()
    print("\nCamera connection closed.")

#########################################################################
#########################################################################
# MISC FUNCTIONS #######################################################
#########################################################################
#########################################################################
def save_image(data, filename):
    """
    Save image as a NumPy .npy file.

    :param data: Image array
    :param filename: Filename to save the image
    """
    path = os.path.join(Paths.TEST_IMAGES, filename)
    np.save(path, data)
    print(f"Saved image to {path}")


def print_roi(cam) -> None:
    """
    Print the current ROI settings of the camera in a fancyier way, with documentation:
    Return tuple (hstart, hend, vstart, vend, hbin, vbin). hstart and hend specify horizontal image extent, vstart and vend specify vertical image extent (start is inclusive, stop is exclusive, starting from 0), hbin and vbin specify binning.
    """
    roi = cam.get_roi()
    print(f"ROI settings:")
    print(f"Horizontal start: {roi[0]}")
    print(f"Horizontal end: {roi[1]}")
    print(f"Vertical start: {roi[2]}")
    print(f"Vertical end: {roi[3]}")
    print(f"Horizontal binning: {roi[4]}")
    print(f"Vertical binning: {roi[5]}")
    print("\n")

def print_array_info(arr):
    """
    Prints information about a NumPy array including:
    - The array contents
    - Shape
    - Number of dimensions
    - Number of elements
    - Data type
    - Bytes per element
    - Total bytes
    - Detailed NumPy info

    Raises:
        TypeError: If `arr` is not a numpy.ndarray.
    """

    print("Array:\n", arr, "\n")
    print("Shape:", arr.shape)
    print("Number of dimensions:", arr.ndim)
    print("Number of elements:", arr.size)
    print("Data type (dtype):", arr.dtype)
    print("Bytes per element (itemsize):", arr.itemsize)
    print("Total bytes (nbytes):", arr.nbytes, "\n")

    # # Check if `arr` is a numpy array
    # if not isinstance(arr, np.ndarray):
    #     raise TypeError("Input must be a NumPy array (np.ndarray).")
    # print("Detailed NumPy info on the array:")
    # np.info(arr)


def print_frames_status(cam) -> None:
    """
    Calls cam.get_frames_status() and prints the acquisition and buffer status
    in a fancy format.

    :param cam: Camera object that has a method get_frames_status().
    """
    console = Console()

    # Get the status from the camera
    status = cam.get_frames_status()

    # Unpacking the tuple
    acquired, unread, skipped, buffer_size = status

    # Title panel
    title = Panel.fit("[bold cyan]Acquisition & Buffer Status[/bold cyan]",
                      border_style="cyan", padding=(1, 2))

    # Table for details
    table = Table(box=None)
    table.add_column("Metric", style="bold yellow", justify="right")
    table.add_column("Value", style="bold white", justify="left")

    table.add_row("[cyan]Acquired Frames[/cyan]", f"{acquired}")
    table.add_row("[cyan]Unread Frames[/cyan]", f"{unread}")
    table.add_row("[cyan]Skipped Frames[/cyan]", f"{skipped}")
    table.add_row("[cyan]Buffer Size[/cyan]", f"{buffer_size}")

    console.print(title)
    console.print(table)


# Run main function #############################################################
if __name__ == "__main__":
    main()

blob
mark :10
data 5766
# A TRIGGER HAS TO RUN FOR THIS SCRIPT TO WORK!!!!
import numpy as np
import time
import matplotlib.pyplot as plt
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 1
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    REFRESH_TIME_S = 0.01
    ENERGY_FILE = r"C:\Users\Moritz\Desktop\Pixis_data\Spec.txt"


# MAIN FUNCTION ################################################################
def main():
    # Load energy axis from file
    try:
        with open(Settings.ENERGY_FILE, "r") as f:
            energy_eV = np.loadtxt(f)
    except Exception as e:
        print(f"Failed to load energy axis from file: {e}")
        return

    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]})")
        return

    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Setup camera
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)

    time.sleep(0.2)

    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()

    print("Starting live display... (Press Ctrl+C to stop)")

    # Setup matplotlib
    plt.ion()
    fig, ax = plt.subplots()

    # Create "Keep" button
    button_ax = fig.add_axes([0.01, 0.9, 0.08, 0.06])  # [left, bottom, width, height]
    keep_button = Button(button_ax, "Keep", color='lightgray', hovercolor='lightgreen')

    # Initial ylim max
    ylim_max = 60000

    def on_key(event):
        nonlocal ylim_max
        if event.key == 'up':
            ylim_max *= 1.2  # Increase upper limit
        elif event.key == 'down':
            ylim_max /= 1.2  # Decrease upper limit
        ylim_max = max(1000, ylim_max)  # Avoid too low values
        ax.set_ylim(0, ylim_max)
        fig.canvas.draw()
        print(f"Updated ylim_max: {ylim_max:.0f}")

    def on_keep_clicked(event):
        if spectrum is not None:
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            fig.canvas.draw()
            print("Spectrum kept.")

    keep_button.on_clicked(on_keep_clicked)

    fig.canvas.mpl_connect('key_press_event', on_key)

    y = np.zeros_like(energy_eV)
    line, = ax.plot(energy_eV, y, zorder=2)

    ref_line, = ax.plot(energy_eV, np.zeros_like(y), color='green', linestyle='--', linewidth=1.5, label="Kept",
                        zorder=1)
    ref_line.set_visible(False)  # Only show when spectrum is saved

    # Buffer to store max values
    max_vals_buffer = deque([0] * 200, maxlen=200)

    # Create a small inset axes for max value trace (top-left corner)
    ax2 = fig.add_axes([0.69, 0.7, 0.3, 0.25])  # [left, bottom, width, height] in figure coords
    ax2.set_title("Max Trace", fontsize=9)
    ax2.set_ylabel("Max", fontsize=8)
    ax2.set_xticks([])
    # add a grid
    ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax2.tick_params(labelsize=8)
    max_line, = ax2.plot(list(max_vals_buffer), color='red')
    ax2.set_ylim(0, 65535)  # Adjust based on expected range of max values

    ax.set_title("Use up/down arrows to change limits", loc='left')
    ax.set_xlabel("Energy (eV)")
    ax.set_ylabel("Counts")
    ax.grid(True)
    fig.canvas.draw()
    fig.canvas.flush_events()

    try:
        spectrum = None
        while True:
            data = cam.read_newest_image()

            if data is None:
                time.sleep(Settings.EXP_TIME_MS / 1000 / 10)
                continue

            spectrum = data.ravel().astype(np.uint16)

            # Update max buffer and trace
            current_max = np.max(spectrum)
            max_vals_buffer.append(current_max)

            # Update max trace plot
            max_line.set_ydata(list(max_vals_buffer))
            max_line.set_xdata(np.arange(len(max_vals_buffer)))  # X-axis: just index
            ax2.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))  # Dynamic Y-limits
            ax2.set_xlim(0, len(max_vals_buffer))

            if spectrum.shape[0] != energy_eV.shape[0]:
                print(f"Unexpected spectrum shape: {spectrum.shape}")
                continue

            # Update plot
            try:
                line.set_ydata(spectrum)
                ax.set_ylim(0, ylim_max)
                fig.canvas.draw()
                fig.canvas.flush_events()
                time.sleep(Settings.REFRESH_TIME_S)
            except Exception as e:
                print(f"Plot update error: {e}")
                continue

    except KeyboardInterrupt:
        print("User stopped with Ctrl+C.")

    finally:
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()
        cam.close()
        print("Camera disconnected.")
        plt.ioff()
        plt.show()


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()

blob
mark :11
data 281
# TODO:

- [x] check shutter times
    - 'Shutter Closing Delay': 8.0
    - 'Shutter Delay Resolution': 1000.0
- [x] create Git
- [x] implement memmap
    - [x] run memmap test script: Windows = Terrible
- Implement save measurement settings!!!
    - ask support via email
blob
mark :12
data 4670
import json
import numpy as np
import os
from typing import Dict
import matplotlib.pyplot as plt
import subprocess
import platform
import tkinter as tk
from tkinter import filedialog

# Default data path
DEFAULT_PATH = r"C:\Users\Moritz\Desktop\Pixis_data\01_XUV_Spectra"


def main():
    # GUI file picker
    metadata_file = select_metadata_file()
    if not metadata_file:
        print("No file selected. Exiting.")
        return

    # Load spectra and metadata
    spectra = get_princeton_spectra(metadata_file)
    timestamps = get_timestamps(metadata_file)
    metadata = get_metadata(metadata_file)

    # Remove rows with only zeros
    mask = ~np.all(spectra == 0, axis=1)
    filtered_spectra = spectra[mask]
    filtered_timestamps = timestamps[mask]
    skipped_rows = spectra.shape[0] - filtered_spectra.shape[0]
    print(f"Skipped {skipped_rows} rows containing only zeros.")

    # Remove the first acquired spectrum (lowest timestamp) from spectra only
    index_of_earliest = np.argmin(filtered_timestamps)
    filtered_spectra = np.delete(filtered_spectra, index_of_earliest, axis=0)
    print(f"Removed spectrum at index {index_of_earliest} (earliest timestamp).")

    # Print timestamps and violations
    print(f"\nTimestamps (us) [full length]: {timestamps}")
    print("\nViolations at indices:", metadata.get("violations", []))

    # Plot spectra as image
    plt.figure(figsize=(10, 6))
    im = plt.imshow(
        filtered_spectra,
        aspect="auto",
        cmap="viridis",
        extent=(0, filtered_spectra.shape[1], filtered_spectra.shape[0], 0)
    )
    plt.colorbar(im, label="Intensity (a.u.)")
    plt.xlabel("Pixel Index")
    plt.ylabel("Frame Index")
    plt.title("XUV Spectra Matrix (excluding first acquired frame)")
    plt.tight_layout()
    plt.show()


# FUNCTIONS TO LOAD DATA AND METADATA #########################################
def get_metadata(json_path: str) -> Dict:
    """
    Load metadata from a JSON file.

    :param json_path: Path to the metadata JSON file.
    :return: Metadata dictionary.
    """
    with open(json_path, "r") as f:
        metadata = json.load(f)
    return metadata


def get_timestamps(json_path: str) -> np.ndarray:
    """
    Extract timestamps from the memory-mapped file.

    :param json_path: Path to the metadata JSON file.
    :return: Array of timestamps in microseconds.
    """
    metadata = get_metadata(json_path)
    memmap_path = os.path.join(os.path.dirname(json_path), metadata["memmap_file"])

    dtype = np.dtype([
        ("spectrum", np.uint16, metadata["spectra_shape"][1]),
        ("timestamp_us", np.uint64)
    ])
    mmap = np.memmap(memmap_path, dtype=dtype, mode="r")
    return mmap["timestamp_us"]


def get_princeton_spectra(json_path: str) -> np.ndarray:
    """
    Extract the Princeton spectra as a 2D NumPy array from the structured memmap.

    :param json_path: Path to the metadata JSON file.
    :return: 2D NumPy array with shape (N_frames, N_pixels).
    """
    metadata = get_metadata(json_path)
    memmap_path = os.path.join(os.path.dirname(json_path), metadata["memmap_file"])

    dtype = np.dtype([
        ("spectrum", np.uint16, metadata["spectra_shape"][1]),
        ("timestamp_us", np.uint64)
    ])
    mmap = np.memmap(memmap_path, dtype=dtype, mode="r")
    return mmap["spectrum"]


def select_metadata_file() -> str:
    """
    Cross-platform file selection:
    - Uses Zenity on Linux (if available)
    - Falls back to Tkinter on Windows or if Zenity is unavailable

    :return: Path to the selected file, or empty string if cancelled.
    """
    system = platform.system()

    if system == "Linux":
        try:
            result = subprocess.run(
                [
                    'zenity',
                    '--file-selection',
                    '--title=Select Metadata JSON File',
                    '--file-filter=*.json'
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            if result.returncode == 0:
                return result.stdout.strip()
            else:
                return ""
        except FileNotFoundError:
            print("Zenity not found. Falling back to Tkinter.")

    # Tkinter fallback
    root = tk.Tk()
    root.withdraw()
    path = filedialog.askopenfilename(
        title="Select Metadata JSON File",
        filetypes=[("JSON files", "*.json")]
    )
    return path


if __name__ == "__main__":
    main()

blob
mark :13
data 11266
from lib2to3.pgen2.token import NUMBER

from pylablib.devices import PrincetonInstruments
import os
from datetime import datetime
import time
import numpy as np
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

# PATHS: ##############################################################
class Paths:
    BACKUP_ALL_ATTRIBUTES_AT_START = r"C:\Users\Moritz\Desktop\Pixis_data\attribute_backups"
    TEST_IMAGES = r"C:\Users\Moritz\Desktop\Pixis_data\test_images"

# PROGRAM SETTINGS: ####################################################
class Settings:
    EXP_TIME_MS = 20  # Set exposure time
    # ROI = (300, 800, 100, 350)  # (x_start, x_end, y_start, y_end) - Example ROI inside 1340x400
    BINNING = (1, 400)  # (x_binning, y_binning) - Bin all rows into 1, keeping full width

    NUMBER_OF_IMAGES = int(1e5)  # Number of images to acquire

#########################################################################
# MAIN FUNCTION ########################################################
#########################################################################
def main():
    # generate timestamp:
    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(rf"Current timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    # initialize the camera object:
    cam = PrincetonInstruments.PicamCamera('2105050003')

    # print the current pixel format:
    print(rf"Current pixel format: {cam.get_attribute_value('Pixel Format')}")

    # Print the temperatures:
    print(rf"Sensor Temperature Set Point: {cam.get_attribute_value('Sensor Temperature Set Point')} K")
    print(rf"Sensor Temperature Reading: {cam.get_attribute_value('Sensor Temperature Reading')} K")

    # Set the exposure time -----------------------------------------------
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    # print the set exp time:
    # print(rf"Exposure time set to {cam.cav['Exposure Time']} ms")
    print(rf"Exposure time set to {cam.get_attribute_value('Exposure Time')} ms")  # alternatively

    # save the list of all attributes to a txt file at the start: --------------------------------
    path = os.path.join(Paths.BACKUP_ALL_ATTRIBUTES_AT_START, timestamp + "_all_attributes_backup.txt")
    with open(path, "w") as file:
        file.write(str(cam.get_all_attribute_values()).removeprefix("Dictionary(").removesuffix(")"))
    print("\n")

    # # Print all attributes: -------------------------------------------------------
    # print("List of all camera attributes:")
    # print(str(cam.get_all_attribute_values()).removeprefix("Dictionary(").removesuffix(
    #     ")"))  # prints all camera attributes and their values
    # print("\n")

    #########################################################################
    #########################################################################
    # IMAGE ACQUISITION ####################################################
    #########################################################################
    #########################################################################
    print("\n")
    print("IMAGE ACQUISITION: ##############################################")
    print("\n")

    #########################################################################
    # BACKUP ORIGINAL SETTINGS #############################################
    #########################################################################

    # get the original ROI settings:
    ROI_ORIGINAL = cam.get_roi()
    # get the original Trigger determination settings
    TRIGGER_DETERMINATION_ORIGINAL = cam.get_attribute_value("Trigger Determination")
    # get the original Trigger Response settings
    TRIGGER_RESPONSE_ORIGINAL = cam.get_attribute_value("Trigger Response")

    #######################################################################
    # Set up the camera for acquisition ###################################
    #######################################################################

    # Set up the ROI: ####################################################
    print("Current settings:")
    print(cam.get_settings())

    # print the roi limits:
    print(f"ROI limits: {cam.get_roi_limits()}")

    # set the ROI to the desired settings:
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    # set the ROI to the desired settings:
    print("Current ROI settings:")
    print_roi(cam)

    # print max calculated fps:
    print(f"Max calculated fps: {cam.get_attribute_value('Frame Rate Calculation')}")

    # wait 0.2 seconds:
    time.sleep(0.2)

    # Set up the trigger: ####################################################
    # set the trigger determination to 'Positive Polarity':
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")

    # set the trigger response to 'Readout Per Trigger':
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")

    # print the trigger determination:
    print(f"Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")
    # print the trigger response:
    print(f"Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    # wait 0.2 seconds:
    time.sleep(0.2)

    # set up the acquisition settings: ####################################
    # cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES + 1)
    cam.setup_acquisition(mode="sequence", nframes=100)

    # start the acquisition: ####################################################
    # start the timer:
    start_time = time.time()

    print("\n")
    print("Starting acquisition...")
    print("\n")
    cam.start_acquisition()

    image_count = 0
    image_acquired = True
    first_image = True
    while image_count < Settings.NUMBER_OF_IMAGES:
        # get the oldest image:
        data_full = cam.read_oldest_image()

        if data_full is None:
            if image_acquired:
                print("Waiting for image to be available...")
            # Wait for the image to be available:
            time.sleep(round((Settings.EXP_TIME_MS / 1000) / 4, 3))
            image_acquired = False
            continue

        if first_image:
            start_time = time.time()
            first_image = False
            # print_frames_status(cam)

        save_image(data_full, rf"{timestamp}_full_image_{image_count}.npy")
        image_count += 1
        image_acquired = True

        # stop the timer:
        end_time = time.time()
        print(rf"{image_count} images acquired in {end_time - start_time:.3f} seconds.")
        print(rf"Frequency: {(image_count - 1)/ (end_time - start_time):.3f} Hz")

    #######################################################################
    # Some final steps ####################################################
    #######################################################################

    # stop the acquisition:
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
        print("Acquisition stopped.")

    # clear the acquisition settings:
    cam.clear_acquisition()

    # set back to the original attributes:
    cam.set_roi(*ROI_ORIGINAL)
    cam.set_attribute_value("Trigger Determination", TRIGGER_DETERMINATION_ORIGINAL)
    cam.set_attribute_value("Trigger Response", TRIGGER_RESPONSE_ORIGINAL)

    # Close the camera ####################################################
    cam.close()
    print("\nCamera connection closed.")

#########################################################################
#########################################################################
# MISC FUNCTIONS #######################################################
#########################################################################
#########################################################################
def save_image(data, filename):
    """
    Save image as a NumPy .npy file.

    :param data: Image array
    :param filename: Filename to save the image
    """
    path = os.path.join(Paths.TEST_IMAGES, filename)
    np.save(path, data)
    print(f"Saved image to {path}")


def print_roi(cam) -> None:
    """
    Print the current ROI settings of the camera in a fancyier way, with documentation:
    Return tuple (hstart, hend, vstart, vend, hbin, vbin). hstart and hend specify horizontal image extent, vstart and vend specify vertical image extent (start is inclusive, stop is exclusive, starting from 0), hbin and vbin specify binning.
    """
    roi = cam.get_roi()
    print(f"ROI settings:")
    print(f"Horizontal start: {roi[0]}")
    print(f"Horizontal end: {roi[1]}")
    print(f"Vertical start: {roi[2]}")
    print(f"Vertical end: {roi[3]}")
    print(f"Horizontal binning: {roi[4]}")
    print(f"Vertical binning: {roi[5]}")
    print("\n")

def print_array_info(arr):
    """
    Prints information about a NumPy array including:
    - The array contents
    - Shape
    - Number of dimensions
    - Number of elements
    - Data type
    - Bytes per element
    - Total bytes
    - Detailed NumPy info

    Raises:
        TypeError: If `arr` is not a numpy.ndarray.
    """

    print("Array:\n", arr, "\n")
    print("Shape:", arr.shape)
    print("Number of dimensions:", arr.ndim)
    print("Number of elements:", arr.size)
    print("Data type (dtype):", arr.dtype)
    print("Bytes per element (itemsize):", arr.itemsize)
    print("Total bytes (nbytes):", arr.nbytes, "\n")

    # # Check if `arr` is a numpy array
    # if not isinstance(arr, np.ndarray):
    #     raise TypeError("Input must be a NumPy array (np.ndarray).")
    # print("Detailed NumPy info on the array:")
    # np.info(arr)


def print_frames_status(cam) -> None:
    """
    Calls cam.get_frames_status() and prints the acquisition and buffer status
    in a fancy format.

    :param cam: Camera object that has a method get_frames_status().
    """
    console = Console()

    # Get the status from the camera
    status = cam.get_frames_status()

    # Unpacking the tuple
    acquired, unread, skipped, buffer_size = status

    # Title panel
    title = Panel.fit("[bold cyan]Acquisition & Buffer Status[/bold cyan]",
                      border_style="cyan", padding=(1, 2))

    # Table for details
    table = Table(box=None)
    table.add_column("Metric", style="bold yellow", justify="right")
    table.add_column("Value", style="bold white", justify="left")

    table.add_row("[cyan]Acquired Frames[/cyan]", f"{acquired}")
    table.add_row("[cyan]Unread Frames[/cyan]", f"{unread}")
    table.add_row("[cyan]Skipped Frames[/cyan]", f"{skipped}")
    table.add_row("[cyan]Buffer Size[/cyan]", f"{buffer_size}")

    console.print(title)
    console.print(table)


# Run main function #############################################################
if __name__ == "__main__":
    main()

blob
mark :14
data 5731
# A TRIGGER HAS TO RUN FOR THIS SCRIPT TO WORK!!!!
import numpy as np
import time
import matplotlib.pyplot as plt
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 1
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    REFRESH_TIME_S = 0.01
    ENERGY_FILE = r"Spec.txt"


# MAIN FUNCTION ################################################################
def main():
    # Load energy axis from file
    try:
        with open(Settings.ENERGY_FILE, "r") as f:
            energy_eV = np.loadtxt(f)
    except Exception as e:
        print(f"Failed to load energy axis from file: {e}")
        return

    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]})")
        return

    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Setup camera
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)

    time.sleep(0.2)

    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()

    print("Starting live display... (Press Ctrl+C to stop)")

    # Setup matplotlib
    plt.ion()
    fig, ax = plt.subplots()

    # Create "Keep" button
    button_ax = fig.add_axes([0.01, 0.9, 0.08, 0.06])  # [left, bottom, width, height]
    keep_button = Button(button_ax, "Keep", color='lightgray', hovercolor='lightgreen')

    # Initial ylim max
    ylim_max = 60000

    def on_key(event):
        nonlocal ylim_max
        if event.key == 'up':
            ylim_max *= 1.2  # Increase upper limit
        elif event.key == 'down':
            ylim_max /= 1.2  # Decrease upper limit
        ylim_max = max(1000, ylim_max)  # Avoid too low values
        ax.set_ylim(0, ylim_max)
        fig.canvas.draw()
        print(f"Updated ylim_max: {ylim_max:.0f}")

    def on_keep_clicked(event):
        if spectrum is not None:
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            fig.canvas.draw()
            print("Spectrum kept.")

    keep_button.on_clicked(on_keep_clicked)

    fig.canvas.mpl_connect('key_press_event', on_key)

    y = np.zeros_like(energy_eV)
    line, = ax.plot(energy_eV, y, zorder=2)

    ref_line, = ax.plot(energy_eV, np.zeros_like(y), color='green', linestyle='--', linewidth=1.5, label="Kept",
                        zorder=1)
    ref_line.set_visible(False)  # Only show when spectrum is saved

    # Buffer to store max values
    max_vals_buffer = deque([0] * 200, maxlen=200)

    # Create a small inset axes for max value trace (top-left corner)
    ax2 = fig.add_axes([0.69, 0.7, 0.3, 0.25])  # [left, bottom, width, height] in figure coords
    ax2.set_title("Max Trace", fontsize=9)
    ax2.set_ylabel("Max", fontsize=8)
    ax2.set_xticks([])
    # add a grid
    ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax2.tick_params(labelsize=8)
    max_line, = ax2.plot(list(max_vals_buffer), color='red')
    ax2.set_ylim(0, 65535)  # Adjust based on expected range of max values

    ax.set_title("Use up/down arrows to change limits", loc='left')
    ax.set_xlabel("Energy (eV)")
    ax.set_ylabel("Counts")
    ax.grid(True)
    fig.canvas.draw()
    fig.canvas.flush_events()

    try:
        spectrum = None
        while True:
            data = cam.read_newest_image()

            if data is None:
                time.sleep(Settings.EXP_TIME_MS / 1000 / 10)
                continue

            spectrum = data.ravel().astype(np.uint16)

            # Update max buffer and trace
            current_max = np.max(spectrum)
            max_vals_buffer.append(current_max)

            # Update max trace plot
            max_line.set_ydata(list(max_vals_buffer))
            max_line.set_xdata(np.arange(len(max_vals_buffer)))  # X-axis: just index
            ax2.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))  # Dynamic Y-limits
            ax2.set_xlim(0, len(max_vals_buffer))

            if spectrum.shape[0] != energy_eV.shape[0]:
                print(f"Unexpected spectrum shape: {spectrum.shape}")
                continue

            # Update plot
            try:
                line.set_ydata(spectrum)
                ax.set_ylim(0, ylim_max)
                fig.canvas.draw()
                fig.canvas.flush_events()
                time.sleep(Settings.REFRESH_TIME_S)
            except Exception as e:
                print(f"Plot update error: {e}")
                continue

    except KeyboardInterrupt:
        print("User stopped with Ctrl+C.")

    finally:
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()
        cam.close()
        print("Camera disconnected.")
        plt.ioff()
        plt.show()


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()

blob
mark :15
data 9238
15.539	15.549	15.558	15.568	15.578	15.588	15.597	15.607	15.617	15.627	15.637	15.646	15.656	15.666	15.676	15.686	15.696	15.706	15.716	15.726	15.736	15.746	15.756	15.766	15.776	15.786	15.796	15.806	15.816	15.826	15.836	15.846	15.856	15.866	15.876	15.887	15.897	15.907	15.917	15.927	15.938	15.948	15.958	15.968	15.979	15.989	15.999	16.009	16.02	16.03	16.04	16.051	16.061	16.072	16.082	16.092	16.103	16.113	16.124	16.134	16.145	16.155	16.166	16.176	16.187	16.197	16.208	16.218	16.229	16.24	16.25	16.261	16.272	16.282	16.293	16.304	16.314	16.325	16.336	16.347	16.357	16.368	16.379	16.39	16.401	16.411	16.422	16.433	16.444	16.455	16.466	16.477	16.488	16.499	16.51	16.521	16.531	16.543	16.554	16.565	16.576	16.587	16.598	16.609	16.62	16.631	16.642	16.653	16.665	16.676	16.687	16.698	16.709	16.721	16.732	16.743	16.754	16.766	16.777	16.788	16.8	16.811	16.823	16.834	16.845	16.857	16.868	16.88	16.891	16.903	16.914	16.926	16.937	16.949	16.96	16.972	16.984	16.995	17.007	17.018	17.03	17.042	17.054	17.065	17.077	17.089	17.1	17.112	17.124	17.136	17.148	17.16	17.171	17.183	17.195	17.207	17.219	17.231	17.243	17.255	17.267	17.279	17.291	17.303	17.315	17.327	17.339	17.351	17.363	17.376	17.388	17.4	17.412	17.424	17.437	17.449	17.461	17.473	17.486	17.498	17.51	17.523	17.535	17.547	17.56	17.572	17.585	17.597	17.61	17.622	17.635	17.647	17.66	17.672	17.685	17.697	17.71	17.723	17.735	17.748	17.761	17.773	17.786	17.799	17.812	17.824	17.837	17.85	17.863	17.876	17.889	17.901	17.914	17.927	17.94	17.953	17.966	17.979	17.992	18.005	18.018	18.031	18.045	18.058	18.071	18.084	18.097	18.11	18.124	18.137	18.15	18.163	18.177	18.19	18.203	18.217	18.23	18.243	18.257	18.27	18.284	18.297	18.311	18.324	18.338	18.351	18.365	18.378	18.392	18.406	18.419	18.433	18.447	18.46	18.474	18.488	18.501	18.515	18.529	18.543	18.557	18.571	18.585	18.598	18.612	18.626	18.64	18.654	18.668	18.682	18.696	18.71	18.725	18.739	18.753	18.767	18.781	18.795	18.81	18.824	18.838	18.852	18.867	18.881	18.896	18.91	18.924	18.939	18.953	18.968	18.982	18.997	19.011	19.026	19.04	19.055	19.07	19.084	19.099	19.114	19.128	19.143	19.158	19.173	19.187	19.202	19.217	19.232	19.247	19.262	19.277	19.292	19.307	19.322	19.337	19.352	19.367	19.382	19.397	19.412	19.427	19.443	19.458	19.473	19.488	19.504	19.519	19.534	19.55	19.565	19.581	19.596	19.612	19.627	19.643	19.658	19.674	19.689	19.705	19.721	19.736	19.752	19.768	19.783	19.799	19.815	19.831	19.847	19.862	19.878	19.894	19.91	19.926	19.942	19.958	19.974	19.99	20.006	20.023	20.039	20.055	20.071	20.087	20.104	20.12	20.136	20.152	20.169	20.185	20.202	20.218	20.235	20.251	20.268	20.284	20.301	20.317	20.334	20.351	20.367	20.384	20.401	20.417	20.434	20.451	20.468	20.485	20.502	20.519	20.536	20.553	20.57	20.587	20.604	20.621	20.638	20.655	20.672	20.69	20.707	20.724	20.741	20.759	20.776	20.793	20.811	20.828	20.846	20.863	20.881	20.898	20.916	20.934	20.951	20.969	20.987	21.004	21.022	21.04	21.058	21.076	21.094	21.112	21.13	21.148	21.166	21.184	21.202	21.22	21.238	21.256	21.274	21.293	21.311	21.329	21.347	21.366	21.384	21.403	21.421	21.44	21.458	21.477	21.495	21.514	21.532	21.551	21.57	21.589	21.607	21.626	21.645	21.664	21.683	21.702	21.721	21.74	21.759	21.778	21.797	21.816	21.835	21.854	21.874	21.893	21.912	21.932	21.951	21.97	21.99	22.009	22.029	22.048	22.068	22.088	22.107	22.127	22.147	22.166	22.186	22.206	22.226	22.246	22.266	22.286	22.306	22.326	22.346	22.366	22.386	22.406	22.427	22.447	22.467	22.488	22.508	22.528	22.549	22.569	22.59	22.61	22.631	22.652	22.672	22.693	22.714	22.735	22.755	22.776	22.797	22.818	22.839	22.86	22.881	22.902	22.923	22.945	22.966	22.987	23.008	23.03	23.051	23.072	23.094	23.115	23.137	23.158	23.18	23.202	23.223	23.245	23.267	23.289	23.311	23.332	23.354	23.376	23.398	23.42	23.443	23.465	23.487	23.509	23.531	23.554	23.576	23.598	23.621	23.643	23.666	23.688	23.711	23.734	23.756	23.779	23.802	23.825	23.848	23.871	23.893	23.916	23.94	23.963	23.986	24.009	24.032	24.055	24.079	24.102	24.126	24.149	24.172	24.196	24.22	24.243	24.267	24.291	24.314	24.338	24.362	24.386	24.41	24.434	24.458	24.482	24.506	24.531	24.555	24.579	24.603	24.628	24.652	24.677	24.701	24.726	24.75	24.775	24.8	24.825	24.85	24.874	24.899	24.924	24.949	24.974	25	25.025	25.05	25.075	25.101	25.126	25.151	25.177	25.202	25.228	25.254	25.279	25.305	25.331	25.357	25.383	25.409	25.435	25.461	25.487	25.513	25.539	25.566	25.592	25.618	25.645	25.671	25.698	25.724	25.751	25.778	25.805	25.831	25.858	25.885	25.912	25.939	25.966	25.994	26.021	26.048	26.075	26.103	26.13	26.158	26.185	26.213	26.241	26.268	26.296	26.324	26.352	26.38	26.408	26.436	26.464	26.493	26.521	26.549	26.578	26.606	26.635	26.663	26.692	26.721	26.749	26.778	26.807	26.836	26.865	26.894	26.923	26.952	26.982	27.011	27.04	27.07	27.099	27.129	27.159	27.188	27.218	27.248	27.278	27.308	27.338	27.368	27.398	27.429	27.459	27.489	27.52	27.55	27.581	27.612	27.642	27.673	27.704	27.735	27.766	27.797	27.828	27.859	27.891	27.922	27.953	27.985	28.016	28.048	28.08	28.111	28.143	28.175	28.207	28.239	28.271	28.304	28.336	28.368	28.401	28.433	28.466	28.498	28.531	28.564	28.597	28.63	28.663	28.696	28.729	28.762	28.796	28.829	28.863	28.896	28.93	28.964	28.997	29.031	29.065	29.099	29.134	29.168	29.202	29.236	29.271	29.305	29.34	29.375	29.409	29.444	29.479	29.514	29.549	29.585	29.62	29.655	29.691	29.726	29.762	29.797	29.833	29.869	29.905	29.941	29.977	30.013	30.05	30.086	30.123	30.159	30.196	30.233	30.269	30.306	30.343	30.381	30.418	30.455	30.492	30.53	30.567	30.605	30.643	30.681	30.719	30.757	30.795	30.833	30.871	30.91	30.948	30.987	31.026	31.064	31.103	31.142	31.181	31.221	31.26	31.299	31.339	31.378	31.418	31.458	31.498	31.538	31.578	31.618	31.658	31.699	31.739	31.78	31.82	31.861	31.902	31.943	31.984	32.026	32.067	32.108	32.15	32.192	32.233	32.275	32.317	32.359	32.401	32.444	32.486	32.529	32.571	32.614	32.657	32.7	32.743	32.786	32.83	32.873	32.917	32.96	33.004	33.048	33.092	33.136	33.18	33.225	33.269	33.314	33.359	33.404	33.449	33.494	33.539	33.584	33.63	33.675	33.721	33.767	33.813	33.859	33.905	33.951	33.998	34.044	34.091	34.138	34.185	34.232	34.279	34.327	34.374	34.422	34.47	34.518	34.566	34.614	34.662	34.71	34.759	34.808	34.857	34.906	34.955	35.004	35.053	35.103	35.152	35.202	35.252	35.302	35.353	35.403	35.454	35.504	35.555	35.606	35.657	35.708	35.76	35.811	35.863	35.915	35.967	36.019	36.071	36.124	36.176	36.229	36.282	36.335	36.388	36.442	36.495	36.549	36.603	36.657	36.711	36.765	36.82	36.874	36.929	36.984	37.039	37.095	37.15	37.206	37.262	37.318	37.374	37.43	37.487	37.543	37.6	37.657	37.714	37.772	37.829	37.887	37.945	38.003	38.061	38.119	38.178	38.237	38.296	38.355	38.414	38.473	38.533	38.593	38.653	38.713	38.774	38.834	38.895	38.956	39.017	39.079	39.14	39.202	39.264	39.326	39.389	39.451	39.514	39.577	39.64	39.703	39.767	39.831	39.895	39.959	40.023	40.088	40.153	40.218	40.283	40.348	40.414	40.48	40.546	40.612	40.679	40.745	40.812	40.879	40.947	41.014	41.082	41.15	41.218	41.287	41.356	41.425	41.494	41.563	41.633	41.703	41.773	41.843	41.914	41.985	42.056	42.127	42.199	42.271	42.343	42.415	42.487	42.56	42.633	42.707	42.78	42.854	42.928	43.002	43.077	43.152	43.227	43.302	43.378	43.454	43.53	43.606	43.683	43.76	43.837	43.915	43.992	44.07	44.149	44.227	44.306	44.385	44.465	44.544	44.624	44.705	44.785	44.866	44.947	45.029	45.111	45.193	45.275	45.358	45.441	45.524	45.608	45.692	45.776	45.86	45.945	46.03	46.116	46.201	46.287	46.374	46.461	46.548	46.635	46.723	46.811	46.899	46.988	47.077	47.166	47.256	47.346	47.437	47.527	47.618	47.71	47.802	47.894	47.986	48.079	48.172	48.266	48.36	48.454	48.549	48.644	48.74	48.835	48.932	49.028	49.125	49.222	49.32	49.418	49.517	49.616	49.715	49.815	49.915	50.015	50.116	50.217	50.319	50.421	50.524	50.627	50.73	50.834	50.938	51.043	51.148	51.254	51.36	51.466	51.573	51.68	51.788	51.896	52.005	52.114	52.223	52.333	52.444	52.555	52.666	52.778	52.89	53.003	53.117	53.23	53.345	53.46	53.575	53.691	53.807	53.924	54.041	54.159	54.277	54.396	54.515	54.635	54.756	54.877	54.998	55.12	55.243	55.366	55.49	55.614	55.739	55.864	55.99	56.116	56.243	56.371	56.499	56.628	56.757	56.887	57.018	57.149	57.281	57.413	57.546	57.68	57.814	57.949	58.084	58.221	58.357	58.495	58.633	58.772	58.911	59.051	59.192	59.333	59.475	59.618	59.761	59.905	60.05	60.196	60.342	60.489	60.637	60.785	60.934	61.084	61.234	61.386	61.538	61.691	61.844	61.999	62.154	62.31	62.466	62.624	62.782	62.941	63.101	63.262	63.423	63.585	63.749	63.913	64.078	64.243	64.41	64.577	64.746	64.915	65.085	65.256	65.428	65.6	65.774	65.949	66.124	66.301	66.478	66.657	66.836	67.016	67.198	67.38	67.563	67.747	67.933	68.119	68.306	68.494	68.684	68.874	69.066	69.258	69.452	69.647	69.842	70.039	70.237	70.437	70.637	70.838	71.041	71.245	71.45	71.656	71.863	72.071	72.281	72.492	72.704	72.918	73.132	73.348	73.565	73.784	74.004	74.225	74.447	74.671	74.896	75.123	75.351	75.58	75.81	76.043	76.276	76.511	76.747	76.985	77.225	77.465	77.708	77.952	78.197	78.444	78.692	78.942	79.194	79.447	79.702	79.959	80.217	80.477	80.738	81.002	81.267	81.533	81.802	82.072	82.344	82.618	82.894	83.171	83.451	83.732	84.015	84.3	84.587	84.876	85.167	85.46	85.755	86.052	86.351	86.653	86.956	87.261	87.569	87.879	88.191	88.505	88.821	89.14	89.461	89.785	90.11	90.438	90.769	91.102	91.437	91.775	92.115	92.458	92.803	93.151	93.502	93.855	94.211	94.57	94.931	95.295	95.662
blob
mark :16
data 5803
# A TRIGGER HAS TO RUN FOR THIS SCRIPT TO WORK!!!!
import numpy as np
import time
import matplotlib.pyplot as plt
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 1
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    REFRESH_TIME_S = 0.01
    import os
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")


# MAIN FUNCTION ################################################################
def main():
    # Load energy axis from file
    try:
        with open(Settings.ENERGY_FILE, "r") as f:
            energy_eV = np.loadtxt(f)
    except Exception as e:
        print(f"Failed to load energy axis from file: {e}")
        return

    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]})")
        return

    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Setup camera
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)

    time.sleep(0.2)

    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()

    print("Starting live display... (Press Ctrl+C to stop)")

    # Setup matplotlib
    plt.ion()
    fig, ax = plt.subplots()

    # Create "Keep" button
    button_ax = fig.add_axes([0.01, 0.9, 0.08, 0.06])  # [left, bottom, width, height]
    keep_button = Button(button_ax, "Keep", color='lightgray', hovercolor='lightgreen')

    # Initial ylim max
    ylim_max = 60000

    def on_key(event):
        nonlocal ylim_max
        if event.key == 'up':
            ylim_max *= 1.2  # Increase upper limit
        elif event.key == 'down':
            ylim_max /= 1.2  # Decrease upper limit
        ylim_max = max(1000, ylim_max)  # Avoid too low values
        ax.set_ylim(0, ylim_max)
        fig.canvas.draw()
        print(f"Updated ylim_max: {ylim_max:.0f}")

    def on_keep_clicked(event):
        if spectrum is not None:
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            fig.canvas.draw()
            print("Spectrum kept.")

    keep_button.on_clicked(on_keep_clicked)

    fig.canvas.mpl_connect('key_press_event', on_key)

    y = np.zeros_like(energy_eV)
    line, = ax.plot(energy_eV, y, zorder=2)

    ref_line, = ax.plot(energy_eV, np.zeros_like(y), color='green', linestyle='--', linewidth=1.5, label="Kept",
                        zorder=1)
    ref_line.set_visible(False)  # Only show when spectrum is saved

    # Buffer to store max values
    max_vals_buffer = deque([0] * 200, maxlen=200)

    # Create a small inset axes for max value trace (top-left corner)
    ax2 = fig.add_axes([0.69, 0.7, 0.3, 0.25])  # [left, bottom, width, height] in figure coords
    ax2.set_title("Max Trace", fontsize=9)
    ax2.set_ylabel("Max", fontsize=8)
    ax2.set_xticks([])
    # add a grid
    ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax2.tick_params(labelsize=8)
    max_line, = ax2.plot(list(max_vals_buffer), color='red')
    ax2.set_ylim(0, 65535)  # Adjust based on expected range of max values

    ax.set_title("Use up/down arrows to change limits", loc='left')
    ax.set_xlabel("Energy (eV)")
    ax.set_ylabel("Counts")
    ax.grid(True)
    fig.canvas.draw()
    fig.canvas.flush_events()

    try:
        spectrum = None
        while True:
            data = cam.read_newest_image()

            if data is None:
                time.sleep(Settings.EXP_TIME_MS / 1000 / 10)
                continue

            spectrum = data.ravel().astype(np.uint16)

            # Update max buffer and trace
            current_max = np.max(spectrum)
            max_vals_buffer.append(current_max)

            # Update max trace plot
            max_line.set_ydata(list(max_vals_buffer))
            max_line.set_xdata(np.arange(len(max_vals_buffer)))  # X-axis: just index
            ax2.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))  # Dynamic Y-limits
            ax2.set_xlim(0, len(max_vals_buffer))

            if spectrum.shape[0] != energy_eV.shape[0]:
                print(f"Unexpected spectrum shape: {spectrum.shape}")
                continue

            # Update plot
            try:
                line.set_ydata(spectrum)
                ax.set_ylim(0, ylim_max)
                fig.canvas.draw()
                fig.canvas.flush_events()
                time.sleep(Settings.REFRESH_TIME_S)
            except Exception as e:
                print(f"Plot update error: {e}")
                continue

    except KeyboardInterrupt:
        print("User stopped with Ctrl+C.")

    finally:
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()
        cam.close()
        print("Camera disconnected.")
        plt.ioff()
        plt.show()


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()

blob
mark :17
data 5834
import numpy as np
import time
import matplotlib
# We can still suggest a good backend, though FuncAnimation is more robust
matplotlib.use('QtAgg') 
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button
import os

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 10
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")

# MAIN FUNCTION ################################################################
def main():
    # Load energy axis from file
    
    try:
        with open(Settings.ENERGY_FILE, "r") as f:
            energy_eV = np.loadtxt(f)
    except Exception as e:
        print(f"Failed to load energy axis from file: {e}")
        return

    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]})")
        return

    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Setup camera
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)
    time.sleep(0.2)
    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()
    print("Starting live display... (Close the plot window to stop)")

    # Setup matplotlib
    fig, ax = plt.subplots()
    fig.tight_layout(rect=[0, 0, 1, 0.95])

    # --- Plot Artists ---
    y = np.zeros_like(energy_eV)
    line, = ax.plot(energy_eV, y, zorder=2)
    ref_line, = ax.plot(energy_eV, np.zeros_like(y), color='green', linestyle='--', linewidth=1.5, label="Kept", zorder=1)
    ref_line.set_visible(False)

    max_vals_buffer = deque([0] * 200, maxlen=200)
    ax2 = fig.add_axes([0.69, 0.7, 0.3, 0.25])
    max_line, = ax2.plot(list(max_vals_buffer), color='red')
    
    # --- Plot Styling ---
    ax.set_title("Use up/down arrows to change limits", loc='left')
    ax.set_xlabel("Energy (eV)")
    ax.set_ylabel("Counts")
    ax.grid(True)
    ax.set_ylim(0, 60000)
    ax2.set_title("Max Trace", fontsize=9)
    ax2.set_ylabel("Max", fontsize=8)
    ax2.set_xticks([])
    ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax2.tick_params(labelsize=8)
    ax2.set_ylim(0, 65535)

    # --- Interaction Handlers ---
# In your main() function, replace the on_key function with this one:

    def on_key(event):
        current_ylim = ax.get_ylim()
        if event.key == 'up':
            new_max = current_ylim[1] * 1.2
        elif event.key == 'down':
            new_max = current_ylim[1] / 1.2
        else:
            return
            
        ax.set_ylim(0, max(1000, new_max))
        fig.canvas.draw_idle() # <--- ADD THIS LINE
        
        print(f"Updated ylim_max: {ax.get_ylim()[1]:.0f}")

    def on_keep_clicked(event):
        # Read the latest data from the camera to 'freeze' it
        data = cam.read_newest_image()
        if data is not None:
            spectrum = data.ravel().astype(np.uint16)
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            print("Spectrum kept.")

    button_ax = fig.add_axes([0.01, 0.9, 0.08, 0.06])
    keep_button = Button(button_ax, "Keep", color='lightgray', hovercolor='lightgreen')
    keep_button.on_clicked(on_keep_clicked)
    fig.canvas.mpl_connect('key_press_event', on_key)

    # --- Animation Core ---
    def update(frame):
        data = cam.read_newest_image()
        if data is None:
            # If no new data, return the artists unchanged
            return line, max_line, ref_line

        spectrum = data.ravel().astype(np.uint16)
        if spectrum.shape[0] != energy_eV.shape[0]:
            print(f"Unexpected spectrum shape: {spectrum.shape}")
            return line, max_line, ref_line

        # Update plot data
        line.set_ydata(spectrum)
        
        # Update max trace
        current_max = np.max(spectrum)
        max_vals_buffer.append(current_max)
        max_line.set_ydata(list(max_vals_buffer))
        max_line.set_xdata(np.arange(len(max_vals_buffer)))
        ax2.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))
        
        # Return a tuple of all artists that were modified
        return line, max_line, ref_line

    # Create the animation object. 
    # interval=1 tries to run as fast as possible. The camera read time will be the real limit.
    # blit=True enables the high-speed updates.
    ani = FuncAnimation(fig, update, interval=1, blit=True, cache_frame_data=False)

    # Show the plot and start the animation. This is a blocking call.
    plt.show()

    # --- Cleanup ---
    # This code will only run after you close the matplotlib window
    print("Plot window closed. Disconnecting camera...")
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
    cam.clear_acquisition()
    cam.close()
    print("Camera disconnected.")


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()
blob
mark :18
data 6387
import numpy as np
import time
import matplotlib
# We can still suggest a good backend, though FuncAnimation is more robust
matplotlib.use('QtAgg') 
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button
import os

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 10
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")

# MAIN FUNCTION ################################################################
def main():
    # Load energy axis from file
    ani = None
    try:
        with open(Settings.ENERGY_FILE, "r") as f:
            energy_eV = np.loadtxt(f)
    except Exception as e:
        print(f"Failed to load energy axis from file: {e}")
        return

    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]})")
        return

    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Setup camera
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)
    time.sleep(0.2)
    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()
    print("Starting live display... (Close the plot window to stop)")

    # Setup matplotlib
    fig, ax = plt.subplots()
    fig.tight_layout(rect=[0, 0, 1, 0.95])

    # --- Plot Artists ---
    y = np.zeros_like(energy_eV)
    line, = ax.plot(energy_eV, y, zorder=2)
    ref_line, = ax.plot(energy_eV, np.zeros_like(y), color='green', linestyle='--', linewidth=1.5, label="Kept", zorder=1)
    ref_line.set_visible(False)

    max_vals_buffer = deque([0] * 200, maxlen=200)
    ax2 = fig.add_axes([0.69, 0.7, 0.3, 0.25])
    max_line, = ax2.plot(list(max_vals_buffer), color='red')
    
    # --- Plot Styling ---
    ax.set_title("Use up/down arrows to change limits", loc='left')
    ax.set_xlabel("Energy (eV)")
    ax.set_ylabel("Counts")
    ax.grid(True)
    ax.set_ylim(0, 60000)
    ax2.set_title("Max Trace", fontsize=9)
    ax2.set_ylabel("Max", fontsize=8)
    ax2.set_xticks([])
    ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax2.tick_params(labelsize=8)
    ax2.set_ylim(0, 65535)

    # --- Interaction Handlers ---
# In your main() function, replace the on_key function with this one:
    def on_key(event):
            nonlocal ani
            if ani is None:
                return

            # --- Stop the animation ---
            ani.event_source.stop()

            current_ylim = ax.get_ylim()
            if event.key == 'up':
                new_max = current_ylim[1] * 1.2
            elif event.key == 'down':
                new_max = current_ylim[1] / 1.2
            else:
                # If not a key we care about, resume immediately
                ani.event_source.start()
                return
            
            ax.set_ylim(0, max(1000, new_max))
            
            # --- Manually perform a full redraw NOW ---
            fig.canvas.draw()
            fig.canvas.flush_events()
            
            print(f"Updated ylim_max: {ax.get_ylim()[1]:.0f}")

            # --- Resume the fast animation ---
            ani.event_source.start()

    fig.canvas.mpl_connect('key_press_event', on_key)

    def on_keep_clicked(event):
        # Read the latest data from the camera to 'freeze' it
        data = cam.read_newest_image()
        if data is not None:
            spectrum = data.ravel().astype(np.uint16)
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            print("Spectrum kept.")

    button_ax = fig.add_axes([0.01, 0.9, 0.08, 0.06])
    keep_button = Button(button_ax, "Keep", color='lightgray', hovercolor='lightgreen')
    keep_button.on_clicked(on_keep_clicked)
    fig.canvas.mpl_connect('key_press_event', on_key)

    # --- Animation Core ---
    def update(frame):
        data = cam.read_newest_image()
        if data is None:
            # If no new data, return the artists unchanged
            return line, max_line, ref_line

        spectrum = data.ravel().astype(np.uint16)
        if spectrum.shape[0] != energy_eV.shape[0]:
            print(f"Unexpected spectrum shape: {spectrum.shape}")
            return line, max_line, ref_line

        # Update plot data
        line.set_ydata(spectrum)
        
        # Update max trace
        current_max = np.max(spectrum)
        max_vals_buffer.append(current_max)
        max_line.set_ydata(list(max_vals_buffer))
        max_line.set_xdata(np.arange(len(max_vals_buffer)))
        ax2.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))
        
        # Return a tuple of all artists that were modified
        return line, max_line, ref_line

    # Create the animation object. 
    # interval=1 tries to run as fast as possible. The camera read time will be the real limit.
    # blit=True enables the high-speed updates.
    ani = FuncAnimation(fig, update, interval=1, blit=True, cache_frame_data=False)

    # Show the plot and start the animation. This is a blocking call.
    plt.show()

    # --- Cleanup ---
    # This code will only run after you close the matplotlib window
    print("Plot window closed. Disconnecting camera...")
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
    cam.clear_acquisition()
    cam.close()
    print("Camera disconnected.")


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()
blob
mark :19
data 6569
import numpy as np
import time
import matplotlib
# We can still suggest a good backend, though FuncAnimation is more robust
matplotlib.use('QtAgg') 
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button
import os

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 20 #This one actually does not matter in this script :D
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")

# MAIN FUNCTION ################################################################
def main():
    # Load energy axis from file
    ani = None
    try:
        with open(Settings.ENERGY_FILE, "r") as f:
            energy_eV = np.loadtxt(f)
    except Exception as e:
        print(f"Failed to load energy axis from file: {e}")
        return

    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]})")
        return

    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Setup camera
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)
    time.sleep(0.2)
    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()
    print("Starting live display... (Close the plot window to stop)")

    # Setup matplotlib
    fig, ax = plt.subplots()
    fig.tight_layout(rect=[0, 0, 1, 0.95])

    # --- Plot Artists ---
    y = np.zeros_like(energy_eV)
    line, = ax.plot(energy_eV, y, zorder=2, color='#0072BD')
    ref_line, = ax.plot(energy_eV, np.zeros_like(y), color='#D95319', linestyle='--', linewidth=1, label="Kept", zorder=1)
    ref_line.set_visible(False)

    max_vals_buffer = deque([0] * 200, maxlen=200)
    ax2 = fig.add_axes([0.69, 0.7, 0.3, 0.25])
    max_line, = ax2.plot(list(max_vals_buffer), color='#A2142F')
    
    # --- Plot Styling ---
    ax.set_title("Use up/down arrows to change limits", loc='left')
    ax.set_xlabel("Energy (eV)")
    ax.set_ylabel("Counts")
    ax.grid(True)
    ax.set_ylim(0, np.max(cam.read_newest_image().ravel().astype(np.uint16)) * 2)
    ax.set_xlim(20, 75)

    ax2.set_title("Max Trace", fontsize=9)
    ax2.set_ylabel("Max", fontsize=8)
    ax2.set_xticks([])
    ax2.set_yticks([])
    ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    #ax2.tick_params(labelsize=8)
    ax2.set_ylim(0, 65535)

    # --- Interaction Handlers ---
# In your main() function, replace the on_key function with this one:
    def on_key(event):
            nonlocal ani
            if ani is None:
                return

            # --- Stop the animation ---
            ani.event_source.stop()

            current_ylim = ax.get_ylim()
            if event.key == 'up':
                new_max = current_ylim[1] * 1.2
            elif event.key == 'down':
                new_max = current_ylim[1] / 1.2
            else:
                # If not a key we care about, resume immediately
                ani.event_source.start()
                return
            
            ax.set_ylim(0, max(1000, new_max))
            
            # --- Manually perform a full redraw NOW ---
            fig.canvas.draw()
            fig.canvas.flush_events()
            
            print(f"Updated ylim_max: {ax.get_ylim()[1]:.0f}")

            # --- Resume the fast animation ---
            ani.event_source.start()

    fig.canvas.mpl_connect('key_press_event', on_key)

    def on_keep_clicked(event):
        # Read the latest data from the camera to 'freeze' it
        data = cam.read_newest_image()
        if data is not None:
            spectrum = data.ravel().astype(np.uint16)
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            print("Spectrum kept.")

    button_ax = fig.add_axes([0.01, 0.9, 0.08, 0.06])
    keep_button = Button(button_ax, "Keep", color='lightgray', hovercolor='lightgreen')
    keep_button.on_clicked(on_keep_clicked)
    fig.canvas.mpl_connect('key_press_event', on_key)

    # --- Animation Core ---
    def update(frame):
        data = cam.read_newest_image()
        if data is None:
            # If no new data, return the artists unchanged
            return line, max_line, ref_line

        spectrum = data.ravel().astype(np.uint16)
        if spectrum.shape[0] != energy_eV.shape[0]:
            print(f"Unexpected spectrum shape: {spectrum.shape}")
            return line, max_line, ref_line

        # Update plot data
        line.set_ydata(spectrum)
        
        # Update max trace
        current_max = np.max(spectrum)
        max_vals_buffer.append(current_max)
        max_line.set_ydata(list(max_vals_buffer))
        max_line.set_xdata(np.arange(len(max_vals_buffer)))
        ax2.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))
        
        # Return a tuple of all artists that were modified
        return line, max_line, ref_line

    # Create the animation object. 
    # interval=1 tries to run as fast as possible. The camera read time will be the real limit.
    # blit=True enables the high-speed updates.
    ani = FuncAnimation(fig, update, interval=1, blit=True, cache_frame_data=False)

    # Show the plot and start the animation. This is a blocking call.
    plt.show()

    # --- Cleanup ---
    # This code will only run after you close the matplotlib window
    print("Plot window closed. Disconnecting camera...")
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
    cam.clear_acquisition()
    cam.close()
    print("Camera disconnected.")


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()
blob
mark :20
data 8891
import numpy as np
import time
import matplotlib
# We can still suggest a good backend, though FuncAnimation is more robust
matplotlib.use('QtAgg') 
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec # Import GridSpec for advanced layouts
from matplotlib.animation import FuncAnimation
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button
import os

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 20 #This one actually does not matter in this script :D
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")

# MAIN FUNCTION ################################################################
def main():
    # Load energy axis from file
    ani = None
    try:
        with open(Settings.ENERGY_FILE, "r") as f:
            energy_eV = np.loadtxt(f)
    except Exception as e:
        print(f"Failed to load energy axis from file: {e}")
        return

    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]})")
        return

    # --- Find indices for the Standard Deviation ROI (20-75 eV) ---
    roi_indices = np.where((energy_eV >= 20) & (energy_eV <= 75))[0]
    
    # --- Data buffers for the new plot ---
    # Buffer for the last 2 seconds of total counts for calculation
    counts_buffer_2s = deque() 
    # Buffer for the last 20 seconds of std dev values for plotting
    std_dev_plot_buffer_20s = deque()

    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Setup camera
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)
    time.sleep(0.2)
    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()
    print("Starting live display... (Close the plot window to stop)")

    # --- Setup matplotlib figure and layout using GridSpec ---
    fig = plt.figure(figsize=(12, 8))
    gs = GridSpec(2, 2, figure=fig, height_ratios=[1, 4])

    ax_spec = fig.add_subplot(gs[1, :])      # Bottom plot, spanning both columns
    ax_top_left = fig.add_subplot(gs[0, 0])   # Top-left plot
    ax_max_trace = fig.add_subplot(gs[0, 1])  # Top-right plot
    
    # --- Plot Artists ---
    y = np.zeros_like(energy_eV)
    line, = ax_spec.plot(energy_eV, y, zorder=2, color='#0072BD')
    ref_line, = ax_spec.plot(energy_eV, np.zeros_like(y), color='#D95319', linestyle='--', linewidth=1, label="Kept", zorder=1)
    ref_line.set_visible(False)

    max_vals_buffer = deque([0] * 200, maxlen=200)
    max_line, = ax_max_trace.plot(list(max_vals_buffer), color='#A2142F')
    
    # Artist for the new standard deviation plot
    std_dev_line, = ax_top_left.plot([], [], color='#EDB120')

    # --- Plot Styling ---
    # Bottom Spectrum Plot
    ax_spec.set_title("Use up/down arrows to change limits", loc='left')
    ax_spec.set_xlabel("Energy (eV)")
    ax_spec.set_ylabel("Counts")
    ax_spec.grid(True)
    ax_spec.set_ylim(0, np.max(cam.read_newest_image().ravel().astype(np.uint16)) * 2)
    xmin = 20
    xmax = 75
    ax_spec.set_xlim(xmin, xmax)
    ax_spec.hlines(y=65535, xmin=xmin, xmax=xmax, colors='#7E2F8E', linestyles='--', linewidth=1)

    # Top-Right Max Trace Plot
    ax_max_trace.set_title("Max Trace", fontsize=9)
    ax_max_trace.set_ylabel("Max", fontsize=8)
    ax_max_trace.set_xticks([])
    ax_max_trace.tick_params(axis='y', labelsize=8)
    ax_max_trace.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax_max_trace.set_ylim(0, 65535)

    # Top-Left Standard Deviation Plot
    ax_top_left.set_title("Std Dev of Counts in ROI (2s window)", fontsize=9)
    ax_top_left.set_xlabel("Time ago (s)", fontsize=8)
    ax_top_left.set_ylabel("Std Dev", fontsize=8)
    ax_top_left.tick_params(labelsize=8)
    ax_top_left.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax_top_left.set_xlim(20, 0) # Inverted axis to show 'time ago'
    ax_top_left.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%d'))
    
    # --- Interaction Handlers ---
    def on_key(event):
        nonlocal ani
        if ani is None: return
        ani.event_source.stop()
        current_ylim = ax_spec.get_ylim()
        if event.key == 'up': new_max = current_ylim[1] * 1.2
        elif event.key == 'down': new_max = current_ylim[1] / 1.2
        else:
            ani.event_source.start()
            return
        ax_spec.set_ylim(0, max(1000, new_max))
        fig.canvas.draw()
        fig.canvas.flush_events()
        print(f"Updated ylim_max: {ax_spec.get_ylim()[1]:.0f}")
        ani.event_source.start()

    fig.canvas.mpl_connect('key_press_event', on_key)

    def on_keep_clicked(event):
        data = cam.read_newest_image()
        if data is not None:
            spectrum = data.ravel().astype(np.uint16)
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            print("Spectrum kept.")

    button_ax = fig.add_axes([0.01, 0.92, 0.08, 0.06])
    keep_button = Button(button_ax, "Keep", color='lightgray', hovercolor='lightgreen')
    keep_button.on_clicked(on_keep_clicked)
    
    plt.tight_layout(rect=[0, 0, 1, 0.92])

    # --- Animation Core ---
    def update(frame):
        current_time = time.time()
        data = cam.read_newest_image()
        if data is None:
            return line, max_line, ref_line, std_dev_line

        spectrum = data.ravel().astype(np.uint16)
        if spectrum.shape[0] != energy_eV.shape[0]:
            print(f"Unexpected spectrum shape: {spectrum.shape}")
            return line, max_line, ref_line, std_dev_line

        # Update main spectrum plot
        line.set_ydata(spectrum)
        
        # Update max trace plot
        current_max = np.max(spectrum)
        max_vals_buffer.append(current_max)
        max_line.set_ydata(list(max_vals_buffer))
        max_line.set_xdata(np.arange(len(max_vals_buffer)))
        ax_max_trace.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))
        
        # --- NEW: Update Standard Deviation Plot ---
        # 1. Calculate total counts in the defined ROI
        total_counts_in_roi = np.sum(spectrum[roi_indices])
        
        # 2. Add to 2-second buffer and prune old data
        counts_buffer_2s.append((current_time, total_counts_in_roi))
        while counts_buffer_2s and (current_time - counts_buffer_2s[0][0] > 2):
            counts_buffer_2s.popleft()
            
        # 3. Calculate std dev if buffer is sufficient
        std_dev = 0.0
        if len(counts_buffer_2s) >= 2:
            counts_values = [item[1] for item in counts_buffer_2s]
            std_dev = np.std(counts_values)
            
        # 4. Add to 20-second plotting buffer and prune old data
        std_dev_plot_buffer_20s.append((current_time, std_dev))
        while std_dev_plot_buffer_20s and (current_time - std_dev_plot_buffer_20s[0][0] > 20):
            std_dev_plot_buffer_20s.popleft()

        # 5. Update the plot with the last 20s of data
        if len(std_dev_plot_buffer_20s) >= 2:
            time_vals, std_vals = zip(*std_dev_plot_buffer_20s)
            # Plot against 'time ago'
            relative_time_ago = np.array(time_vals) - current_time
            
            std_dev_line.set_data(relative_time_ago, std_vals)
            ax_top_left.set_ylim(0, max(1, np.max(std_vals) * 1.2)) # Avoid ylim of 0
            ax_top_left.set_xlim(-20, 0)
            #print(np.max(std_vals))

        # Return a tuple of all artists that were modified
        return line, max_line, ref_line, std_dev_line

    ani = FuncAnimation(fig, update, interval=1, blit=True, cache_frame_data=False)
    plt.show()

    # --- Cleanup ---
    print("Plot window closed. Disconnecting camera...")
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
    cam.clear_acquisition()
    cam.close()
    print("Camera disconnected.")


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()
blob
mark :21
data 8460
import numpy as np
import time
import matplotlib
# We can still suggest a good backend, though FuncAnimation is more robust
matplotlib.use('QtAgg') 
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec # Import GridSpec for advanced layouts
# --- CHANGE: Import ticker for formatting the new gauge's labels ---
import matplotlib.ticker as mticker
from matplotlib.animation import FuncAnimation
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button
import os

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 20 #This one actually does not matter in this script :D
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")

# MAIN FUNCTION ################################################################
def main():
    # Load energy axis from file
    ani = None
    try:
        with open(Settings.ENERGY_FILE, "r") as f:
            energy_eV = np.loadtxt(f)
    except Exception as e:
        print(f"Failed to load energy axis from file: {e}")
        return

    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]})")
        return

    # --- Find indices for the Standard Deviation ROI (20-75 eV) ---
    roi_indices = np.where((energy_eV >= 20) & (energy_eV <= 75))[0]
    
    # --- Data buffer for the std dev calculation ---
    # Buffer for the last 2 seconds of total counts for calculation
    counts_buffer_2s = deque() 

    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())
    print('Please standby... MarcelCorp.TM Code loading')
    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Setup camera
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)
    time.sleep(0.2)
    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()
    print("Starting live display... (Close the plot window to stop)")

    # --- Setup matplotlib figure and layout using GridSpec ---
    fig = plt.figure(figsize=(12, 8))
    gs = GridSpec(2, 2, figure=fig, height_ratios=[1, 4], width_ratios=[10, 1])

    ax_spec = fig.add_subplot(gs[1, 0])      # Bottom plot, spanning both columns
    ax_std = fig.add_subplot(gs[:, 1])   # Top-left plot
    ax_max_trace = fig.add_subplot(gs[0, 0])  # Top-right plot
    ax_std.yaxis.tick_right()
    ax_std.yaxis.set_label_position("right")
    # --- Plot Artists ---
    y = np.zeros_like(energy_eV)
    line, = ax_spec.plot(energy_eV, y, zorder=2, color='#0072BD')
    ref_line, = ax_spec.plot(energy_eV, np.zeros_like(y), color='#D95319', linestyle='--', linewidth=1, label="Kept", zorder=1)
    ref_line.set_visible(False)

    max_vals_buffer = deque([0] * 200, maxlen=200)
    max_line, = ax_max_trace.plot(list(max_vals_buffer), color='#A2142F')
    
    # --- CHANGE: Artist for the new "water fill" gauge ---
    # Create a bar container, then get the single bar patch from it.
    bar_container = ax_std.bar(0, 0, color='#0077BE', width=1.0)
    std_dev_bar_patch = bar_container[0]


    # --- Plot Styling ---
    # Bottom Spectrum Plot
    ax_spec.set_title("Use up/down arrows to change limits", loc='left')
    ax_spec.set_xlabel("Energy (eV)")
    ax_spec.set_ylabel("Counts")
    ax_spec.grid(True)
    ax_spec.set_ylim(0, np.max(cam.read_newest_image().ravel().astype(np.uint16)) * 2)
    xmin = 20
    xmax = 75
    ax_spec.set_xlim(xmin, xmax)
    ax_spec.hlines(y=65535, xmin=xmin, xmax=xmax, colors='#7E2F8E', linestyles='--', linewidth=1)

    # Top-Right Max Trace Plot
    ax_max_trace.set_title("Max Trace", fontsize=9)
    ax_max_trace.set_ylabel("Max", fontsize=8)
    ax_max_trace.set_xticks([])
    ax_max_trace.tick_params(axis='y', labelsize=8)
    ax_max_trace.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax_max_trace.set_ylim(0, 65535)

    # --- CHANGE: Styling for the new "water fill" gauge ---
    ax_std.set_title("Live Std Dev (2s window)", fontsize=9)
    # Set a fixed vertical scale from 0 to 100,000
    ax_std.set_ylim(0, 100000)
    # The x-axis is not meaningful, so hide it
    ax_std.set_xticks([])
    ax_std.set_xlim(-0.5, 0.5)
    # Format y-axis labels to be more readable (e.g., 50k)
    ax_std.yaxis.set_major_formatter(mticker.EngFormatter())
    ax_std.tick_params(labelsize=8)
    
    # --- Interaction Handlers ---
    def on_key(event):
        nonlocal ani
        if ani is None: return
        ani.event_source.stop()
        current_ylim = ax_spec.get_ylim()
        if event.key == 'up': new_max = current_ylim[1] * 1.2
        elif event.key == 'down': new_max = current_ylim[1] / 1.2
        else:
            ani.event_source.start()
            return
        ax_spec.set_ylim(0, max(1000, new_max))
        fig.canvas.draw()
        fig.canvas.flush_events()
        ani.event_source.start()

    fig.canvas.mpl_connect('key_press_event', on_key)

    def on_keep_clicked(event):
        data = cam.read_newest_image()
        if data is not None:
            spectrum = data.ravel().astype(np.uint16)
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            print("Spectrum kept.")

    button_ax = fig.add_axes([0.01, 0.92, 0.08, 0.06])
    keep_button = Button(button_ax, "Keep", color='lightgray', hovercolor='lightgreen')
    keep_button.on_clicked(on_keep_clicked)
    
    plt.tight_layout(rect=[0, 0, 1, 0.92])

    # --- Animation Core ---
    def update(frame):
        current_time = time.time()
        data = cam.read_newest_image()
        if data is None:
            return line, max_line, ref_line, std_dev_bar_patch

        spectrum = data.ravel().astype(np.uint16)
        if spectrum.shape[0] != energy_eV.shape[0]:
            print(f"Unexpected spectrum shape: {spectrum.shape}")
            return line, max_line, ref_line, std_dev_bar_patch

        # Update main spectrum plot
        line.set_ydata(spectrum)
        
        # Update max trace plot
        current_max = np.max(spectrum)
        max_vals_buffer.append(current_max)
        max_line.set_ydata(list(max_vals_buffer))
        max_line.set_xdata(np.arange(len(max_vals_buffer)))
        ax_max_trace.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))
        
        # --- CHANGE: Update Standard Deviation GAUGE ---
        # 1. Calculate total counts in the defined ROI
        total_counts_in_roi = np.sum(spectrum[roi_indices])
        
        # 2. Add to 2-second buffer and prune old data
        counts_buffer_2s.append((current_time, total_counts_in_roi))
        while counts_buffer_2s and (current_time - counts_buffer_2s[0][0] > 2):
            counts_buffer_2s.popleft()
            
        # 3. Calculate std dev if buffer is sufficient
        std_dev = 0.0
        if len(counts_buffer_2s) >= 2:
            counts_values = [item[1] for item in counts_buffer_2s]
            std_dev = np.std(counts_values)

        # 4. Update the height of the bar patch
        std_dev_bar_patch.set_height(std_dev)

        # Return a tuple of all artists that were modified
        return line, max_line, ref_line, std_dev_bar_patch

    ani = FuncAnimation(fig, update, interval=1, blit=True, cache_frame_data=False)
    plt.show()

    # --- Cleanup ---
    print("Plot window closed. Disconnecting camera...")
    print("Thank you for choosing code from MarcelCorp. TM!")
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
    cam.clear_acquisition()
    cam.close()
    print("Camera disconnected.")


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()
blob
mark :22
data 9237
import numpy as np
import time
import matplotlib
# We can still suggest a good backend, though FuncAnimation is more robust
matplotlib.use('QtAgg') 
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec # Import GridSpec for advanced layouts
# --- CHANGE: Import ticker for formatting the new gauge's labels ---
import matplotlib.ticker as mticker
from matplotlib.animation import FuncAnimation
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button
import os
from matplotlib.ticker import PercentFormatter

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 1 #This one actually does not matter in this script :D
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")

# MAIN FUNCTION ################################################################
def main():
    # Load energy axis from file
    ani = None
    try:
        with open(Settings.ENERGY_FILE, "r") as f:
            energy_eV = np.loadtxt(f)
    except Exception as e:
        print(f"Failed to load energy axis from file: {e}")
        return

    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]})")
        return

    # --- Find indices for the Standard Deviation ROI (20-75 eV) ---
    roi_indices = np.where((energy_eV >= 20) & (energy_eV <= 75))[0]
    
    # --- Data buffer for the std dev calculation ---
    # Buffer for the last 2 seconds of full spectra for calculation
    spectrum_buffer_2s = deque() 

    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())
    #print('Please standby... Code from MarcelCorp.TM loading')
    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Setup camera
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)
    time.sleep(0.2)
    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()
    print("Starting live display... (Close the plot window to stop)")

    # --- Setup matplotlib figure and layout using GridSpec ---
    fig = plt.figure(figsize=(12, 8))
    gs = GridSpec(2, 2, figure=fig, height_ratios=[1, 4], width_ratios=[14, 1])

    # --- Create subplots ---
    ax_spec = fig.add_subplot(gs[1, 0])      # Bottom plot, spanning both columns
    ax_std = fig.add_subplot(gs[:, 1])   # Top-left plot
    ax_max_trace = fig.add_subplot(gs[0, 0])  # Top-right plot
    ax_std.yaxis.tick_right()
    ax_std.yaxis.set_label_position("right")
    # --- Plot Artists ---
    y = np.zeros_like(energy_eV)
    line, = ax_spec.plot(energy_eV, y, zorder=2, color='#0072BD')
    ref_line, = ax_spec.plot(energy_eV, np.zeros_like(y), color='#D95319', linestyle='--', linewidth=1, label="Kept", zorder=1)
    ref_line.set_visible(False)

    max_vals_buffer = deque([0] * 200, maxlen=200)
    max_line, = ax_max_trace.plot(list(max_vals_buffer), color='#A2142F')
    
    # --- CHANGE: Artist for the new "water fill" gauge ---
    # Create a bar container, then get the single bar patch from it.
    bar_container = ax_std.bar(0, 0, color='#33A02C', width=1.0)
    std_dev_bar_patch = bar_container[0]

    # --- Plot Styling ---
    # Bottom Spectrum Plot
    ax_spec.set_title("Use up/down arrows to change limits", loc='left')
    ax_spec.set_xlabel("Energy (eV)")
    ax_spec.set_ylabel("Counts")
    ax_spec.grid(True)
    ax_spec.set_ylim(0, np.max(cam.read_newest_image().ravel().astype(np.uint16)) * 2)
    xmin = 20
    xmax = 75
    ax_spec.set_xlim(xmin, xmax)
    ax_spec.hlines(y=65535, xmin=xmin, xmax=xmax, colors='#7E2F8E', linestyles='--', linewidth=1)

    # Top-Right Max Trace Plot
    ax_max_trace.set_title("Max Trace", fontsize=9)
    ax_max_trace.set_ylabel("Max", fontsize=8)
    ax_max_trace.set_xticks([])
    ax_max_trace.tick_params(axis='y', labelsize=8)
    ax_max_trace.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax_max_trace.set_ylim(0, 65535)

    # --- CHANGE: Styling for the new "water fill" gauge ---
    ax_std.set_title("Avg. Norm. Std (2s window)", fontsize=9)
    # Set a fixed vertical scale from 0 to 1
    ax_std.set_ylim(0, 5) # in percent
    # The x-axis is not meaningful, so hide it
    ax_std.set_xticks([])
    ax_std.set_xlim(-0.5, 0.5)
    # Format y-axis labels to be more readable (e.g., 50k)
    ax_std.yaxis.set_major_formatter(mticker.PercentFormatter())
    ax_std.tick_params(labelsize=8)
    
    # --- Interaction Handlers ---
    def on_key(event):
        nonlocal ani
        if ani is None: return
        ani.event_source.stop()
        current_ylim = ax_spec.get_ylim()
        if event.key == 'up': new_max = current_ylim[1] * 1.2
        elif event.key == 'down': new_max = current_ylim[1] / 1.2
        else:
            ani.event_source.start()
            return
        ax_spec.set_ylim(0, max(1000, new_max))
        fig.canvas.draw()
        fig.canvas.flush_events()
        ani.event_source.start()

    fig.canvas.mpl_connect('key_press_event', on_key)

    def on_keep_clicked(event):
        data = cam.read_newest_image()
        if data is not None:
            spectrum = data.ravel().astype(np.uint16)
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            print("Spectrum kept.")

    button_ax = fig.add_axes([0.01, 0.92, 0.08, 0.06])
    keep_button = Button(button_ax, "Keep", color='lightgray', hovercolor='lightgreen')
    keep_button.on_clicked(on_keep_clicked)
    
    #plt.tight_layout(rect=[0, 0, 1, 0.92])

    # --- Animation Core ---
    def update(frame):
        current_time = time.time()
        data = cam.read_newest_image()
        if data is None:
            return line, max_line, ref_line, std_dev_bar_patch

        spectrum = data.ravel().astype(np.uint16)
        if spectrum.shape[0] != energy_eV.shape[0]:
            print(f"Unexpected spectrum shape: {spectrum.shape}")
            return line, max_line, ref_line, std_dev_bar_patch

        # Update main spectrum plot
        line.set_ydata(spectrum)
        
        # Update max trace plot
        current_max = np.max(spectrum)
        max_vals_buffer.append(current_max)
        max_line.set_ydata(list(max_vals_buffer))
        max_line.set_xdata(np.arange(len(max_vals_buffer)))
        ax_max_trace.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))
        
        # --- CHANGE: Update Standard Deviation GAUGE ---
        # 1. Add full spectrum to 2-second buffer and prune old data
        spectrum_buffer_2s.append((current_time, spectrum))
        while spectrum_buffer_2s and (current_time - spectrum_buffer_2s[0][0] > 2):
            spectrum_buffer_2s.popleft()
            
        # 2. Calculate normalized std dev if buffer is sufficient
        avg_norm_std = 0.0
        if len(spectrum_buffer_2s) >= 2:
            # Create a 2D array of spectra over time
            spectra_over_time = np.array([item[1] for item in spectrum_buffer_2s])
            
            # Calculate std and mean for each energy bin (column-wise)
            std_per_energy = np.std(spectra_over_time, axis=0)
            mean_per_energy = np.mean(spectra_over_time, axis=0)
            
            # Calculate normalized std, avoiding division by zero
            normalized_std = np.divide(std_per_energy, mean_per_energy, 
                                      out=np.zeros_like(std_per_energy), 
                                      where=mean_per_energy!=0)
            
            # Average the normalized std over the ROI
            if roi_indices.size > 0:
                avg_norm_std = np.mean(normalized_std[roi_indices])
        avg_norm_std = avg_norm_std * 100 #Change units to percent
        # 3. Update the height of the bar patch
        std_dev_bar_patch.set_height(avg_norm_std)

        # Return a tuple of all artists that were modified
        return line, max_line, ref_line, std_dev_bar_patch

    ani = FuncAnimation(fig, update, interval=1, blit=True, cache_frame_data=False)
    plt.show()

    # --- Cleanup ---
    print("Plot window closed. Disconnecting camera...")
    #print("Thank you for choosing code from MarcelCorp. TM!")
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
    cam.clear_acquisition()
    cam.close()
    print("Camera disconnected.")

# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()
blob
mark :23
data 10875
import numpy as np
import time
import matplotlib
# We can still suggest a good backend, though FuncAnimation is more robust
matplotlib.use('QtAgg') 
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec # Import GridSpec for advanced layouts
# --- CHANGE: Import ticker for formatting the new gauge's labels ---
import matplotlib.ticker as mticker
from matplotlib.animation import FuncAnimation
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button, TextBox
import os
from matplotlib.ticker import PercentFormatter

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 1 #This one actually does not matter in this script :D
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")

# MAIN FUNCTION ################################################################
def main():
    # Load energy axis from file
    ani = None
    try:
        with open(Settings.ENERGY_FILE, "r") as f:
            energy_eV = np.loadtxt(f)
    except Exception as e:
        print(f"Failed to load energy axis from file: {e}")
        return

    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]})")
        return

    # --- Set default Standard Deviation ROI to full range ---
    roi_indices = np.arange(energy_eV.shape[0])
    
    # --- Data buffer for the std dev calculation ---
    # Buffer for the last 2 seconds of full spectra for calculation
    spectrum_buffer_2s = deque() 

    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())
    #print('Please standby... Code from MarcelCorp.TM loading')
    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Setup camera
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)
    time.sleep(0.2)
    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()
    print("Starting live display... (Close the plot window to stop)")

    # --- Setup matplotlib figure and layout using GridSpec ---
    fig = plt.figure(figsize=(12, 8))
    gs = GridSpec(2, 2, figure=fig, height_ratios=[1, 4], width_ratios=[14, 1])

    # --- Create subplots ---
    ax_spec = fig.add_subplot(gs[1, 0])      # Bottom plot, spanning both columns
    ax_std = fig.add_subplot(gs[:, 1])   # Top-left plot
    ax_max_trace = fig.add_subplot(gs[0, 0])  # Top-right plot
    ax_std.yaxis.tick_right()
    ax_std.yaxis.set_label_position("right")
    # --- Plot Artists ---
    y = np.zeros_like(energy_eV)
    line, = ax_spec.plot(energy_eV, y, zorder=2, color='#0072BD')
    ref_line, = ax_spec.plot(energy_eV, np.zeros_like(y), color='#D95319', linestyle='--', linewidth=1, label="Kept", zorder=1)
    ref_line.set_visible(False)

    max_vals_buffer = deque([0] * 200, maxlen=200)
    max_line, = ax_max_trace.plot(list(max_vals_buffer), color='#A2142F')
    
    # --- CHANGE: Artist for the new "water fill" gauge ---
    # Create a bar container, then get the single bar patch from it.
    bar_container = ax_std.bar(0, 0, color='#33A02C', width=1.0)
    std_dev_bar_patch = bar_container[0]

    # --- Plot Styling ---
    # Bottom Spectrum Plot
    ax_spec.set_title("Use up/down arrows to change limits", loc='left')
    ax_spec.set_xlabel("Energy (eV)")
    ax_spec.set_ylabel("Counts")
    ax_spec.grid(True)
    ax_spec.set_ylim(0, np.max(cam.read_newest_image().ravel().astype(np.uint16)) * 2)
    xmin = 20
    xmax = 75
    ax_spec.set_xlim(xmin, xmax)
    ax_spec.hlines(y=65535, xmin=xmin, xmax=xmax, colors='#7E2F8E', linestyles='--', linewidth=1)

    # Top-Right Max Trace Plot
    ax_max_trace.set_title("Max Trace", fontsize=9)
    ax_max_trace.set_ylabel("Max", fontsize=8)
    ax_max_trace.set_xticks([])
    ax_max_trace.tick_params(axis='y', labelsize=8)
    ax_max_trace.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax_max_trace.set_ylim(0, 65535)

    # --- CHANGE: Styling for the new "water fill" gauge ---
    ax_std.set_title("Avg. Norm. Std (2s window)", fontsize=9)
    # Set a fixed vertical scale from 0 to 1
    ax_std.set_ylim(0, 5) # in percent
    # The x-axis is not meaningful, so hide it
    ax_std.set_xticks([])
    ax_std.set_xlim(-0.5, 0.5)
    # Format y-axis labels to be more readable (e.g., 50k)
    ax_std.yaxis.set_major_formatter(mticker.PercentFormatter())
    ax_std.tick_params(labelsize=8)
    
    # --- Interaction Handlers ---
    def on_key(event):
        nonlocal ani
        if ani is None: return
        ani.event_source.stop()
        current_ylim = ax_spec.get_ylim()
        if event.key == 'up': new_max = current_ylim[1] * 1.2
        elif event.key == 'down': new_max = current_ylim[1] / 1.2
        else:
            ani.event_source.start()
            return
        ax_spec.set_ylim(0, max(1000, new_max))
        fig.canvas.draw()
        fig.canvas.flush_events()
        ani.event_source.start()

    fig.canvas.mpl_connect('key_press_event', on_key)

    def on_keep_clicked(event):
        data = cam.read_newest_image()
        if data is not None:
            spectrum = data.ravel().astype(np.uint16)
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            print("Spectrum kept.")

    button_ax = fig.add_axes([0.01, 0.92, 0.08, 0.06])
    keep_button = Button(button_ax, "Keep", color='lightgray', hovercolor='lightgreen')
    keep_button.on_clicked(on_keep_clicked)
    
    # --- Add text boxes for Std Dev ROI control ---
    def on_roi_submit(text):
        nonlocal roi_indices
        try:
            min_val = float(text_box_min.text)
            max_val = float(text_box_max.text)
        except ValueError:
            print("Invalid ROI input. Please enter numbers.")
            return

        if min_val >= max_val:
            print("Min ROI must be less than Max ROI.")
            return

        roi_indices = np.where((energy_eV >= min_val) & (energy_eV <= max_val))[0]
        print(f"Std Dev ROI set to {min_val:.1f}-{max_val:.1f} eV.")

    def on_std_full_clicked(event):
        nonlocal roi_indices
        roi_indices = np.arange(energy_eV.shape[0])
        # Update text boxes to reflect the full range
        text_box_min.set_val(f"{energy_eV[0]:.1f}")
        text_box_max.set_val(f"{energy_eV[-1]:.1f}")
        print("Std Dev ROI set to full spectrum.")

    # Create text boxes for min and max ROI
    ax_roi_min = fig.add_axes([0.10, 0.92, 0.05, 0.06])
    text_box_min = TextBox(ax_roi_min, 'Min (eV)', initial=f"{xmin:.1f}")
    text_box_min.on_submit(on_roi_submit)

    ax_roi_max = fig.add_axes([0.16, 0.92, 0.05, 0.06])
    text_box_max = TextBox(ax_roi_max, 'Max (eV)', initial=f"{xmax:.1f}")
    text_box_max.on_submit(on_roi_submit)
    
    # Manually trigger the first ROI calculation with initial values
    on_roi_submit(None)

    std_full_ax = fig.add_axes([0.22, 0.92, 0.1, 0.06])
    std_full_button = Button(std_full_ax, "Std Dev (Full)", color='lightgray', hovercolor='lightblue')
    std_full_button.on_clicked(on_std_full_clicked)

    #plt.tight_layout(rect=[0, 0, 1, 0.92])

    # --- Animation Core ---
    def update(frame):
        current_time = time.time()
        data = cam.read_newest_image()
        if data is None:
            return line, max_line, ref_line, std_dev_bar_patch

        spectrum = data.ravel().astype(np.uint16)
        if spectrum.shape[0] != energy_eV.shape[0]:
            print(f"Unexpected spectrum shape: {spectrum.shape}")
            return line, max_line, ref_line, std_dev_bar_patch

        # Update main spectrum plot
        line.set_ydata(spectrum)
        
        # Update max trace plot
        current_max = np.max(spectrum)
        max_vals_buffer.append(current_max)
        max_line.set_ydata(list(max_vals_buffer))
        max_line.set_xdata(np.arange(len(max_vals_buffer)))
        ax_max_trace.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))
        
        # --- CHANGE: Update Standard Deviation GAUGE ---
        # 1. Add full spectrum to 2-second buffer and prune old data
        spectrum_buffer_2s.append((current_time, spectrum))
        while spectrum_buffer_2s and (current_time - spectrum_buffer_2s[0][0] > 2):
            spectrum_buffer_2s.popleft()
            
        # 2. Calculate normalized std dev if buffer is sufficient
        avg_norm_std = 0.0
        if len(spectrum_buffer_2s) >= 2:
            # Create a 2D array of spectra over time
            spectra_over_time = np.array([item[1] for item in spectrum_buffer_2s])
            
            # Calculate std and mean for each energy bin (column-wise)
            std_per_energy = np.std(spectra_over_time, axis=0)
            mean_per_energy = np.mean(spectra_over_time, axis=0)
            
            # Calculate normalized std, avoiding division by zero
            normalized_std = np.divide(std_per_energy, mean_per_energy, 
                                      out=np.zeros_like(std_per_energy), 
                                      where=mean_per_energy!=0)
            
            # Average the normalized std over the ROI
            if roi_indices.size > 0:
                avg_norm_std = np.mean(normalized_std[roi_indices])
        avg_norm_std = avg_norm_std * 100 #Change units to percent
        # 3. Update the height of the bar patch
        std_dev_bar_patch.set_height(avg_norm_std)

        # Return a tuple of all artists that were modified
        return line, max_line, ref_line, std_dev_bar_patch

    ani = FuncAnimation(fig, update, interval=1, blit=True, cache_frame_data=False)
    plt.show()

    # --- Cleanup ---
    print("Plot window closed. Disconnecting camera...")
    #print("Thank you for choosing code from MarcelCorp. TM!")
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
    cam.clear_acquisition()
    cam.close()
    print("Camera disconnected.")

# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()
blob
mark :24
data 11041
import numpy as np
import time
import matplotlib
# We can still suggest a good backend, though FuncAnimation is more robust
matplotlib.use('QtAgg') 
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec # Import GridSpec for advanced layouts
# --- CHANGE: Import ticker for formatting the new gauge's labels ---
import matplotlib.ticker as mticker
from matplotlib.animation import FuncAnimation
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button, TextBox
import os
from matplotlib.ticker import PercentFormatter

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 1  # This one actually does not matter in this script :D
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")

# MAIN FUNCTION ################################################################
def main():
    # Load energy axis from file
    ani = None
    try:
        with open(Settings.ENERGY_FILE, "r") as f:
            energy_eV = np.loadtxt(f)
    except Exception as e:
        print(f"Failed to load energy axis from file: {e}")
        return

    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]})")
        return

    # --- Set default Standard Deviation ROI to full range ---
    roi_indices = np.arange(energy_eV.shape[0])
    
    # --- Data buffer for the std dev calculation ---
    # Buffer for the last 2 seconds of full spectra for calculation
    spectrum_buffer_2s = deque() 

    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())
    #print('Please standby... Code from MarcelCorp.TM loading')
    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Setup camera
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)
    time.sleep(0.2)
    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()
    print("Starting live display... (Close the plot window to stop)")

    # --- Setup matplotlib figure and layout using GridSpec ---
    fig = plt.figure(figsize=(12, 8))
    gs = GridSpec(2, 2, figure=fig, height_ratios=[1, 4], width_ratios=[14, 1])

    # --- Create subplots ---
    ax_spec = fig.add_subplot(gs[1, 0])      # Bottom plot, spanning both columns
    ax_std = fig.add_subplot(gs[:, 1])   # Top-left plot
    ax_max_trace = fig.add_subplot(gs[0, 0])  # Top-right plot
    ax_std.yaxis.tick_right()
    ax_std.yaxis.set_label_position("right")
    # --- Plot Artists ---
    y = np.zeros_like(energy_eV)
    line, = ax_spec.plot(energy_eV, y, zorder=2, color='#0072BD')
    ref_line, = ax_spec.plot(energy_eV, np.zeros_like(y), color='#D95319', linestyle='--', linewidth=1, label="Kept", zorder=1)
    ref_line.set_visible(False)

    max_vals_buffer = deque([0] * 200, maxlen=200)
    max_line, = ax_max_trace.plot(list(max_vals_buffer), color='#A2142F')
    
    # --- CHANGE: Artist for the new "water fill" gauge ---
    # Create a bar container, then get the single bar patch from it.
    bar_container = ax_std.bar(0, 0, color='#33A02C', width=1.0)
    std_dev_bar_patch = bar_container[0]

    # --- Plot Styling ---
    # Bottom Spectrum Plot
    ax_spec.set_title("Use up/down arrows to change limits", loc='left')
    ax_spec.set_xlabel("Energy (eV)")
    ax_spec.set_ylabel("Counts")
    ax_spec.grid(True)
    ax_spec.set_ylim(0, np.max(cam.read_newest_image().ravel().astype(np.uint16)) * 2)
    xmin = 20
    xmax = 75
    ax_spec.set_xlim(xmin, xmax)
    ax_spec.hlines(y=65535, xmin=xmin, xmax=xmax, colors='#7E2F8E', linestyles='--', linewidth=1)

    # Top-Right Max Trace Plot
    ax_max_trace.set_title("Max Trace", fontsize=9)
    ax_max_trace.set_ylabel("Max", fontsize=8)
    ax_max_trace.set_xticks([])
    ax_max_trace.tick_params(axis='y', labelsize=8)
    ax_max_trace.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax_max_trace.set_ylim(0, 65535)

    # --- CHANGE: Styling for the new "water fill" gauge ---
    ax_std.set_title("Avg. Norm. Std (2s window)", fontsize=9)
    # Set a fixed vertical scale from 0 to 1
    ax_std.set_ylim(0, 5) # in percent
    # The x-axis is not meaningful, so hide it
    ax_std.set_xticks([])
    ax_std.set_xlim(-0.5, 0.5)
    # Format y-axis labels to be more readable (e.g., 50k)
    ax_std.yaxis.set_major_formatter(mticker.PercentFormatter())
    ax_std.tick_params(labelsize=8)
    
    # --- Interaction Handlers ---
    def on_key(event):
        nonlocal ani
        if ani is None: return
        ani.event_source.stop()
        current_ylim = ax_spec.get_ylim()
        if event.key == 'up': new_max = current_ylim[1] * 1.2
        elif event.key == 'down': new_max = current_ylim[1] / 1.2
        else:
            ani.event_source.start()
            return
        ax_spec.set_ylim(0, max(1000, new_max))
        fig.canvas.draw()
        fig.canvas.flush_events()
        ani.event_source.start()

    fig.canvas.mpl_connect('key_press_event', on_key)

    def on_keep_clicked(event):
        data = cam.read_newest_image()
        if data is not None:
            spectrum = data.ravel().astype(np.uint16)
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            print("Spectrum kept.")

    button_ax = fig.add_axes([0.01, 0.92, 0.08, 0.06])
    keep_button = Button(button_ax, "Keep", color='lightgreen', hovercolor='green')
    keep_button.on_clicked(on_keep_clicked)
    
    # --- Add text boxes for Std Dev ROI control ---
    def on_roi_submit(text):
        nonlocal roi_indices
        try:
            min_val = float(text_box_min.text)
            max_val = float(text_box_max.text)
        except ValueError:
            print("Invalid ROI input. Please enter numbers.")
            return

        if min_val >= max_val:
            print("Min ROI must be less than Max ROI.")
            return

        roi_indices = np.where((energy_eV >= min_val) & (energy_eV <= max_val))[0]
        print(f"Std Dev ROI set to {min_val:.1f}-{max_val:.1f} eV.")

    def on_std_full_clicked(event):
        nonlocal roi_indices
        roi_indices = np.arange(energy_eV.shape[0])
        # Update text boxes to reflect the full range
        text_box_min.set_val(f"{energy_eV[0]:.1f}")
        text_box_max.set_val(f"{energy_eV[-1]:.1f}")
        print("Std Dev ROI set to full spectrum.")

    # Create text boxes for min and max ROI
    ax_roi_min = fig.add_axes([0.10, 0.92, 0.1, 0.06]) #xpos ypos width height
    text_box_min = TextBox(ax_roi_min, 'Min (eV)', initial=f"{xmin:.0f}", textalignment="right")
    text_box_min.label.set_horizontalalignment('left')
    text_box_min.on_submit(on_roi_submit)

    ax_roi_max = fig.add_axes([0.21, 0.92, 0.1, 0.06])
    text_box_max = TextBox(ax_roi_max, 'Max (eV)', initial=f"{xmax:.0f}", textalignment="right")
    text_box_max.label.set_horizontalalignment('left')
    text_box_max.on_submit(on_roi_submit)
    # Manually trigger the first ROI calculation with initial values
    on_roi_submit(None)

    std_full_ax = fig.add_axes([0.32, 0.92, 0.1, 0.06])
    std_full_button = Button(std_full_ax, "Std Dev (Full)", color='yellow', hovercolor='orange')
    std_full_button.on_clicked(on_std_full_clicked)

    #plt.tight_layout(rect=[0, 0, 1, 0.92])

    # --- Animation Core ---
    def update(frame):
        current_time = time.time()
        data = cam.read_newest_image()
        if data is None:
            return line, max_line, ref_line, std_dev_bar_patch

        spectrum = data.ravel().astype(np.uint16)
        if spectrum.shape[0] != energy_eV.shape[0]:
            print(f"Unexpected spectrum shape: {spectrum.shape}")
            return line, max_line, ref_line, std_dev_bar_patch

        # Update main spectrum plot
        line.set_ydata(spectrum)
        
        # Update max trace plot
        current_max = np.max(spectrum)
        max_vals_buffer.append(current_max)
        max_line.set_ydata(list(max_vals_buffer))
        max_line.set_xdata(np.arange(len(max_vals_buffer)))
        ax_max_trace.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))
        
        # --- CHANGE: Update Standard Deviation GAUGE ---
        # 1. Add full spectrum to 2-second buffer and prune old data
        spectrum_buffer_2s.append((current_time, spectrum))
        while spectrum_buffer_2s and (current_time - spectrum_buffer_2s[0][0] > 2):
            spectrum_buffer_2s.popleft()
            
        # 2. Calculate normalized std dev if buffer is sufficient
        avg_norm_std = 0.0
        if len(spectrum_buffer_2s) >= 2:
            # Create a 2D array of spectra over time
            spectra_over_time = np.array([item[1] for item in spectrum_buffer_2s])
            
            # Calculate std and mean for each energy bin (column-wise)
            std_per_energy = np.std(spectra_over_time, axis=0)
            mean_per_energy = np.mean(spectra_over_time, axis=0)
            
            # Calculate normalized std, avoiding division by zero
            normalized_std = np.divide(std_per_energy, mean_per_energy, 
                                      out=np.zeros_like(std_per_energy), 
                                      where=mean_per_energy!=0)
            
            # Average the normalized std over the ROI
            if roi_indices.size > 0:
                avg_norm_std = np.mean(normalized_std[roi_indices])
        avg_norm_std = avg_norm_std * 100 #Change units to percent
        # 3. Update the height of the bar patch
        std_dev_bar_patch.set_height(avg_norm_std)

        # Return a tuple of all artists that were modified
        return line, max_line, ref_line, std_dev_bar_patch

    ani = FuncAnimation(fig, update, interval=1, blit=True, cache_frame_data=False)
    plt.show()

    # --- Cleanup ---
    print("Plot window closed. Disconnecting camera...")
    #print("Thank you for choosing code from MarcelCorp. TM!")
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
    cam.clear_acquisition()
    cam.close()
    print("Camera disconnected.")

# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()
blob
mark :25
data 13197
import numpy as np
import time
import matplotlib
# Suggest a backend compatible with animations. 'QtAgg' is a robust choice.
matplotlib.use('QtAgg') 
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import matplotlib.ticker as mticker
from matplotlib.animation import FuncAnimation
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button, TextBox
import os

# --- Configuration ---
# A class to hold all settings for easy modification.
class Settings:
    """Holds static configuration parameters for the script."""
    # Exposure time in milliseconds. Note: In trigger mode, the camera waits for a trigger,
    # so this value is less critical than the trigger rate itself.
    EXP_TIME_MS = 1  
    # Binning settings (horizontal, vertical). Here, we bin all vertical pixels into one line.
    BINNING = (1, 400)
    # Expected shape of the spectrum data (rows, columns).
    SPECTRA_SHAPE = (1, 1340)
    # Path to the energy calibration file. Assumes it's in the same directory as the script.
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")

# --- Main Application Logic ---
def main():
    """Initializes the camera, sets up the plot, and runs the live display."""
    
    # --- 1. Initialization and Setup ---
    
    # Load the energy axis from the calibration file.
    try:
        energy_eV = np.loadtxt(Settings.ENERGY_FILE)
    except Exception as e:
        print(f"Error: Failed to load energy axis from '{Settings.ENERGY_FILE}'. {e}")
        return

    # Validate that the energy axis matches the expected spectrum shape.
    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Error: Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]}).")
        return

    # Initialize the Region of Interest (ROI) for standard deviation to the full spectrum.
    roi_indices = np.arange(energy_eV.shape[0])
    
    # Create a deque (a fast, double-ended queue) to buffer recent spectra for calculations.
    spectrum_buffer_2s = deque() 

    # Connect to the Princeton Instruments camera.
    try:
        print("Available cameras:", PrincetonInstruments.list_cameras())
        cam = PrincetonInstruments.PicamCamera('2105050003') # Replace with your camera's serial number if different.
        print("Camera connected successfully.")
    except Exception as e:
        print(f"Error: Could not connect to the camera. {e}")
        return

    # Configure camera acquisition settings.
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)
    time.sleep(0.2) # A brief pause to ensure settings are applied.
    
    # Start the acquisition sequence.
    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()
    print("Starting live display... (Close the plot window to stop)")

    # --- 2. Plot and Widget Layout ---

    # Create the main figure and a GridSpec layout for flexible subplot arrangement.
    fig = plt.figure(figsize=(12, 8))
    gs = GridSpec(2, 2, figure=fig, height_ratios=[1, 4], width_ratios=[14, 1])

    # Define the subplots within the grid.
    ax_spec = fig.add_subplot(gs[1, 0])      # Main spectrum plot (bottom left)
    ax_max_trace = fig.add_subplot(gs[0, 0])  # Max value trace plot (top left)
    ax_std_gauge = fig.add_subplot(gs[:, 1])   # Standard deviation gauge (right column)
    
    # --- 3. Plot Artists and Styling ---

    # Artists for the main spectrum plot.
    line, = ax_spec.plot(energy_eV, np.zeros_like(energy_eV), zorder=2, color='#0072BD', label="Live Spectrum")
    ref_line, = ax_spec.plot(energy_eV, np.zeros_like(energy_eV), color='#D95319', linestyle='--', linewidth=1, label="Kept Spectrum", zorder=1)
    ref_line.set_visible(False) # Hide the reference line initially.

    # Artists for the max value trace plot.
    max_vals_buffer = deque([0] * 200, maxlen=200) # Buffer for the last 200 max values.
    max_line, = ax_max_trace.plot(list(max_vals_buffer), color='#A2142F')
    
    # Artist for the standard deviation gauge (a single vertical bar).
    bar_container = ax_std_gauge.bar(0, 0, color='#33A02C', width=1.0)
    std_dev_bar_patch = bar_container[0]

    # --- Styling for the Spectrum Plot (ax_spec) ---
    ax_spec.set_title("Use up/down arrow keys to adjust Y-axis", loc='left')
    ax_spec.set_xlabel("Energy (eV)")
    ax_spec.set_ylabel("Counts")
    ax_spec.grid(True)
    initial_ylim = np.max(cam.read_newest_image().ravel().astype(np.uint16)) * 2
    ax_spec.set_ylim(0, initial_ylim)
    xmin, xmax = 20, 75 # Default X-axis limits.
    ax_spec.set_xlim(xmin, xmax)
    ax_spec.hlines(y=65535, xmin=xmin, xmax=xmax, colors='#7E2F8E', linestyles='--', linewidth=1, label="Saturation (16-bit)")
    ax_spec.legend()

    # --- Styling for the Max Trace Plot (ax_max_trace) ---
    ax_max_trace.set_title("Max Count Trace", fontsize=9)
    ax_max_trace.set_ylabel("Max", fontsize=8)
    ax_max_trace.set_xticks([]) # Hide x-axis ticks.
    ax_max_trace.tick_params(axis='y', labelsize=8)
    ax_max_trace.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax_max_trace.set_ylim(0, 65535)

    # --- Styling for the Standard Deviation Gauge (ax_std_gauge) ---
    ax_std_gauge.set_title("Avg. Norm. Std\n(2s window, %)", fontsize=9)
    ax_std_gauge.set_ylim(0, 5) # Set a fixed vertical scale in percent.
    ax_std_gauge.set_xlim(-0.5, 0.5) # Center the bar.
    ax_std_gauge.set_xticks([]) # Hide x-axis ticks.
    ax_std_gauge.yaxis.tick_right() # Move ticks and labels to the right.
    ax_std_gauge.yaxis.set_label_position("right")
    ax_std_gauge.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=100))
    ax_std_gauge.tick_params(labelsize=8)
    
    # --- 4. Interactive Widget Handlers ---

    def on_key(event):
        """Handles key presses to adjust the Y-axis of the spectrum plot."""
        nonlocal animation # Use nonlocal to modify the animation object defined in the outer scope.
        if animation is None: return
        
        animation.event_source.stop() # Pause animation to prevent redraw conflicts.
        current_ylim = ax_spec.get_ylim()
        if event.key == 'up':
            new_max = current_ylim[1] * 1.2
        elif event.key == 'down':
            new_max = current_ylim[1] / 1.2
        else:
            animation.event_source.start() # Resume for unhandled keys.
            return
        
        ax_spec.set_ylim(0, max(1000, new_max)) # Apply new limit, with a minimum floor.
        fig.canvas.draw_idle() # Redraw the canvas.
        animation.event_source.start() # Resume animation.

    def on_keep_clicked(event):
        """Saves the current spectrum as a reference line."""
        data = cam.read_newest_image()
        if data is not None:
            spectrum = data.ravel().astype(np.uint16)
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            print("Reference spectrum updated.")

    def on_roi_submit(text):
        """Updates the ROI for the standard deviation calculation from text boxes."""
        nonlocal roi_indices
        try:
            min_val = float(text_box_min.text)
            max_val = float(text_box_max.text)
        except ValueError:
            print("Invalid ROI input. Please enter numbers.")
            return

        if min_val >= max_val:
            print("Min ROI must be less than Max ROI.")
            return
        
        # Find the indices of the energy axis that fall within the specified range.
        roi_indices = np.where((energy_eV >= min_val) & (energy_eV <= max_val))[0]
        print(f"Std Dev ROI set to {min_val:.1f}-{max_val:.1f} eV.")

    def on_std_full_clicked(event):
        """Resets the standard deviation ROI to the full spectrum."""
        nonlocal roi_indices
        roi_indices = np.arange(energy_eV.shape[0])
        # Update text boxes to reflect the full range.
        text_box_min.set_val(f"{energy_eV[0]:.1f}")
        text_box_max.set_val(f"{energy_eV[-1]:.1f}")
        print("Std Dev ROI reset to full spectrum.")

    # --- Create and place widgets on the figure ---
    fig.canvas.mpl_connect('key_press_event', on_key)

    keep_button_ax = fig.add_axes([0.01, 0.92, 0.08, 0.06])
    keep_button = Button(keep_button_ax, "Keep", color='lightgreen', hovercolor='green')
    keep_button.on_clicked(on_keep_clicked)

    ax_roi_min = fig.add_axes([0.10, 0.92, 0.1, 0.06])
    text_box_min = TextBox(ax_roi_min, 'Min (eV)', initial=f"{xmin:.0f}", textalignment="right")
    text_box_min.label.set_horizontalalignment('left')
    text_box_min.on_submit(on_roi_submit)

    ax_roi_max = fig.add_axes([0.21, 0.92, 0.1, 0.06])
    text_box_max = TextBox(ax_roi_max, 'Max (eV)', initial=f"{xmax:.0f}", textalignment="right")
    text_box_max.label.set_horizontalalignment('left')
    text_box_max.on_submit(on_roi_submit)
    
    std_full_ax = fig.add_axes([0.32, 0.92, 0.1, 0.06])
    std_full_button = Button(std_full_ax, "Std Dev (Full)", color='yellow', hovercolor='orange')
    std_full_button.on_clicked(on_std_full_clicked)

    # Manually trigger the first ROI calculation with initial values from the text boxes.
    on_roi_submit(None)

    # --- 5. Animation Core ---
    def update(frame):
        """This function is called repeatedly to update the plot data."""
        current_time = time.time()
        
        # Read the latest image from the camera.
        data = cam.read_newest_image()
        if data is None:
            return line, max_line, ref_line, std_dev_bar_patch # Return unchanged artists if no data.

        spectrum = data.ravel().astype(np.uint16)
        if spectrum.shape[0] != energy_eV.shape[0]:
            print(f"Warning: Unexpected spectrum shape received: {spectrum.shape}")
            return line, max_line, ref_line, std_dev_bar_patch

        # --- Update Spectrum Plot ---
        line.set_ydata(spectrum)
        
        # --- Update Max Trace Plot ---
        max_vals_buffer.append(np.max(spectrum))
        max_line.set_ydata(list(max_vals_buffer))
        max_line.set_xdata(np.arange(len(max_vals_buffer)))
        # Dynamically adjust the y-axis of the max trace plot.
        ax_max_trace.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))
        
        # --- Update Standard Deviation Gauge ---
        # 1. Add the new spectrum to the 2-second buffer and remove old ones.
        spectrum_buffer_2s.append((current_time, spectrum))
        while spectrum_buffer_2s and (current_time - spectrum_buffer_2s[0][0] > 2):
            spectrum_buffer_2s.popleft()
            
        # 2. Calculate the average normalized standard deviation.
        avg_norm_std_percent = 0.0
        if len(spectrum_buffer_2s) >= 2: # Need at least two points to calculate std dev.
            # Stack spectra from the buffer into a 2D array (time x energy).
            spectra_over_time = np.array([item[1] for item in spectrum_buffer_2s])
            
            # Calculate standard deviation and mean for each energy bin (column-wise).
            std_per_energy = np.std(spectra_over_time, axis=0)
            mean_per_energy = np.mean(spectra_over_time, axis=0)
            
            # Calculate normalized std dev (std/mean), avoiding division by zero.
            normalized_std = np.divide(std_per_energy, mean_per_energy, 
                                      out=np.zeros_like(std_per_energy), 
                                      where=mean_per_energy != 0)
            
            # Average the normalized std dev over the user-defined ROI.
            if roi_indices.size > 0:
                avg_norm_std = np.mean(normalized_std[roi_indices])
                avg_norm_std_percent = avg_norm_std * 100 # Convert to percent for display.

        # 3. Update the height of the gauge bar.
        std_dev_bar_patch.set_height(avg_norm_std_percent)

        # Return a tuple of all artists that were modified for blitting.
        return line, max_line, ref_line, std_dev_bar_patch

    # Create and start the animation.
    animation = FuncAnimation(fig, update, interval=1, blit=True, cache_frame_data=False)
    plt.show()

    # --- 6. Cleanup ---
    print("Plot window closed. Disconnecting camera...")
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
    cam.clear_acquisition()
    cam.close()
    print("Camera disconnected.")

# --- Script Entry Point ---
if __name__ == "__main__":
    main()
blob
mark :26
data 13403
import numpy as np
import time
import matplotlib
# Suggest a backend compatible with animations. 'QtAgg' is a robust choice.
matplotlib.use('QtAgg') 
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import matplotlib.ticker as mticker
from matplotlib.animation import FuncAnimation
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button, TextBox
import os

# --- Configuration ---
# A class to hold all settings for easy modification.
class Settings:
    """Holds static configuration parameters for the script."""
    # Exposure time in milliseconds. Note: In trigger mode, the camera waits for a trigger,
    # so this value is less critical than the trigger rate itself.
    EXP_TIME_MS = 1  
    # Binning settings (horizontal, vertical). Here, we bin all vertical pixels into one line.
    BINNING = (1, 400)
    # Expected shape of the spectrum data (rows, columns).
    SPECTRA_SHAPE = (1, 1340)
    # Path to the energy calibration file. Assumes it's in the same directory as the script.
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")

# --- Main Application Logic ---
def main():
    """Initializes the camera, sets up the plot, and runs the live display."""
    
    # --- 1. Initialization and Setup ---
    
    # Load the energy axis from the calibration file.
    try:
        energy_eV = np.loadtxt(Settings.ENERGY_FILE)
    except Exception as e:
        print(f"Error: Failed to load energy axis from '{Settings.ENERGY_FILE}'. {e}")
        return

    # Validate that the energy axis matches the expected spectrum shape.
    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Error: Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]}).")
        return

    # Initialize the Region of Interest (ROI) for standard deviation to the full spectrum.
    roi_indices = np.arange(energy_eV.shape[0])
    
    # Create a deque (a fast, double-ended queue) to buffer recent spectra for calculations.
    spectrum_buffer_2s = deque() 

    # Connect to the Princeton Instruments camera.
    try:
        print("Available cameras:", PrincetonInstruments.list_cameras())
        cam = PrincetonInstruments.PicamCamera('2105050003') # Replace with your camera's serial number if different.
        print("Camera connected successfully.")
    except Exception as e:
        print(f"Error: Could not connect to the camera. {e}")
        return

    # Configure camera acquisition settings.
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)
    time.sleep(0.2) # A brief pause to ensure settings are applied.
    
    # Start the acquisition sequence.
    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()
    print("Starting live display... (Close the plot window to stop)")

    # --- 2. Plot and Widget Layout ---

    # Create the main figure and a GridSpec layout for flexible subplot arrangement.
    fig = plt.figure(figsize=(12, 8))
    gs = GridSpec(2, 2, figure=fig, height_ratios=[1, 4], width_ratios=[14, 1])

    # Define the subplots within the grid.
    ax_spec = fig.add_subplot(gs[1, 0])      # Main spectrum plot (bottom left)
    ax_max_trace = fig.add_subplot(gs[0, 0])  # Max value trace plot (top left)
    ax_std_gauge = fig.add_subplot(gs[:, 1])   # Standard deviation gauge (right column)
    
    # --- 3. Plot Artists and Styling ---

    # Artists for the main spectrum plot.
    line, = ax_spec.plot(energy_eV, np.zeros_like(energy_eV), zorder=2, color='#0072BD', label="Live Spectrum")
    ref_line, = ax_spec.plot(energy_eV, np.zeros_like(energy_eV), color='#D95319', linestyle='--', linewidth=1, label="Kept Spectrum", zorder=1)
    ref_line.set_visible(False) # Hide the reference line initially.

    # Artists for the max value trace plot.
    max_vals_buffer = deque([0] * 200, maxlen=200) # Buffer for the last 200 max values.
    max_line, = ax_max_trace.plot(list(max_vals_buffer), color='#A2142F')
    
    # Artist for the standard deviation gauge (a single vertical bar).
    bar_container = ax_std_gauge.bar(0, 0, color='#33A02C', width=1.0)
    std_dev_bar_patch = bar_container[0]

    # --- Styling for the Spectrum Plot (ax_spec) ---
    ax_spec.set_title("Use up/down arrow keys to adjust Y-axis", loc='left')
    ax_spec.set_xlabel("Energy (eV)")
    ax_spec.set_ylabel("Counts")
    ax_spec.grid(True)
    initial_ylim = np.max(cam.read_newest_image().ravel().astype(np.uint16)) * 2
    ax_spec.set_ylim(0, initial_ylim)
    xmin, xmax = 20, 75 # Default X-axis limits.
    ax_spec.set_xlim(xmin, xmax)
    ax_spec.hlines(y=65535, xmin=xmin, xmax=xmax, colors='#7E2F8E', linestyles='--', linewidth=1, label="Saturation (16-bit)")
    ax_spec.legend()

    # --- Styling for the Max Trace Plot (ax_max_trace) ---
    ax_max_trace.set_title("Max Count Trace", fontsize=9)
    ax_max_trace.set_ylabel("Max", fontsize=8)
    ax_max_trace.set_xticks([]) # Hide x-axis ticks.
    ax_max_trace.tick_params(axis='y', labelsize=8)
    ax_max_trace.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax_max_trace.set_ylim(0, 65535)

    # --- Styling for the Standard Deviation Gauge (ax_std_gauge) ---
    ax_std_gauge.set_title("Avg. Norm. Std\n(2s window, %)", fontsize=9)
    ax_std_gauge.set_ylim(0, 5) # Set a fixed vertical scale in percent.
    ax_std_gauge.set_xlim(-0.5, 0.5) # Center the bar.
    ax_std_gauge.set_xticks([]) # Hide x-axis ticks.
    ax_std_gauge.yaxis.tick_right() # Move ticks and labels to the right.
    ax_std_gauge.yaxis.set_label_position("right")
    ax_std_gauge.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=100))
    ax_std_gauge.tick_params(labelsize=8)
    
    # --- 4. Interactive Widget Handlers ---

    def on_key(event):
        """Handles key presses to adjust the Y-axis of the spectrum plot."""
        nonlocal animation # Use nonlocal to modify the animation object defined in the outer scope.
        if animation is None: return
        
        animation.event_source.stop() # Pause animation to prevent redraw conflicts.
        current_ylim = ax_spec.get_ylim()
        if event.key == 'up':
            new_max = current_ylim[1] * 1.2
        elif event.key == 'down':
            new_max = current_ylim[1] / 1.2
        else:
            animation.event_source.start() # Resume for unhandled keys.
            return
        
        ax_spec.set_ylim(0, max(1000, new_max)) # Apply new limit, with a minimum floor.
        fig.canvas.draw_idle() # Redraw the canvas.
        animation.event_source.start() # Resume animation.

    def on_keep_clicked(event):
        """Saves the current spectrum as a reference line."""
        data = cam.read_newest_image()
        if data is not None:
            spectrum = data.ravel().astype(np.uint16)
            ref_line.set_ydata(spectrum.copy())
            ref_line.set_visible(True)
            print("Reference spectrum updated.")

    def on_roi_submit(text):
        """Updates the ROI for the standard deviation calculation from text boxes."""
        nonlocal roi_indices
        try:
            min_val = float(text_box_min.text)
            max_val = float(text_box_max.text)
        except ValueError:
            print("Invalid ROI input. Please enter numbers.")
            return

        if min_val >= max_val:
            print("Min ROI must be less than Max ROI.")
            return
        
        # Find the indices of the energy axis that fall within the specified range.
        roi_indices = np.where((energy_eV >= min_val) & (energy_eV <= max_val))[0]
        print(f"Std Dev ROI set to {min_val:.1f}-{max_val:.1f} eV.")

    def on_std_full_clicked(event):
        """Resets the standard deviation ROI to the full spectrum."""
        nonlocal roi_indices
        roi_indices = np.arange(energy_eV.shape[0])
        # Update text boxes to reflect the full range.
        text_box_min.set_val(f"{energy_eV[0]:.1f}")
        text_box_max.set_val(f"{energy_eV[-1]:.1f}")
        print("Std Dev ROI reset to full spectrum.")

    # --- Create and place widgets on the figure ---
    fig.canvas.mpl_connect('key_press_event', on_key)

    keep_button_ax = fig.add_axes([0.01, 0.92, 0.08, 0.06])
    keep_button = Button(keep_button_ax, "Keep", color='lightgreen', hovercolor='green')
    keep_button.on_clicked(on_keep_clicked)

    ax_roi_min = fig.add_axes([0.10, 0.92, 0.1, 0.06])
    text_box_min = TextBox(ax_roi_min, 'Min (eV)', initial=f"{xmin:.0f}", textalignment="right")
    text_box_min.label.set_horizontalalignment('left')
    text_box_min.on_submit(on_roi_submit)

    ax_roi_max = fig.add_axes([0.21, 0.92, 0.1, 0.06])
    text_box_max = TextBox(ax_roi_max, 'Max (eV)', initial=f"{xmax:.0f}", textalignment="right")
    text_box_max.label.set_horizontalalignment('left')
    text_box_max.on_submit(on_roi_submit)
    
    std_full_ax = fig.add_axes([0.32, 0.92, 0.1, 0.06])
    std_full_button = Button(std_full_ax, "Std Dev (Full)", color='yellow', hovercolor='orange')
    std_full_button.on_clicked(on_std_full_clicked)

    # Manually trigger the first ROI calculation with initial values from the text boxes.
    on_roi_submit(None)

    # --- 5. Animation Core ---
    def update(frame):
        """This function is called repeatedly to update the plot data."""
        current_time = time.time()
        
        # Read the latest image from the camera.
        data = cam.read_newest_image()
        if data is None:
            return line, max_line, ref_line, std_dev_bar_patch # Return unchanged artists if no data.

        spectrum = data.ravel().astype(np.uint16)
        if spectrum.shape[0] != energy_eV.shape[0]:
            print(f"Warning: Unexpected spectrum shape received: {spectrum.shape}")
            return line, max_line, ref_line, std_dev_bar_patch

        # --- Update Spectrum Plot ---
        line.set_ydata(spectrum)
        
        # --- Update Max Trace Plot ---
        max_vals_buffer.append(np.max(spectrum))
        max_line.set_ydata(list(max_vals_buffer))
        max_line.set_xdata(np.arange(len(max_vals_buffer)))
        # Dynamically adjust the y-axis of the max trace plot.
        ax_max_trace.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))
        
        # --- Update Standard Deviation Gauge ---
        # 1. Add the new spectrum to the 2-second buffer and remove old ones.
        spectrum_buffer_2s.append((current_time, spectrum))
        while spectrum_buffer_2s and (current_time - spectrum_buffer_2s[0][0] > 2):
            spectrum_buffer_2s.popleft()
        #print(rf"Sensor Temperature Set Point: {cam.get_attribute_value('Sensor Temperature Set Point')} K")
        #print(rf"Sensor Temperature Reading: {cam.get_attribute_value('Sensor Temperature Reading')} K")  
        # 2. Calculate the average normalized standard deviation.
        avg_norm_std_percent = 0.0
        if len(spectrum_buffer_2s) >= 2: # Need at least two points to calculate std dev.
            # Stack spectra from the buffer into a 2D array (time x energy).
            spectra_over_time = np.array([item[1] for item in spectrum_buffer_2s])
            
            # Calculate standard deviation and mean for each energy bin (column-wise).
            std_per_energy = np.std(spectra_over_time, axis=0)
            mean_per_energy = np.mean(spectra_over_time, axis=0)
            
            # Calculate normalized std dev (std/mean), avoiding division by zero.
            normalized_std = np.divide(std_per_energy, mean_per_energy, 
                                      out=np.zeros_like(std_per_energy), 
                                      where=mean_per_energy != 0)
            
            # Average the normalized std dev over the user-defined ROI.
            if roi_indices.size > 0:
                avg_norm_std = np.mean(normalized_std[roi_indices])
                avg_norm_std_percent = avg_norm_std * 100 # Convert to percent for display.

        # 3. Update the height of the gauge bar.
        std_dev_bar_patch.set_height(avg_norm_std_percent)

        # Return a tuple of all artists that were modified for blitting.
        return line, max_line, ref_line, std_dev_bar_patch

    # Create and start the animation.
    animation = FuncAnimation(fig, update, interval=1, blit=True, cache_frame_data=False)
    plt.show()

    # --- 6. Cleanup ---
    print("Plot window closed. Disconnecting camera...")
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
    cam.clear_acquisition()
    cam.close()
    print("Camera disconnected.")

# --- Script Entry Point ---
if __name__ == "__main__":
    main()
blob
mark :27
data 11764
import numpy as np
import os
import time
import json
from datetime import datetime
from pynput import keyboard
from pylablib.devices import PrincetonInstruments
import gc

# Global stop flag
stop_loop = False

# PATHS ########################################################################
class Paths:
    BASE_DIR = r"C:\Users\Moritz\Desktop\TESTDATA"


# DIRECTORY AND FILE MANAGEMENT ###############################################
def create_data_directory_and_paths():
    """
    Creates the directory structure: base_dir/YYYY/STRA/YYMMDD/YYMMDD_XXX/
    Returns the directory path and base filename (without extension).
    Automatically increments XXX if directory already exists.
    """
    now = datetime.now()
    year = now.strftime("%Y")
    date_str = now.strftime("%y%m%d")
    
    # Create the base directory structure
    year_dir = os.path.join(Paths.BASE_DIR, year)
    stra_dir = os.path.join(year_dir, "STRA")
    date_dir = os.path.join(stra_dir, date_str)
    
    # Create directories if they don't exist
    os.makedirs(date_dir, exist_ok=True)
    
    # Find the next available sequence number
    sequence_num = 1
    while True:
        sequence_str = f"{date_str}_{sequence_num:03d}"
        final_dir = os.path.join(date_dir, sequence_str)
        
        if not os.path.exists(final_dir):
            os.makedirs(final_dir, exist_ok=True)
            break
        
        sequence_num += 1
        
        # Safety check to prevent infinite loop
        if sequence_num > 999:
            raise ValueError("Too many acquisitions for this date (>999)")
    
    base_filename = sequence_str
    return final_dir, base_filename

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 25
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    NUMBER_OF_IMAGES = int(300e3)

    ACQUISITION_TIME_VIOLATION_THRESHOLD_MS = EXP_TIME_MS*1.8


# MAIN FUNCTION ################################################################
def main():
    global stop_loop

    # Start the keyboard listener
    listener = keyboard.Listener(on_press=on_press)
    listener.start()

    # Create directory structure and get file paths
    data_dir, base_filename = create_data_directory_and_paths()
    print(f"Data directory: {data_dir}")
    print(f"Base filename: {base_filename}")

    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(f"Timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Save camera attributes BEFORE acquisition
    all_attrs_at_start = cam.get_all_attribute_values()

    # Backup original attributes
    original_roi = cam.get_roi()
    original_trigger_det = cam.get_attribute_value("Trigger Determination")
    original_trigger_resp = cam.get_attribute_value("Trigger Response")
    original_shutter_delay = cam.get_attribute_value("Shutter Closing Delay")
    original_delay_res = cam.get_attribute_value("Shutter Delay Resolution")
    original_shutter_mode = cam.get_attribute_value("Shutter Timing Mode")

    # Setup camera for acquisition
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    print(f"[SET] Exposure Time: {cam.get_attribute_value('Exposure Time')} ms")

    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    print_roi(cam)

    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    print(f"[SET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    print(f"[SET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    cam.set_attribute_value("Clean Until Trigger", False)
    print(f"[SET] Clean Until Trigger: {cam.get_attribute_value('Clean Until Trigger')}")

    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    print(f"[SET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

    cam.set_attribute_value("Shutter Closing Delay", 0)
    print(f"[SET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

    time.sleep(0.2)

    # Save attributes after setup
    all_attrs_measurement = cam.get_all_attribute_values()

    # Set up acquisition
    cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES)

    # Structured dtype for mmap
    dtype = np.dtype([
        ("spectrum", np.uint16, Settings.SPECTRA_SHAPE[1]),
        ("timestamp_us", np.uint64)
    ])
    mmap_path = os.path.join(data_dir, f"{base_filename}.npy")
    mmap = np.memmap(mmap_path, dtype=dtype, mode="w+", shape=(Settings.NUMBER_OF_IMAGES,))
    mmap[:] = 0
    mmap.flush()

    # Save metadata
    start_time_unix = time.time()
    metadata = {
        "timestamp": timestamp,
        "exp_time_ms": Settings.EXP_TIME_MS,
        "binning": Settings.BINNING,
        "spectra_shape": Settings.SPECTRA_SHAPE,
        "number_of_images_planned": Settings.NUMBER_OF_IMAGES,
        "start_time_unix": start_time_unix,
        "memmap_file": os.path.basename(mmap_path),
        "dtype": {
            "spectrum": "uint16",
            "timestamp_us": "uint64"
        },
        "camera_attributes_at_start": [list(item) for item in all_attrs_at_start.items()],
        "camera_attributes_measurement": [list(item) for item in all_attrs_measurement.items()]
    }
    metadata_path = os.path.join(data_dir, f"{base_filename}.json")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=4)

    # Start acquisition
    print("Starting acquisition... (Press 'Esc' to stop early)")
    cam.start_acquisition()

    violations = []
    t_prev = time.time()
    i = 0
    # run main acquisition loop ########################################################
    try:
        while i < Settings.NUMBER_OF_IMAGES:
            data = cam.read_oldest_image()

            if data is None:
                time.sleep(Settings.EXP_TIME_MS / 1000 / 20)
                continue

            t_now = time.time()
            timestamp_us = int(t_now * 1e6)
            # Efficient max value extraction
            max_val = data.max()

            mmap[i] = (data.ravel().astype(np.uint16), timestamp_us)

            if i % 100 == 0:
                mmap.flush()
                print(f"Image {i} flushed.")

            dt = t_now - t_prev

            # print status:
            print(rf"Image no {i} acquired in {dt:.4f} s, max value: {max_val}")

            if dt > Settings.ACQUISITION_TIME_VIOLATION_THRESHOLD_MS / 1000:
                print(f"ACQUISITION TIME VIOLATION at {i}: {dt:.4f}s --------------------------------------")
                violations.append(i)

            t_prev = t_now

            if stop_loop:
                print("User interrupted acquisition with 'Esc'.")
                break
            
            i += 1

    finally:
        # Final flush
        mmap.flush()
        del mmap
        gc.collect()

        # remove zeros from memmap
        print("Cleaning memmap...")
        load_and_clean_memmap(mmap_path, Settings.SPECTRA_SHAPE[1])

        # Stop acquisition
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()

        # Reset original camera settings
        print("\nResetting camera to original settings:")
        cam.set_roi(*original_roi)
        print_roi(cam)

        cam.set_attribute_value("Trigger Determination", original_trigger_det)
        print(f"[RESET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

        cam.set_attribute_value("Trigger Response", original_trigger_resp)
        print(f"[RESET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

        cam.set_attribute_value("Shutter Closing Delay", original_shutter_delay)
        print(f"[RESET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

        cam.set_attribute_value("Shutter Delay Resolution", original_delay_res)
        print(f"[RESET] Shutter Delay Resolution: {cam.get_attribute_value('Shutter Delay Resolution')}")

        cam.set_attribute_value("Shutter Timing Mode", original_shutter_mode)
        print(f"[RESET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

        cam.close()
        print("Camera connection closed.")

        # Finalize metadata
        metadata["images_acquired"] = i
        metadata["violations"] = violations
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=4)

        print(f"Acquisition complete. Images acquired: {i}, Number of Violations: {len(violations)}")
        print(rf"Violations at: {violations}")


# PRINT ROI HELPER ############################################################
def print_roi(cam) -> None:
    roi = cam.get_roi()
    print("ROI settings:")
    print(f"  Horizontal start:  {roi[0]}")
    print(f"  Horizontal end:    {roi[1]}")
    print(f"  Vertical start:    {roi[2]}")
    print(f"  Vertical end:      {roi[3]}")
    print(f"  Horizontal binning:{roi[4]}")
    print(f"  Vertical binning:  {roi[5]}")


# PYNPUT CALLBACK #############################################################
def on_press(key):
    global stop_loop
    try:
        if key == keyboard.Key.esc:
            print("Detected 'Esc' key, breaking the loop.")
            stop_loop = True
            return False  # Stop the listener
    except AttributeError:
        pass

# CLEAR ZEROS FROM MEMMAP #######################################################
def load_and_clean_memmap(file_path: str, spectrum_length: int) -> None:
    """
    Loads the memmap, removes all-zero rows, and overwrites the original file safely via a temp file.

    :param file_path: Path to the .npy file.
    :param spectrum_length: Length of the spectrum.
    """
    import numpy as np
    import os
    import tempfile
    import gc

    print(f"Loading memmap from: {file_path}")

    dtype = np.dtype([
        ("intensities", np.uint16, spectrum_length),
        ("timestamp_us", np.uint64)
    ])

    # Step 1: Open and filter data
    mmap = np.memmap(file_path, dtype=dtype, mode="r")
    nonzero_mask = ~(
        (mmap["timestamp_us"] == 0) &
        (np.all(mmap["intensities"] == 0, axis=1))
    )
    cleaned_data = mmap[nonzero_mask].copy()  # Load into RAM
    print(f"Original rows: {len(mmap)}, Non-zero rows: {len(cleaned_data)}")

    # Step 2: Fully release original mmap (important on Windows)
    if hasattr(mmap, '_mmap'):
        mmap._mmap.close()
    del mmap
    gc.collect()

    # Step 3: Write to a temporary file
    temp_fd, temp_path = tempfile.mkstemp(dir=os.path.dirname(file_path))
    os.close(temp_fd)

    cleaned_mmap = np.memmap(temp_path, dtype=dtype, mode="w+", shape=(len(cleaned_data),))
    cleaned_mmap[:] = cleaned_data
    cleaned_mmap.flush()

    if hasattr(cleaned_mmap, '_mmap'):
        cleaned_mmap._mmap.close()
    del cleaned_mmap
    gc.collect()

    # Step 4: Atomically replace original file
    os.replace(temp_path, file_path)

    print(f"Cleaned memmap saved to: {file_path}")


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()

blob
mark :28
data 24499
# -*- coding: utf-8 -*-
"""
This script processes raw data from an Attosecond Transient Absorption
Spectroscopy (ATAS) experiment. It focuses on synchronising XUV spectra
with delay stage positions based on timestamps.

The main steps are:
1.  Load XUV spectra, XUV timestamps, and delay stage trigger timestamps.
2.  Identify synchronisation patterns within the XUV timestamps. These patterns
    correspond to the start of each delay scan.
3.  Segment the continuous XUV data into chunks, where each chunk represents
    one full scan of the delay stage.
4.  Within each chunk, label individual spectra as 'ON' (pump and probe beams
    are present), 'OFF' (only probe beam is present), or 'DISCARDED' based on
    the known experimental sequence.
5.  Generate and display the final ATAS trace (ΔA) by processing the labelled
    'ON' and 'OFF' spectra.
"""
import json
import os
import re
import sys
import warnings
from enum import IntEnum
from typing import Any, Dict, List, Optional, Tuple, cast

import matplotlib.pyplot as plt
import numpy as np

# Assuming the 'misc_functions' module is in the specified path and contains the necessary plotting functions.
sys.path.append('C:\\Users\\Moritz\\OneDrive - ETH Zurich\\Code for ThinkPad to test!')
from misc_functions import atas_on_off_clas, plot_on_off_clas_shift_sweep, distance_mm_to_delay_fs


class Paths:
    """Defines the main directory paths for input data."""
    BASE_DIR = r"C:\Users\Moritz\Desktop\TESTDATA"
    DELAY_STAGE_TIMES = r"Z:\\Personal\\MoritzJ\\10_measurements\\04_Delay_Stage"
    # The folder where all generated plots and data will be stored.
    OUTPUT_FOLDER = r"C:\Users\Moritz\Desktop\Pixis_data\output"


def find_xuv_files(yymmdd: str, sequence_num: int) -> Tuple[str, str]:
    """
    Find XUV .npy and .json files based on the new directory structure.
    
    Args:
        yymmdd: Date in YYMMDD format (e.g., "250903")
        sequence_num: Sequence number (e.g., 1 for _001)
        
    Returns:
        Tuple of (json_path, npy_path)
    """
    year = "20" + yymmdd[:2]  # Convert YY to YYYY
    sequence_str = f"{yymmdd}_{sequence_num:03d}"
    
    # Build the directory path
    data_dir = os.path.join(Paths.BASE_DIR, year, "STRA", yymmdd, sequence_str)
    
    json_path = os.path.join(data_dir, f"{sequence_str}.json")
    npy_path = os.path.join(data_dir, f"{sequence_str}.npy")
    
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"JSON file not found: {json_path}")
    if not os.path.exists(npy_path):
        raise FileNotFoundError(f"NPY file not found: {npy_path}")
        
    return json_path, npy_path

class TrainingSets:
    """
    Contains configurations for different experimental measurement sets.
    Each dictionary defines file locations and processing parameters.
    """
    TS0028 = {
        "Name": "TS0028",
        "Comment": "28ms acquisition time, ATAS, stab off",
        "XUV_date": "250903",  # YYMMDD format
        "XUV_sequence": 3,     # X for YYMMDD_00X
        "DELAY_STAGE_date": "2025_07_24-21_55_56",
        "XUV_eV_calibration_file": "Spec.txt",  # Always the same
        # Gaps between sync pulses in microseconds. Always the same pattern.
        "SYNC_GAPS_PATTERN": [1, 4, 3, 3, 1],
        # Tolerance for detecting the sync pattern gaps.
        "SYNC_PATTERN_TOLERANCE_US": 40000,
        # Number of initial pulses in a chunk to discard (related to sync sequence).
        "SYNC_PULSES_TO_SKIP": 3,
        # Number of pulses to discard before the first shutter action begins.
        "PULSES_TO_SKIP_BEFORE_FIRST_SHUTTER": 5,
        # Manual shift of the ON/OFF labelling, in units of pulses.
        "SHIFT": -1,
    }

    # --- Select the desired training set for processing here ---
    SELECTED = TS0028
    
    @staticmethod
    def create_training_set(name: str, comment: str, xuv_date: str, xuv_sequence: int, 
                          delay_stage_date: str, sync_pulses_to_skip: int = 3,
                          pulses_to_skip_before_first_shutter: int = 5, shift: int = -1) -> dict:
        """
        Helper function to create a new training set with default values.
        
        Args:
            name: Name of the training set
            comment: Description of the experiment
            xuv_date: Date in YYMMDD format
            xuv_sequence: Sequence number (1 for _001, 2 for _002, etc.)
            delay_stage_date: Delay stage timestamp format
            sync_pulses_to_skip: Number of sync pulses to skip
            pulses_to_skip_before_first_shutter: Number of pulses to skip before shutter
            shift: Manual shift for ON/OFF labelling
        
        Returns:
            Dictionary with training set configuration
        """
        return {
            "Name": name,
            "Comment": comment,
            "XUV_date": xuv_date,
            "XUV_sequence": xuv_sequence,
            "DELAY_STAGE_date": delay_stage_date,
            "XUV_eV_calibration_file": "Spec.txt",  # Always the same
            "SYNC_GAPS_PATTERN": [1, 4, 3, 3, 1],  # Always the same
            "SYNC_PATTERN_TOLERANCE_US": 40000,
            "SYNC_PULSES_TO_SKIP": sync_pulses_to_skip,
            "PULSES_TO_SKIP_BEFORE_FIRST_SHUTTER": pulses_to_skip_before_first_shutter,
            "SHIFT": shift,
        }

class Settings:
    """Contains global settings and flags for the script's execution."""
    # Plot an overview of all timestamps and detected sync points. Useful for debugging timing issues.
    PLOT_TIMING_OVERVIEW = False
    # Plot the final on/off spectra and the calculated ATAS trace.
    PLOT_ON_OFF_SPECTRUM = True


class DataLabel(IntEnum):
    """Enumeration for labelling each acquired data point."""
    OFF = 0  # Pump beam blocked
    ON = 1  # Pump and probe beams present
    TRAINING = 2  # Data not used for ON/OFF, potentially for ML
    DISCARDED = 3  # Data to be ignored entirely


def get_metadata(json_path: str) -> Dict:
    """Loads metadata from a specified JSON file.

    Args:
        json_path: Path to the metadata JSON file.

    Returns:
        A dictionary containing the metadata.
    """
    with open(json_path, "r") as f:
        metadata = json.load(f)
    return metadata


def load_xuv_spectra_and_timestamps(json_path: str) -> Tuple[np.ndarray, np.ndarray]:
    """Loads XUV spectra and their timestamps from a memory-mapped file.

    Args:
        json_path: Path to the metadata JSON file that describes the memmap file.

    Returns:
        A tuple containing:
        - A 2D NumPy array of spectra (N_frames, N_pixels).
        - A 1D NumPy array of timestamps in microseconds.
    """
    metadata = get_metadata(json_path)
    print('the json path is at ', json_path)
    memmap_path = os.path.join(os.path.dirname(json_path), metadata["memmap_file"])

    # Define the structured data type for the memory-mapped file
    dtype = np.dtype([
        ("spectrum", np.uint16, metadata["spectra_shape"][1]),
        ("timestamp_us", np.uint64)
    ])

    memmap_path = "C:\\Users\\Moritz\\Desktop\\TESTDATA\\2025\\STRA\\250903\\250903_003\\250903_003.npy"
    print(memmap_path)
    print("-------------")
    # Read the data and convert from memmap to standard numpy arrays
    mmap = np.memmap(memmap_path, dtype=dtype, mode="r")
    return np.array(mmap["spectrum"]), np.array(mmap["timestamp_us"])


def load_delay_stage_data(filename: str) -> Tuple[np.ndarray, int, int, int, Optional[float]]:
    """Reads delay stage data, extracting timestamps and experimental parameters from the filename.

    Args:
        filename: Path to the .npz file containing delay stage timestamps.

    Returns:
        A tuple containing:
        - timestamps_us (np.ndarray): Trigger timestamps in microseconds.
        - ppas (int): Pulses per acquisition step.
        - spss (int): Spectra per shutter step (duration of ON or OFF block).
        - spds (int): Steps per delay scan (number of ON/OFF pairs).
        - delay_stepsize_mm (float | None): Delay step size in mm, if found.
    """
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File not found: {filename}")

    # Load timestamp data from the memmap file, filtering out any zero entries.
    mmap_data = np.memmap(filename, dtype='int64', mode='r')
    timestamps_us = np.array(mmap_data[mmap_data > 0])

    # Use regex to parse experimental parameters from the filename string.
    match = re.search(r"ppas_(\d+)_spss_(\d+)_spds_(\d+)", filename)
    if not match:
        raise ValueError("Could not find ppas, spss, and spds in delay stage filename.")
    ppas = int(match.group(1))
    spss = int(match.group(2))
    spds = int(match.group(3))

    delay_match = re.search(r"step_([\d\.]+)mm", filename)
    delay_stepsize_mm = float(delay_match.group(1)) if delay_match else None

    return timestamps_us, ppas, spss, spds, delay_stepsize_mm


def find_sync_patterns(timestamps_us: np.ndarray, expected_gaps_us: List[int], tolerance_us: int) -> np.ndarray:
    """Detects synchronisation patterns in a timestamp array.

    A pattern is defined by a specific sequence of time gaps between consecutive timestamps.

    Args:
        timestamps_us: An array of timestamps in microseconds.
        expected_gaps_us: A list of the expected time gaps that form the sync pattern.
        tolerance_us: The allowed deviation in microseconds for each gap.

    Returns:
        An array of indices, where each index points to the start of a detected sync pattern.
    """
    sync_indices = []
    num_gaps = len(expected_gaps_us)
    i = 0
    # Iterate through the timestamps to find the pattern
    while i <= len(timestamps_us) - (num_gaps + 1):
        # Get a slice of timestamps that could form one pattern
        t_slice = timestamps_us[i:i + num_gaps + 1]
        gaps = np.diff(t_slice)

        # Check if all calculated gaps match the expected gaps within the tolerance
        if all(np.abs(g - e) < tolerance_us for g, e in zip(gaps, expected_gaps_us)):
            sync_indices.append(i)
            # Skip forward to prevent re-detecting overlapping patterns.
            # A cooldown of num_gaps is a safe choice.
            i += num_gaps
        else:
            i += 1
    return np.array(sync_indices)


def create_on_off_mask(chunk_length: int, spss: int, spds: int,
                       sync_pulses_to_skip: int, pulses_to_skip: int,
                       shift: int) -> np.ndarray:
    """Creates a mask to label each data point in a chunk as ON, OFF, TRAINING, or DISCARDED.

    Args:
        chunk_length: The total number of data points in the chunk.
        spss: Spectra per shutter step (duration of an ON or OFF block).
        spds: Steps per delay scan (number of ON/OFF pairs).
        sync_pulses_to_skip: Number of initial pulses to label as DISCARDED.
        pulses_to_skip: Number of subsequent pulses to label as TRAINING.
        shift: A manual timing shift to apply to the ON/OFF blocks.

    Returns:
        A 1D NumPy array of labels (see DataLabel enum) for the chunk.
    """
    # Start by labelling everything as TRAINING.
    mask = np.full(chunk_length, DataLabel.TRAINING, dtype=np.uint8)

    # Label initial pulses for synchronisation and stabilisation.
    mask[:sync_pulses_to_skip] = DataLabel.DISCARDED
    mask[sync_pulses_to_skip:pulses_to_skip] = DataLabel.TRAINING

    # Apply the ON/OFF pattern for each delay step.
    current_index = pulses_to_skip + shift
    for _ in range(spds):
        # ON block
        mask[current_index:current_index + spss] = DataLabel.ON
        current_index += spss
        # OFF block
        mask[current_index:current_index + spss] = DataLabel.OFF
        current_index += spss
    return mask

# def prepare_atas_inputs(selected_set_name: Optional[str] = None) -> Dict[str, object]:
#     """Prepare inputs for ATAS computation without plotting.

#     This function mirrors the loading, synchronization, chunking, and labeling
#     steps from main(), but returns the assembled arrays and metadata in a dict
#     so that MATLAB (MoritzReader.m) can compute and plot ATAS.

#     Args:
#         selected_set_name: Optional name of the training set to use (e.g., "TS0028").
#                            If None, uses TrainingSets.SELECTED.

#     Returns:
#         A dictionary with keys:
#             - 'final_spectra' (np.ndarray, float32) [N x E]
#             - 'final_identifiers' (np.ndarray, int32) [N x 3]
#             - 'xuv_energy_ev' (np.ndarray, float32) [E]
#             - 'delay_stepsize_mm' (float)
#             - 'ppas' (int), 'spss' (int), 'spds' (int)
#             - 'config_name' (str)
#     """
#     # Resolve configuration
#     config: Dict[str, Any]
#     if selected_set_name is None:
#         config = cast(Dict[str, Any], TrainingSets.SELECTED)
#     else:
#         # Allow matching by attribute name (e.g., "TS0028") or by config['Name']
#         found: Optional[Dict[str, Any]] = None
#         for attr in dir(TrainingSets):
#             if attr.startswith("TS"):
#                 cfg = getattr(TrainingSets, attr)
#                 if isinstance(cfg, dict) and (attr == selected_set_name or cfg.get("Name") == selected_set_name):
#                     found = cast(Dict[str, Any], cfg)
#                     break
#         if found is None:
#             available = [a for a in dir(TrainingSets) if a.startswith("TS")]
#             raise ValueError(f"Unknown training set '{selected_set_name}'. Available: {available}")
#         config = found

#     # Locate files
#     # xuv_data_file = [f for f in os.listdir(Paths.XUV_SPECTRA) if f.startswith(config["XUV_date"]) and f.endswith(".npy")]
#     # xuv_meta_file = [f for f in os.listdir(Paths.XUV_SPECTRA) if f.startswith(config["XUV_date"]) and f.endswith(".json")]
#     # delay_file = [f for f in os.listdir(Paths.DELAY_STAGE_TIMES) if f.startswith(config["DELAY_STAGE_date"]) and f.endswith(".npz")]
#     # if not (len(xuv_data_file) == 1 and len(xuv_meta_file) == 1 and len(delay_file) == 1):
#     #     raise FileNotFoundError("Could not find exactly one file for each data type (XUV data, XUV meta, Delay).")

#     # Load data
#     # xuv_spectra, xuv_timestamps_us = load_xuv_spectra_and_timestamps(os.path.join(Paths.XUV_SPECTRA, xuv_meta_file[0]))
#     # delay_timestamps_us, ppas, spss, spds, delay_stepsize_mm = load_delay_stage_data(os.path.join(Paths.DELAY_STAGE_TIMES, delay_file[0]))
#     # xuv_energy_ev = np.loadtxt(os.path.join(Paths.XUV_SPECTRA, config["XUV_eV_calibration_file"]))
    


#     # Synchronisation: detect start of scans via sync gaps pattern
#     # sync_gaps_us: List[int] = [int(round(float(element) * (ppas * (1 / 1030) * 1e6))) for element in config["SYNC_GAPS_PATTERN"]]
#     # sync_indices = find_sync_patterns(xuv_timestamps_us, sync_gaps_us, config["SYNC_PATTERN_TOLERANCE_US"]) + 2

#     # Segment into chunks (one per delay scan)
#     chunk_indices = np.split(np.arange(len(xuv_spectra)), sync_indices)[1:]
#     chunk_lengths = [len(c) for c in chunk_indices]
#     if len(chunk_lengths) == 0:
#         raise RuntimeError("No chunks detected from XUV sync patterns; check timestamps or pattern settings.")

#     # Build ON/OFF mask using first complete chunk length
#     chunk_length = chunk_lengths[0]
#     on_off_mask = create_on_off_mask(
#         chunk_length=chunk_length,
#         spss=spss,
#         spds=spds,
#         sync_pulses_to_skip=config["SYNC_PULSES_TO_SKIP"],
#         pulses_to_skip=config["PULSES_TO_SKIP_BEFORE_FIRST_SHUTTER"],
#         shift=config["SHIFT"],
#     )

#     final_spectra: List[np.ndarray] = []
#     # identifiers: (DataLabel, chunk_index, on_off_block_index)
#     final_identifiers: List[Tuple[int, int, int]] = []

#     for chunk_idx, indices in enumerate(chunk_indices):
#         if len(indices) < chunk_length:
#             # skip incomplete last chunk
#             continue
#         block_counter = -1
#         prev_label = None
#         for i in range(chunk_length):
#             label = on_off_mask[i]
#             if label == DataLabel.DISCARDED:
#                 continue
#             if label == DataLabel.ON and prev_label != DataLabel.ON:
#                 block_counter += 1
#             prev_label = label
#             spectrum_index = indices[i]
#             final_spectra.append(xuv_spectra[spectrum_index])
#             final_identifiers.append((int(label), int(chunk_idx), int(block_counter if label != DataLabel.TRAINING else -1)))

#     # Convert to arrays (types friendly for MATLAB)
#     final_spectra_arr = np.asarray(final_spectra, dtype=np.float32)
#     final_identifiers_arr = np.asarray(final_identifiers, dtype=np.int32)
#     xuv_energy_ev_arr = np.asarray(xuv_energy_ev, dtype=np.float32)

#     # Default delay step if missing
#     if not delay_stepsize_mm:
#         delay_stepsize_mm = 0.05

#     return {
#         "final_spectra": final_spectra_arr,
#         "final_identifiers": final_identifiers_arr,
#         "xuv_energy_ev": xuv_energy_ev_arr,
#         "delay_stepsize_mm": float(delay_stepsize_mm),
#         "ppas": int(ppas),
#         "spss": int(spss),
#         "spds": int(spds),
#         "config_name": str(config["Name"]),
#     }

def main():
    """Main function to run the data processing pipeline."""
    config = TrainingSets.SELECTED

    # --- 1. Setup and Data Loading ---
    print("\n--- LOADING DATA ---")
    if not os.path.exists(Paths.OUTPUT_FOLDER):
        os.makedirs(Paths.OUTPUT_FOLDER)
        print(f"Created output directory: {Paths.OUTPUT_FOLDER}")

    print(f"Selected training set: {config['Name']} ({config['Comment']})")

    # Find XUV files using the new directory structure
    xuv_json_path, xuv_npy_path = find_xuv_files(config["XUV_date"], config["XUV_sequence"])
    print(f"XUV JSON: {xuv_json_path}")
    print(f"XUV NPY: {xuv_npy_path}")

    # Find delay stage file (still uses old naming convention)
    delay_file = [f for f in os.listdir(Paths.DELAY_STAGE_TIMES) if f.startswith(config["DELAY_STAGE_date"]) and f.endswith(".npz")]
    if len(delay_file) != 1:
        raise FileNotFoundError(f"Could not find exactly one delay stage file. Found: {delay_file}")
    delay_file_path = os.path.join(Paths.DELAY_STAGE_TIMES, delay_file[0])
    print(f"Delay stage file: {delay_file_path}")

    # Load data from files

    xuv_spectra, xuv_timestamps_us = load_xuv_spectra_and_timestamps(xuv_json_path)
    delay_timestamps_us, ppas, spss, spds, delay_stepsize_mm = load_delay_stage_data(delay_file_path)
    
    # Load energy calibration file from the workspace directory (always Spec.txt)
    spec_file_path = os.path.join(os.path.dirname(__file__), config["XUV_eV_calibration_file"])
    xuv_energy_ev = np.loadtxt(spec_file_path)
    print("\nLOADED DATA SHAPES:")
    print(f"\tXUV Spectra: {xuv_spectra.shape}")
    print(f"\tXUV Timestamps: {xuv_timestamps_us.shape}")
    print(f"\tDelay Timestamps: {delay_timestamps_us.shape}")
    print(f"\tSPSS: {spss}, SPDS: {spds}, Delay Step: {delay_stepsize_mm} mm")
    if delay_stepsize_mm is not None:
        print(f"\tDelay step in fs: {distance_mm_to_delay_fs(delay_stepsize_mm):.2f} fs")
    print(f"\tPPAS: {ppas}")
    # --- 2. Synchronisation and Chunking ---
    print("\n--- SYNCHRONISING DATA ---")
    # Find sync patterns in the XUV data, which mark the beginning of each delay scan.
    # An offset of +2 is added based on empirical observation of the acquisition timing.
    sync_gaps_us = [element * (ppas * (1 / 1030) * 1e6) for element in config["SYNC_GAPS_PATTERN"]]
    sync_indices = find_sync_patterns(xuv_timestamps_us, sync_gaps_us, config["SYNC_PATTERN_TOLERANCE_US"]) + 2

    expected_scans = len(delay_timestamps_us) // 2
    print(f"Expected {expected_scans} scans based on delay stage triggers.")
    print(f"Found {len(sync_indices)} sync patterns in XUV timestamps.")
    if len(sync_indices) != expected_scans:
        warnings.warn("Mismatch between number of sync patterns and expected delay scans.")

    if Settings.PLOT_TIMING_OVERVIEW:
        plt.figure(figsize=(15, 5))
        plt.vlines(xuv_timestamps_us * 1e-6, 0, 0.8, color='blue', alpha=0.3, label='XUV Timestamps')
        plt.scatter(xuv_timestamps_us[sync_indices] * 1e-6, [1.0] * len(sync_indices), color='cyan', marker='^', s=60, label='Detected XUV Sync')
        plt.vlines(delay_timestamps_us * 1e-6, 0, 1.2, color='orange', alpha=1, label='Delay Stage Triggers')
        plt.xlabel("Time [s]")
        plt.title("Timestamp Overview")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(Paths.OUTPUT_FOLDER, "sync_sequences_overview.png"), dpi=300)
        plt.show()

        # plot xuv spectra summed over the energies:
    # xuv_summed = np.sum(xuv_spectra, axis=1)
    # plt.figure()
    # plt.plot(xuv_timestamps_us, xuv_summed)
    # plt.xlabel("Time [us]")
    # plt.ylabel("Summed XUV Signal")
    # plt.title("XUV Spectra Summed Over Energies")
    # plt.grid()
    # plt.show()
    # Split the continuous data stream into chunks based on the sync indices.
    # Each chunk corresponds to one full delay scan.
    chunk_indices = np.split(np.arange(len(xuv_spectra)), sync_indices)[1:]
    # All chunks should ideally have the same length for consistent processing.
    chunk_lengths = [len(c) for c in chunk_indices]
    if len(set(chunk_lengths[:-1])) > 1: # Check all but the last chunk
        warnings.warn(f"Chunk lengths are not uniform: {chunk_lengths}")

    # --- 3. Labelling and Data Restructuring ---
    print("\n--- LABELLING AND RESTRUCTURING DATA ---")
    if not Settings.PLOT_ON_OFF_SPECTRUM:
        print("Skipping plotting as per settings.")
        return

    # Use the first complete chunk's length to create the ON/OFF mask.
    chunk_length = chunk_lengths[0]
    on_off_mask = create_on_off_mask(
        chunk_length=chunk_length,
        spss=spss,
        spds=spds,
        sync_pulses_to_skip=config["SYNC_PULSES_TO_SKIP"],
        pulses_to_skip=config["PULSES_TO_SKIP_BEFORE_FIRST_SHUTTER"],
        shift=config["SHIFT"]
    )

    # Prepare lists to hold the restructured data for plotting.
    final_spectra = []
    # Identifier format: (DataLabel, chunk_index, on_off_block_index)
    final_identifiers = []

    # Iterate through each chunk (delay scan)
    for chunk_idx, indices in enumerate(chunk_indices):
        if len(indices) < chunk_length:
            print(f"Skipping incomplete chunk {chunk_idx} with length {len(indices)}.")
            continue

        block_counter = -1
        prev_label = None
        # Iterate through each spectrum within the chunk
        for i in range(chunk_length):
            label = on_off_mask[i]
            if label == DataLabel.DISCARDED:
                continue

            # Increment the ON/OFF block counter at the start of each new ON block.
            if label == DataLabel.ON and prev_label != DataLabel.ON:
                block_counter += 1
            prev_label = label

            spectrum_index = indices[i]
            final_spectra.append(xuv_spectra[spectrum_index])
            
            identifier = (label, chunk_idx, block_counter if label != DataLabel.TRAINING else -1)
            final_identifiers.append(identifier)

    # Convert lists to NumPy arrays for efficient processing.
    final_spectra = np.array(final_spectra, dtype=np.float32)
    final_identifiers = np.array(final_identifiers, dtype=np.int32)

    # --- 4. Plotting Results ---
    print("\n--- PLOTTING RESULTS ---")
    if not delay_stepsize_mm:
        delay_stepsize_mm = 0.05 # Default value if not found in filename
        print(f"Using default delay step size: {delay_stepsize_mm} mm")

    # Apply a mask to focus on a relevant energy range.
    energy_mask = xuv_energy_ev < 86
    atas_on_off_clas(
        xuv_spectra=final_spectra[:, energy_mask],
        xuv_energies_eV=xuv_energy_ev[energy_mask],
        identifiers=final_identifiers,
        delay_stepsize_mm=delay_stepsize_mm,
        shift=0  # The shift is already applied in the mask, so this is for plotting only.
    )

    plot_on_off_clas_shift_sweep(
        xuv_spectra=final_spectra,
        xuv_energies_eV=xuv_energy_ev,
        identifiers=final_identifiers,
        delay_stepsize_mm=delay_stepsize_mm,
        use_background_correction=False
    )
    print("\n--- PROCESSING COMPLETE ---")

if __name__ == "__main__":
    main()
blob
mark :29
data 15264
import numpy as np
import matplotlib.pyplot as plt
from enum import IntEnum
from tqdm import tqdm
import os
from matplotlib import gridspec
#from atas_metrics import SpectrumMetrics
from typing import Optional, Tuple
# imoort polynomial fitting functions
from numpy.polynomial.polynomial import polyfit, polyval

# for plotting:
import sys
sys.path.append('/home/marcel/Koofr/University/Code/moritz code')
# noinspection PyUnresolvedReferences
#from common_functions import FigSettings  # Import class
#from common_functions import *


class DataLabel(IntEnum):
    OFF = 0
    ON = 1
    TRAINING = 2
    DISCARDED = 3


###################################################################################################################
# ATAS ON/OFF PLOTTING FUNCTION ##############################################################
###################################################################################################################
def atas_on_off_clas(xuv_spectra: np.ndarray, xuv_energies_eV: np.ndarray, identifiers: np.ndarray,
                     shift: int = -1, delay_stepsize_mm = -1, flip: bool = False,
                     save_path_thesis: str = "atas_spectrum.pdf", extract_line_fs: int = 0,
                     save_path: str = None) -> np.ndarray:
    """
    Plots the ON/OFF spectrum classically

    :param xuv_spectra: 2D array of XUV spectra.
    :param xuv_energies_eV: 1D array of XUV energies in eV.
    :param identifiers: 2D array of identifiers indicating: 1) the type of the spectrum according to DataLabel,
                        2) the delaystep number 3) the block number.
    :param shift: If > 0, shifts the indices of the spectra by this value.
    :param delay_stepsize_mm: If provided, the delay steps will be converted to femtoseconds using this value.
                           If -1, the delay steps will be treated as indices.
    :param flip: If True, flips the ATAS OD values for plotting.
    :param save_path_thesis: Path to save the figure, if provided.
    :param save_path: If provided, saves the figure at this path.
    :param extract_line_fs: If > 0, extracts a single line at this delay in femtoseconds.
                            If 0, no line is extracted.
    :return: 2D ATAS spectrum array with shape (n_delays, n_energy).
    """

    # plot the summed up XUV spectra:
    # summed_up_spectra = np.sum(xuv_spectra, axis=1)  # Sum over the first axis (spectra)
    # plt.figure(figsize=(10, 6))
    # plt.plot(summed_up_spectra, label="Summed XUV Spectra", color='blue')
    # plt.xlabel("Index")
    # plt.ylabel("Intensity [a.u.]")
    # plt.title("Summed XUV Spectra")
    # plt.grid(True)
    # plt.legend()
    # plt.tight_layout()
    # plt.show()

    # first count the number of different delay steps and blocks
    delay_steps = np.unique(identifiers[:, 1])
    blocks = np.unique(identifiers[:, 2])
    spds = len(blocks) - 1
    print(f"Found {len(delay_steps)} delay steps and {spds} on-off blocks in the identifiers.")

    # calculate the spss
    _, counts = np.unique(identifiers[:, 2], return_counts=True)
    values, value_counts = np.unique(counts, return_counts=True)

    # initialize the ATAS 2D array for the ON and OFF spectra
    atas_spectrum = np.zeros((len(delay_steps), len(xuv_energies_eV)), dtype=np.float64)

    # set up the ON and OFF spectra (use as SUMs)
    off_spectrum = np.zeros(len(xuv_energies_eV), dtype=np.float64)
    on_spectrum = np.zeros(len(xuv_energies_eV), dtype=np.float64)

    # NEW: counters for unequal counts
    on_count = 0
    off_count = 0

    # calculate the mean off spectrum (unchanged)
    mean_off_spectrum = np.mean(xuv_spectra[identifiers[:, 0] == DataLabel.OFF], axis=0)

    eps = 1e-12  # NEW: guard to avoid log(0)/division-by-zero

    # loop over the delay steps and blocks
    for delay_step in delay_steps:
        indices = np.where(identifiers[:, 1] == delay_step)[0]

        spds_counter = 0
        # NEW: reset accumulators per delay step
        on_spectrum.fill(0.0); off_spectrum.fill(0.0)
        on_count = 0; off_count = 0

        for idx in indices:
            # apply a shift ONCE (fixes the double-shift bug)
            idx = idx + shift
            if idx >= len(xuv_spectra) or idx < 0:
                continue

            # accumulate sums and counts
            if identifiers[idx, 0] == DataLabel.ON:
                on_spectrum += xuv_spectra[idx]  # <- no + shift here
                on_count += 1
            elif identifiers[idx, 0] == DataLabel.OFF:
                off_spectrum += xuv_spectra[idx]
                off_count += 1
            else:
                continue

            # if current is OFF and the next is not OFF, close and compute OD
            if (idx + 1 < len(xuv_spectra)
                    and identifiers[idx, 0] == DataLabel.OFF
                    and identifiers[idx + 1, 0] != DataLabel.OFF):

                if on_count > 0 and off_count > 0:
                    on_mean = on_spectrum / on_count
                    off_mean = off_spectrum / off_count

                    # OD = ln(ON/OFF), with guards for zeros/negatives
                    OD_values = np.log(
                        np.maximum(on_mean, eps) / np.maximum(off_mean, eps)
                    )

                    # accumulate into ATAS spectrum (keep your normalization by spds)
                    atas_spectrum[int(delay_step), :] += OD_values / spds

                # reset for the next pair within this delay step
                on_spectrum.fill(0.0); off_spectrum.fill(0.0)
                on_count = 0; off_count = 0

                spds_counter += 1

        # print(f"Finished delay step {delay_step} with {spds_counter} blocks.")

    # flip the ATAS spectrum if needed
    if flip:
        atas_spectrum = atas_spectrum * -1

    # calculate the timestep axis for the x-axis of the plot
    if delay_stepsize_mm > 0:
        timestep_fs = distance_mm_to_delay_fs(delay_stepsize_mm)
        delay_steps_fs = np.arange(len(delay_steps)) * timestep_fs
        # x_label = rf"Pump-Probe Delay $\tau$ [fs], $\Delta \tau \approx ${int(timestep_fs)} fs"
        x_label = rf"Pump-Probe Delay [fs]"
    else:
        delay_steps_fs = np.arange(len(delay_steps))
        x_label = "Delay Step"

    # Plot the ATAS spectrum
    plt.figure(figsize=(10, 6))
    X, Y = np.meshgrid(delay_steps_fs, xuv_energies_eV)
    vmax = np.abs(atas_spectrum).max()
    vmin = -vmax
    # cmap = plt.get_cmap('seismic')
    cmap = plt.get_cmap('bwr')  # Blue-White-Red colormap
    norm = plt.Normalize(vmin=vmin, vmax=vmax)

    # The pcm object is what we'll make interactive
    pcm = plt.pcolormesh(X, Y, atas_spectrum.T, shading='auto', cmap=cmap, norm=norm)
    # rasterize the pcolormesh for better performance and saving
    pcm.set_rasterized(True)

    plt.xlabel(x_label)
    plt.ylabel("XUV Energy [eV]")
    plt.colorbar(pcm, label=rf"Pump-Induced Signal [$\Delta$mOD]")
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, bbox_inches="tight")

    plt.show()


    # PLOT EXTRACTED LINES:
    # --- Find index closest to 28 eV ---
    # target_energy = 28.0  # eV
    # idx_28eV = np.argmin(np.abs(xuv_energies_eV - target_energy))

    # # --- Extract the line ---
    # line_at_28eV = atas_spectrum[:, idx_28eV]

    # # --- Plot the extracted line ---
    # plt.figure(figsize=(10, 4))
    # plt.plot(delay_steps_fs, line_at_28eV, label=f"{target_energy} eV")
    # plt.xlabel("Delay (fs)")
    # plt.ylabel(rf"Pump-Induced Signal [$\Delta$mOD]")
    # plt.title(f"ATAS Lineout at {target_energy} eV")
    # plt.legend()
    # plt.grid(True)
    # plt.tight_layout()
    # plt.show()

    # # extract the line at the specified time if requested
    # extracted_line, picked_time_fs, idx = extract_atas_line_at_time(
    #     atas_spectrum, delay_steps_fs, extract_line_fs
    # )

    # If a specific line is requested, the helper below can be used by callers separately.
    # We always return the full 2D ATAS spectrum for downstream processing (e.g., MATLAB).
    return atas_spectrum


def plot_on_off_clas_shift_sweep(
    xuv_spectra: np.ndarray,
    xuv_energies_eV: np.ndarray,
    identifiers: np.ndarray,
    delay_stepsize_mm: float = -1.0,
    flip: bool = False,
    use_background_correction: bool = True,
    save_path: Optional[str] = None,
) -> None:
    """
    Sweep the ON/OFF classical OD calculation over integer shifts in [-4, 4] using
    the *same logic* as in `plot_on_off_clas` (including the double-shift on spectra
    access), and plot a 3x3 grid of results.

    :param xuv_spectra: 2D array (n_samples, n_energy) of XUV spectra.
    :param xuv_energies_eV: 1D array (n_energy,) of XUV energies in eV.
    :param identifiers: 2D int array (n_samples, 3): [DataLabel, delay_step, block].
    :param delay_stepsize_mm: If > 0, converts delay steps to femtoseconds via `distance_mm_to_delay_fs`.
    :param flip: If True, multiplies OD values by -1 (sign flip).
    :param use_background_correction: If True, subtracts the 50th percentile per delay row (axis=1).
    :param save_path: If provided, saves the 3×3 figure at this path.
    :return: None
    """
    # --- Delay axis like in your original ---
    delay_steps = np.unique(identifiers[:, 1])
    blocks = np.unique(identifiers[:, 2])
    spds = len(blocks) - 1  # exactly like your code
    n_energy = xuv_energies_eV.shape[0]

    if delay_stepsize_mm > 0:
        timestep_fs = distance_mm_to_delay_fs(delay_stepsize_mm)
        delay_axis = np.arange(len(delay_steps)) * timestep_fs
        x_label = r"Pump-Probe Delay [fs]"
    else:
        delay_axis = np.arange(len(delay_steps))
        x_label = "Delay Step"

    eps = 1e-12  # numerical guard for log

    # === Helper that reproduces your original block logic verbatim (including double shift on spectra) ===
    def compute_atas_spectrum_for_shift(shift: int) -> np.ndarray:
        atas_spectrum = np.zeros((len(delay_steps), n_energy), dtype=np.float64)
        on_spectrum = np.zeros(n_energy, dtype=np.float64)
        off_spectrum = np.zeros(n_energy, dtype=np.float64)

        for dsi, delay_step in enumerate(delay_steps):
            indices = np.where(identifiers[:, 1] == delay_step)[0]

            # reset accumulators for each delay step like in your code
            on_spectrum[:] = 0.0
            off_spectrum[:] = 0.0
            spds_counter = 0

            for idx0 in indices:
                # apply a shift if needed (exactly as in your function)
                idx = idx0 + shift
                if idx < 0 or idx >= len(xuv_spectra):
                    continue

                # === Your original uses identifiers[idx, 0] to decide ON/OFF,
                #     but accumulates xuv_spectra[idx + shift] (double shift) ===
                x_idx = idx + shift
                if x_idx < 0 or x_idx >= len(xuv_spectra):
                    # safety guard to avoid IndexError at the edges for larger shifts
                    continue

                if identifiers[idx, 0] == DataLabel.ON:
                    on_spectrum += xuv_spectra[x_idx]
                elif identifiers[idx, 0] == DataLabel.OFF:
                    off_spectrum += xuv_spectra[x_idx]
                else:
                    continue

                # end-of-OFF-block detection (exactly like your code)
                if (idx + 1) < len(xuv_spectra) and identifiers[idx, 0] == DataLabel.OFF and identifiers[idx + 1, 0] != DataLabel.OFF:
                    ratio = (on_spectrum + eps) / (off_spectrum + eps)
                    OD_values = np.log(ratio)

                    atas_spectrum[int(delay_step), :] += OD_values / max(spds, 1)
                    # reset accumulators
                    on_spectrum[:] = 0.0
                    off_spectrum[:] = 0.0
                    spds_counter += 1

        if flip:
            atas_spectrum *= -1.0

        # mandatory 50th percentile unless disabled
        if use_background_correction:
            atas_spectrum = atas_spectrum - np.percentile(atas_spectrum, 50, axis=1, keepdims=True)

        return atas_spectrum

    # --- Compute all 9 shifts with exactly the same logic ---
    shifts = list(range(-4, 5))  # [-4, ..., +4]
    atas_list = [compute_atas_spectrum_for_shift(s) for s in shifts]

    # consistent color scaling across subplots
    all_vals = np.stack(atas_list, axis=0)
    vmax = float(np.nanmax(np.abs(all_vals)))
    vmin = -vmax
    cmap = plt.get_cmap('bwr')
    norm = plt.Normalize(vmin=vmin, vmax=vmax)

    # --- Plot 3x3 grid ---
    X, Y = np.meshgrid(delay_axis, xuv_energies_eV)
    fig, axs = plt.subplots(3, 3, figsize=(15, 12), sharex=True, sharey=True)
    fig.subplots_adjust(hspace=0.3, wspace=0.25)

    pcm_last = None
    for i in range(3):
        for j in range(3):
            ax = axs[i, j]
            s = shifts[i * 3 + j]
            pcm_last = ax.pcolormesh(X, Y, atas_list[i * 3 + j].T, shading='auto', cmap=cmap, norm=norm)
            ax.set_title(f"shift = {s}", fontsize=10)
            if i == 2:
                ax.set_xlabel(x_label)
            if j == 0:
                ax.set_ylabel("XUV Energy [eV]")

    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])
    fig.colorbar(pcm_last, cax=cbar_ax, label=rf"Pump-Induced Signal [$\Delta$mOD]")

    bc_note = "with 50th-percentile background" if use_background_correction else "without background correction"
    fig.suptitle(f"ATAS (ON/OFF, log ratio) shift sweep (−4…+4), {bc_note}", fontsize=14)
    plt.tight_layout(rect=[0, 0, 0.9, 0.95])

    if save_path:
        plt.savefig(save_path, bbox_inches="tight")
    plt.show()


def extract_atas_line_at_time(
    atas_spectrum: np.ndarray,
    delay_steps_fs: np.ndarray,
    extract_line_fs: float
) -> Tuple[np.ndarray, float, int]:
    """
    Extracts the 1D ATAS spectrum line closest to a given delay time.

    :param atas_spectrum: 2D array shaped (n_delays, n_energies).
    :param delay_steps_fs: 1D array of delays in fs (length n_delays).
    :param extract_line_fs: Target time in fs.
    :return: Tuple containing:
        - line_1d (np.ndarray): The extracted spectrum line.
        - picked_time_fs (float): The actual delay time used.
        - picked_index (int): The index of the extracted line.
    """
    delay_steps_fs = np.asarray(delay_steps_fs, dtype=float)
    idx = int(np.argmin(np.abs(delay_steps_fs - extract_line_fs)))
    return atas_spectrum[idx, :], float(delay_steps_fs[idx]), idx


####################################################################################################
# Functions for miscellaneous tasks ################################################################
####################################################################################################
def distance_mm_to_delay_fs(distance_mm: float) -> float:
    """
    Convert delay stage travel distance to optical delay in femtoseconds.

    :param distance_mm: Distance moved by the delay stage in millimeters.
    :return: Corresponding time delay in femtoseconds.
    """
    c_mm_per_ns = 299.792458  # speed of light in mm/ns
    delay_fs = (2 * distance_mm / c_mm_per_ns) * 1e6  # factor 2 for round-trip, ns -> fs
    return delay_fs
blob
mark :30
data 25459
# -*- coding: utf-8 -*-
"""
This script processes raw data from an Attosecond Transient Absorption
Spectroscopy (ATAS) experiment. It focuses on synchronising XUV spectra
with delay stage positions based on timestamps.

The main steps are:
1.  Load XUV spectra, XUV timestamps, and delay stage trigger timestamps.
2.  Identify synchronisation patterns within the XUV timestamps. These patterns
    correspond to the start of each delay scan.
3.  Segment the continuous XUV data into chunks, where each chunk represents
    one full scan of the delay stage.
4.  Within each chunk, label individual spectra as 'ON' (pump and probe beams
    are present), 'OFF' (only probe beam is present), or 'DISCARDED' based on
    the known experimental sequence.
5.  Generate and display the final ATAS trace (ΔA) by processing the labelled
    'ON' and 'OFF' spectra.
"""
import json
import os
import re
import sys
import warnings
from enum import IntEnum
from typing import Any, Dict, List, Optional, Tuple, cast

import matplotlib.pyplot as plt
import numpy as np

# Assuming the 'misc_functions' module is in the specified path and contains the necessary plotting functions.
sys.path.append('C:\\Users\\Moritz\\OneDrive - ETH Zurich\\Code for ThinkPad to test!')
from misc_functions import atas_on_off_clas, plot_on_off_clas_shift_sweep

# GLOBAL CONSTANTS #############################################################
SYNC_GAPS_PATTERN = [1, 4, 3, 3, 1]  # Always the same for all measurements
XUV_EV_CALIBRATION_FILE = "Spec.txt"  # Always the same for all measurements

# MEASUREMENT CONFIGURATION ####################################################
# Manually define these parameters for each measurement
XUV_DATE = "250903"                         # YYMMDD format
XUV_SEQUENCE = 3                            # X for YYMMDD_00X
DELAY_STAGE_DATE = XUV_DATE                 # YYMMDD format (same as XUV_DATE typically)
DELAY_STAGE_SEQUENCE = XUV_SEQUENCE         # X for YYMMDD_00X (can be different from XUV)
SYNC_PATTERN_TOLERANCE_US = 40000           # Tolerance for detecting sync pattern gaps
SYNC_PULSES_TO_SKIP = 3                     # Number of initial pulses to discard
PULSES_TO_SKIP_BEFORE_FIRST_SHUTTER = 5     # Number of pulses to skip before shutter
SHIFT = -1                                  # Manual shift for ON/OFF labelling


class Paths:
    """Defines the main directory paths for input data."""
    BASE_DIR = r"C:\Users\Moritz\Desktop\TESTDATA"
    SERVER_DIR = r"Z:\Attoline"  # Base server directory
    # The folder where all generated plots and data will be stored.
    OUTPUT_FOLDER = r"C:\Users\Moritz\Desktop\Pixis_data\output"


def find_xuv_files(yymmdd: str, sequence_num: int) -> Tuple[str, str]:
    """
    Find XUV .npy and .json files based on the new directory structure.
    
    Args:
        yymmdd: Date in YYMMDD format (e.g., "250903")
        sequence_num: Sequence number (e.g., 1 for _001)
        
    Returns:
        Tuple of (json_path, npy_path)
    """
    year = "20" + yymmdd[:2]  # Convert YY to YYYY
    sequence_str = f"{yymmdd}_{sequence_num:03d}"
    
    # Build the directory path
    data_dir = os.path.join(Paths.BASE_DIR, year, "STRA", yymmdd, sequence_str)
    
    json_path = os.path.join(data_dir, f"{sequence_str}.json")
    npy_path = os.path.join(data_dir, f"{sequence_str}.npy")
    
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"JSON file not found: {json_path}")
    if not os.path.exists(npy_path):
        raise FileNotFoundError(f"NPY file not found: {npy_path}")
        
    return json_path, npy_path


def find_delay_stage_file(yymmdd: str, sequence_num: int) -> str:
    """
    Find delay stage .npz file based on the new directory structure.
    
    Directory structure: server_dir/2025/DelayStage/2025/DelayStage/YYMMDD/YYMMDD_XXX/delay_YYMMDD_SXXX.npz
    
    Args:
        yymmdd: Date in YYMMDD format (e.g., "250903")
        sequence_num: Sequence number (e.g., 2 for S002)
        
    Returns:
        Full path to the delay stage .npz file
    """
    year = "20" + yymmdd[:2]  # Convert YY to YYYY
    sequence_str = f"{yymmdd}_{sequence_num:03d}"
    
    # Build the directory path: server_dir/2025/DelayStage/2025/DelayStage/250903/250903_002/
    data_dir = os.path.join(
        Paths.SERVER_DIR, 
        year, 
        "DelayStage", 
        year, 
        "DelayStage", 
        yymmdd, 
        sequence_str
    )
    
    # Build the filename: delay_250903_S002.npz
    filename = f"delay_{yymmdd}_S{sequence_num:03d}.npz"
    npz_path = os.path.join(data_dir, filename)
    
    if not os.path.exists(npz_path):
        raise FileNotFoundError(f"Delay stage NPZ file not found: {npz_path}")
        
    return npz_path


class Settings:
    """Contains global settings and flags for the script's execution."""
    # Plot an overview of all timestamps and detected sync points. Useful for debugging timing issues.
    PLOT_TIMING_OVERVIEW = False
    # Plot the final on/off spectra and the calculated ATAS trace.
    PLOT_ON_OFF_SPECTRUM = True


class DataLabel(IntEnum):
    """Enumeration for labelling each acquired data point."""
    OFF = 0  # Pump beam blocked
    ON = 1  # Pump and probe beams present
    TRAINING = 2  # Data not used for ON/OFF, potentially for ML
    DISCARDED = 3  # Data to be ignored entirely


def get_metadata(json_path: str) -> Dict:
    """Loads metadata from a specified JSON file.

    Args:
        json_path: Path to the metadata JSON file.

    Returns:
        A dictionary containing the metadata.
    """
    with open(json_path, "r") as f:
        metadata = json.load(f)
    return metadata


def load_xuv_spectra_and_timestamps(json_path: str) -> Tuple[np.ndarray, np.ndarray]:
    """Loads XUV spectra and their timestamps from a memory-mapped file.

    Args:
        json_path: Path to the metadata JSON file that describes the memmap file.

    Returns:
        A tuple containing:
        - A 2D NumPy array of spectra (N_frames, N_pixels).
        - A 1D NumPy array of timestamps in microseconds.
    """
    metadata = get_metadata(json_path)
    print('the json path is at ', json_path)
    memmap_path = os.path.join(os.path.dirname(json_path), metadata["memmap_file"])

    # Define the structured data type for the memory-mapped file
    dtype = np.dtype([
        ("spectrum", np.uint16, metadata["spectra_shape"][1]),
        ("timestamp_us", np.uint64)
    ])

    memmap_path = "C:\\Users\\Moritz\\Desktop\\TESTDATA\\2025\\STRA\\250903\\250903_003\\250903_003.npy"

    # Read the data and convert from memmap to standard numpy arrays
    mmap = np.memmap(memmap_path, dtype=dtype, mode="r")
    return np.array(mmap["spectrum"]), np.array(mmap["timestamp_us"])


def load_delay_stage_data(filename: str) -> Tuple[np.ndarray, int, int, int, Optional[float]]:
    """Reads delay stage data, extracting timestamps and experimental parameters from the .npz file.

    Args:
        filename: Path to the .npz file containing delay stage timestamps and parameters.

    Returns:
        A tuple containing:
        - timestamps_us (np.ndarray): Trigger timestamps in microseconds.
        - ppas (int): Pulses per acquisition step.
        - spss (int): Spectra per shutter step (duration of ON or OFF block).
        - spds (int): Steps per delay scan (number of ON/OFF pairs).
        - move_step_fs (float | None): Move step size in femtoseconds, if found.
    """
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File not found: {filename}")
    
    # Load data from the .npz file
    print('filename:', filename)
    
    try:
        npz_data = np.load(filename)
        available_keys = list(npz_data.keys())
        print(f"Available arrays in .npz file: {available_keys}")
        
        # Load timestamps
        timestamp_key = None
        for key in ['timestamps', 'timestamps_us', 'data', 'arr_0']:
            if key in available_keys:
                timestamp_key = key
                break
        
        if timestamp_key is None:
            raise ValueError(f"Could not find timestamp data in .npz file. Available keys: {available_keys}")
        
        timestamps_us = np.array(npz_data[timestamp_key])
        timestamps_us = timestamps_us[timestamps_us > 0]  # Filter out zero entries
        print(f"Using key '{timestamp_key}' for timestamp data")
        
        # Load parameters from .npz file variables
        ppas = int(npz_data['ppas']) if 'ppas' in available_keys else None
        spss = int(npz_data['spss']) if 'spss' in available_keys else None
        spds = int(npz_data['spds']) if 'spds' in available_keys else None
        move_step_fs = float(npz_data['move_step_fs']) if 'move_step_fs' in available_keys else None
        
        # Check that required parameters were found
        if ppas is None or spss is None or spds is None:
            # Fallback: try to parse from filename for backward compatibility
            print("Warning: Could not find ppas, spss, spds in .npz file, trying filename parsing...")
            match = re.search(r"ppas_(\d+)_spss_(\d+)_spds_(\d+)", filename)
            if not match:
                raise ValueError("Could not find ppas, spss, and spds in .npz file or filename.")
            ppas = int(match.group(1))
            spss = int(match.group(2))
            spds = int(match.group(3))
        
        print(f"Parameters: ppas={ppas}, spss={spss}, spds={spds}, move_step_fs={move_step_fs}")
        
        return timestamps_us, ppas, spss, spds, move_step_fs
        
    except Exception as e:
        raise ValueError(f"Error loading .npz file: {e}")
    finally:
        if 'npz_data' in locals():
            npz_data.close()


def find_sync_patterns(timestamps_us: np.ndarray, expected_gaps_us: List[int], tolerance_us: int) -> np.ndarray:
    """Detects synchronisation patterns in a timestamp array.

    A pattern is defined by a specific sequence of time gaps between consecutive timestamps.

    Args:
        timestamps_us: An array of timestamps in microseconds.
        expected_gaps_us: A list of the expected time gaps that form the sync pattern.
        tolerance_us: The allowed deviation in microseconds for each gap.

    Returns:
        An array of indices, where each index points to the start of a detected sync pattern.
    """
    sync_indices = []
    num_gaps = len(expected_gaps_us)
    i = 0
    # Iterate through the timestamps to find the pattern
    while i <= len(timestamps_us) - (num_gaps + 1):
        # Get a slice of timestamps that could form one pattern
        t_slice = timestamps_us[i:i + num_gaps + 1]
        gaps = np.diff(t_slice)

        # Check if all calculated gaps match the expected gaps within the tolerance
        if all(np.abs(g - e) < tolerance_us for g, e in zip(gaps, expected_gaps_us)):
            sync_indices.append(i)
            # Skip forward to prevent re-detecting overlapping patterns.
            # A cooldown of num_gaps is a safe choice.
            i += num_gaps
        else:
            i += 1
    return np.array(sync_indices)


def create_on_off_mask(chunk_length: int, spss: int, spds: int,
                       sync_pulses_to_skip: int, pulses_to_skip: int,
                       shift: int) -> np.ndarray:
    """Creates a mask to label each data point in a chunk as ON, OFF, TRAINING, or DISCARDED.

    Args:
        chunk_length: The total number of data points in the chunk.
        spss: Spectra per shutter step (duration of an ON or OFF block).
        spds: Steps per delay scan (number of ON/OFF pairs).
        sync_pulses_to_skip: Number of initial pulses to label as DISCARDED.
        pulses_to_skip: Number of subsequent pulses to label as TRAINING.
        shift: A manual timing shift to apply to the ON/OFF blocks.

    Returns:
        A 1D NumPy array of labels (see DataLabel enum) for the chunk.
    """
    # Start by labelling everything as TRAINING.
    mask = np.full(chunk_length, DataLabel.TRAINING, dtype=np.uint8)

    # Label initial pulses for synchronisation and stabilisation.
    mask[:sync_pulses_to_skip] = DataLabel.DISCARDED
    mask[sync_pulses_to_skip:pulses_to_skip] = DataLabel.TRAINING

    # Apply the ON/OFF pattern for each delay step.
    current_index = pulses_to_skip + shift
    for _ in range(spds):
        # ON block
        mask[current_index:current_index + spss] = DataLabel.ON
        current_index += spss
        # OFF block
        mask[current_index:current_index + spss] = DataLabel.OFF
        current_index += spss
    return mask

# def prepare_atas_inputs(selected_set_name: Optional[str] = None) -> Dict[str, object]:
#     """Prepare inputs for ATAS computation without plotting.

#     This function mirrors the loading, synchronization, chunking, and labeling
#     steps from main(), but returns the assembled arrays and metadata in a dict
#     so that MATLAB (MoritzReader.m) can compute and plot ATAS.

#     Args:
#         selected_set_name: Optional name of the training set to use (e.g., "TS0028").
#                            If None, uses TrainingSets.SELECTED.

#     Returns:
#         A dictionary with keys:
#             - 'final_spectra' (np.ndarray, float32) [N x E]
#             - 'final_identifiers' (np.ndarray, int32) [N x 3]
#             - 'xuv_energy_ev' (np.ndarray, float32) [E]
#             - 'delay_stepsize_mm' (float)
#             - 'ppas' (int), 'spss' (int), 'spds' (int)
#             - 'config_name' (str)
#     """
#     # Resolve configuration
#     config: Dict[str, Any]
#     if selected_set_name is None:
#         config = cast(Dict[str, Any], TrainingSets.SELECTED)
#     else:
#         # Allow matching by attribute name (e.g., "TS0028") or by config['Name']
#         found: Optional[Dict[str, Any]] = None
#         for attr in dir(TrainingSets):
#             if attr.startswith("TS"):
#                 cfg = getattr(TrainingSets, attr)
#                 if isinstance(cfg, dict) and (attr == selected_set_name or cfg.get("Name") == selected_set_name):
#                     found = cast(Dict[str, Any], cfg)
#                     break
#         if found is None:
#             available = [a for a in dir(TrainingSets) if a.startswith("TS")]
#             raise ValueError(f"Unknown training set '{selected_set_name}'. Available: {available}")
#         config = found

#     # Locate files
#     # xuv_data_file = [f for f in os.listdir(Paths.XUV_SPECTRA) if f.startswith(config["XUV_date"]) and f.endswith(".npy")]
#     # xuv_meta_file = [f for f in os.listdir(Paths.XUV_SPECTRA) if f.startswith(config["XUV_date"]) and f.endswith(".json")]
#     # delay_file = [f for f in os.listdir(Paths.DELAY_STAGE_TIMES) if f.startswith(config["DELAY_STAGE_date"]) and f.endswith(".npz")]
#     # if not (len(xuv_data_file) == 1 and len(xuv_meta_file) == 1 and len(delay_file) == 1):
#     #     raise FileNotFoundError("Could not find exactly one file for each data type (XUV data, XUV meta, Delay).")

#     # Load data
#     # xuv_spectra, xuv_timestamps_us = load_xuv_spectra_and_timestamps(os.path.join(Paths.XUV_SPECTRA, xuv_meta_file[0]))
#     # delay_timestamps_us, ppas, spss, spds, delay_stepsize_mm = load_delay_stage_data(os.path.join(Paths.DELAY_STAGE_TIMES, delay_file[0]))
#     # xuv_energy_ev = np.loadtxt(os.path.join(Paths.XUV_SPECTRA, config["XUV_eV_calibration_file"]))
    


#     # Synchronisation: detect start of scans via sync gaps pattern
#     # sync_gaps_us: List[int] = [int(round(float(element) * (ppas * (1 / 1030) * 1e6))) for element in config["SYNC_GAPS_PATTERN"]]
#     # sync_indices = find_sync_patterns(xuv_timestamps_us, sync_gaps_us, config["SYNC_PATTERN_TOLERANCE_US"]) + 2

#     # Segment into chunks (one per delay scan)
#     chunk_indices = np.split(np.arange(len(xuv_spectra)), sync_indices)[1:]
#     chunk_lengths = [len(c) for c in chunk_indices]
#     if len(chunk_lengths) == 0:
#         raise RuntimeError("No chunks detected from XUV sync patterns; check timestamps or pattern settings.")

#     # Build ON/OFF mask using first complete chunk length
#     chunk_length = chunk_lengths[0]
#     on_off_mask = create_on_off_mask(
#         chunk_length=chunk_length,
#         spss=spss,
#         spds=spds,
#         sync_pulses_to_skip=config["SYNC_PULSES_TO_SKIP"],
#         pulses_to_skip=config["PULSES_TO_SKIP_BEFORE_FIRST_SHUTTER"],
#         shift=config["SHIFT"],
#     )

#     final_spectra: List[np.ndarray] = []
#     # identifiers: (DataLabel, chunk_index, on_off_block_index)
#     final_identifiers: List[Tuple[int, int, int]] = []

#     for chunk_idx, indices in enumerate(chunk_indices):
#         if len(indices) < chunk_length:
#             # skip incomplete last chunk
#             continue
#         block_counter = -1
#         prev_label = None
#         for i in range(chunk_length):
#             label = on_off_mask[i]
#             if label == DataLabel.DISCARDED:
#                 continue
#             if label == DataLabel.ON and prev_label != DataLabel.ON:
#                 block_counter += 1
#             prev_label = label
#             spectrum_index = indices[i]
#             final_spectra.append(xuv_spectra[spectrum_index])
#             final_identifiers.append((int(label), int(chunk_idx), int(block_counter if label != DataLabel.TRAINING else -1)))

#     # Convert to arrays (types friendly for MATLAB)
#     final_spectra_arr = np.asarray(final_spectra, dtype=np.float32)
#     final_identifiers_arr = np.asarray(final_identifiers, dtype=np.int32)
#     xuv_energy_ev_arr = np.asarray(xuv_energy_ev, dtype=np.float32)

#     # Default delay step if missing
#     if not delay_stepsize_mm:
#         delay_stepsize_mm = 0.05

#     return {
#         "final_spectra": final_spectra_arr,
#         "final_identifiers": final_identifiers_arr,
#         "xuv_energy_ev": xuv_energy_ev_arr,
#         "delay_stepsize_mm": float(delay_stepsize_mm),
#         "ppas": int(ppas),
#         "spss": int(spss),
#         "spds": int(spds),
#         "config_name": str(config["Name"]),
#     }

def main():
    """Main function to run the data processing pipeline."""
    
    # --- 1. Setup and Data Loading ---
    print("\n--- LOADING DATA ---")
    if not os.path.exists(Paths.OUTPUT_FOLDER):
        os.makedirs(Paths.OUTPUT_FOLDER)
        print(f"Created output directory: {Paths.OUTPUT_FOLDER}")

    print(f"XUV Date: {XUV_DATE}, Sequence: {XUV_SEQUENCE}")
    print(f"Delay Stage Date: {DELAY_STAGE_DATE}, Sequence: {DELAY_STAGE_SEQUENCE}")
    
    # Find XUV files using the new directory structure
    xuv_json_path, xuv_npy_path = find_xuv_files(XUV_DATE, XUV_SEQUENCE)
    print(f"XUV JSON: {xuv_json_path}")
    print(f"XUV NPY: {xuv_npy_path}")

    # Find delay stage file using the new directory structure
    delay_file_path = find_delay_stage_file(DELAY_STAGE_DATE, DELAY_STAGE_SEQUENCE)
    print(f"Delay stage file: {delay_file_path}")

    # Load data from files
    xuv_spectra, xuv_timestamps_us = load_xuv_spectra_and_timestamps(xuv_json_path)
    print(load_delay_stage_data(delay_file_path))
    delay_timestamps_us, ppas, spss, spds, move_step_fs = load_delay_stage_data(delay_file_path)
    
    # Load energy calibration file from the workspace directory (always Spec.txt)
    spec_file_path = os.path.join(os.path.dirname(__file__), XUV_EV_CALIBRATION_FILE)
    xuv_energy_ev = np.loadtxt(spec_file_path)
    print("\nLOADED DATA SHAPES:")
    print(f"\tXUV Spectra: {xuv_spectra.shape}")
    print(f"\tXUV Timestamps: {xuv_timestamps_us.shape}")
    print(f"\tDelay Timestamps: {delay_timestamps_us.shape}")
    print(f"\tSPSS: {spss}, SPDS: {spds}, Move Step: {move_step_fs} fs")
    if move_step_fs is not None:
        print(f"\tMove step in fs: {move_step_fs:.2f} fs")
    print(f"\tPPAS: {ppas}")
    # --- 2. Synchronisation and Chunking ---
    print("\n--- SYNCHRONISING DATA ---")
    # Find sync patterns in the XUV data, which mark the beginning of each delay scan.
    # An offset of +2 is added based on empirical observation of the acquisition timing.
    sync_gaps_us = [element * (ppas * (1 / 1030) * 1e6) for element in SYNC_GAPS_PATTERN]
    sync_indices = find_sync_patterns(xuv_timestamps_us, sync_gaps_us, SYNC_PATTERN_TOLERANCE_US) + 2

    expected_scans = len(delay_timestamps_us) // 2
    print(f"Expected {expected_scans} scans based on delay stage triggers.")
    print(f"Found {len(sync_indices)} sync patterns in XUV timestamps.")
    if len(sync_indices) != expected_scans:
        warnings.warn("Mismatch between number of sync patterns and expected delay scans.")

    if Settings.PLOT_TIMING_OVERVIEW:
        plt.figure(figsize=(15, 5))
        plt.vlines(xuv_timestamps_us * 1e-6, 0, 0.8, color='blue', alpha=0.3, label='XUV Timestamps')
        plt.scatter(xuv_timestamps_us[sync_indices] * 1e-6, [1.0] * len(sync_indices), color='cyan', marker='^', s=60, label='Detected XUV Sync')
        plt.vlines(delay_timestamps_us * 1e-6, 0, 1.2, color='orange', alpha=1, label='Delay Stage Triggers')
        plt.xlabel("Time [s]")
        plt.title("Timestamp Overview")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(Paths.OUTPUT_FOLDER, "sync_sequences_overview.png"), dpi=300)
        plt.show()

        # plot xuv spectra summed over the energies:
    # xuv_summed = np.sum(xuv_spectra, axis=1)
    # plt.figure()
    # plt.plot(xuv_timestamps_us, xuv_summed)
    # plt.xlabel("Time [us]")
    # plt.ylabel("Summed XUV Signal")
    # plt.title("XUV Spectra Summed Over Energies")
    # plt.grid()
    # plt.show()
    # Split the continuous data stream into chunks based on the sync indices.
    # Each chunk corresponds to one full delay scan.
    chunk_indices = np.split(np.arange(len(xuv_spectra)), sync_indices)[1:]
    # All chunks should ideally have the same length for consistent processing.
    chunk_lengths = [len(c) for c in chunk_indices]
    if len(set(chunk_lengths[:-1])) > 1: # Check all but the last chunk
        warnings.warn(f"Chunk lengths are not uniform: {chunk_lengths}")

    # --- 3. Labelling and Data Restructuring ---
    print("\n--- LABELLING AND RESTRUCTURING DATA ---")
    if not Settings.PLOT_ON_OFF_SPECTRUM:
        print("Skipping plotting as per settings.")
        return

    # Use the first complete chunk's length to create the ON/OFF mask.
    chunk_length = chunk_lengths[0]
    on_off_mask = create_on_off_mask(
        chunk_length=chunk_length,
        spss=spss,
        spds=spds,
        sync_pulses_to_skip=SYNC_PULSES_TO_SKIP,
        pulses_to_skip=PULSES_TO_SKIP_BEFORE_FIRST_SHUTTER,
        shift=SHIFT
    )

    # Prepare lists to hold the restructured data for plotting.
    final_spectra = []
    # Identifier format: (DataLabel, chunk_index, on_off_block_index)
    final_identifiers = []

    # Iterate through each chunk (delay scan)
    for chunk_idx, indices in enumerate(chunk_indices):
        if len(indices) < chunk_length:
            print(f"Skipping incomplete chunk {chunk_idx} with length {len(indices)}.")
            continue

        block_counter = -1
        prev_label = None
        # Iterate through each spectrum within the chunk
        for i in range(chunk_length):
            label = on_off_mask[i]
            if label == DataLabel.DISCARDED:
                continue

            # Increment the ON/OFF block counter at the start of each new ON block.
            if label == DataLabel.ON and prev_label != DataLabel.ON:
                block_counter += 1
            prev_label = label

            spectrum_index = indices[i]
            final_spectra.append(xuv_spectra[spectrum_index])
            
            identifier = (label, chunk_idx, block_counter if label != DataLabel.TRAINING else -1)
            final_identifiers.append(identifier)

    # Convert lists to NumPy arrays for efficient processing.
    final_spectra = np.array(final_spectra, dtype=np.float32)
    final_identifiers = np.array(final_identifiers, dtype=np.int32)

    # --- 4. Plotting Results ---
    print("\n--- PLOTTING RESULTS ---")
    
    # Convert move_step_fs to delay_stepsize_mm for plotting functions
    # If move_step_fs is not available, use a default value
    if move_step_fs is not None:
        # Convert fs to mm: 1 fs = c * 1e-15 / 2 meters (round trip)
        # c = 3e8 m/s, so 1 fs = 1.5e-7 m = 0.15 mm
        delay_stepsize_mm = move_step_fs * 0.15e-3  # Convert fs to mm
        print(f"Using move step: {move_step_fs} fs = {delay_stepsize_mm:.6f} mm")
    else:
        delay_stepsize_mm = 0.05  # Default value if not found
        print(f"Using default delay step size: {delay_stepsize_mm} mm")

    # Apply a mask to focus on a relevant energy range.
    energy_mask = xuv_energy_ev < 86
    atas_on_off_clas(
        xuv_spectra=final_spectra[:, energy_mask],
        xuv_energies_eV=xuv_energy_ev[energy_mask],
        identifiers=final_identifiers,
        delay_stepsize_mm=delay_stepsize_mm,
        shift=0  # The shift is already applied in the mask, so this is for plotting only.
    )

    plot_on_off_clas_shift_sweep(
        xuv_spectra=final_spectra,
        xuv_energies_eV=xuv_energy_ev,
        identifiers=final_identifiers,
        delay_stepsize_mm=delay_stepsize_mm,
        use_background_correction=False
    )
    print("\n--- PROCESSING COMPLETE ---")

if __name__ == "__main__":
    main()
blob
mark :31
data 15721
import numpy as np
import matplotlib.pyplot as plt
from enum import IntEnum
from tqdm import tqdm
import os
from matplotlib import gridspec
#from atas_metrics import SpectrumMetrics
from typing import Optional, Tuple
# imoort polynomial fitting functions
from numpy.polynomial.polynomial import polyfit, polyval

# for plotting:
import sys
sys.path.append('/home/marcel/Koofr/University/Code/moritz code')
# noinspection PyUnresolvedReferences
#from common_functions import FigSettings  # Import class
#from common_functions import *


class DataLabel(IntEnum):
    OFF = 0
    ON = 1
    TRAINING = 2
    DISCARDED = 3


###################################################################################################################
# ATAS ON/OFF PLOTTING FUNCTION ##############################################################
###################################################################################################################
def atas_on_off_clas(xuv_spectra: np.ndarray, xuv_energies_eV: np.ndarray, identifiers: np.ndarray,
                     shift: int = -1, delay_stepsize_mm = -1, flip: bool = False,
                     save_path_thesis: str = "atas_spectrum.pdf", extract_line_fs: int = 0,
                     save_path: str = None) -> np.ndarray:
    """
    Plots the ON/OFF spectrum classically

    :param xuv_spectra: 2D array of XUV spectra.
    :param xuv_energies_eV: 1D array of XUV energies in eV.
    :param identifiers: 2D array of identifiers indicating: 1) the type of the spectrum according to DataLabel,
                        2) the delaystep number 3) the block number.
    :param shift: If > 0, shifts the indices of the spectra by this value.
    :param delay_stepsize_mm: If provided, the delay steps will be converted to femtoseconds using this value.
                           If -1, the delay steps will be treated as indices.
    :param flip: If True, flips the ATAS OD values for plotting.
    :param save_path_thesis: Path to save the figure, if provided.
    :param save_path: If provided, saves the figure at this path.
    :param extract_line_fs: If > 0, extracts a single line at this delay in femtoseconds.
                            If 0, no line is extracted.
    :return: 2D ATAS spectrum array with shape (n_delays, n_energy).
    """

    # plot the summed up XUV spectra:
    # summed_up_spectra = np.sum(xuv_spectra, axis=1)  # Sum over the first axis (spectra)
    # plt.figure(figsize=(10, 6))
    # plt.plot(summed_up_spectra, label="Summed XUV Spectra", color='blue')
    # plt.xlabel("Index")
    # plt.ylabel("Intensity [a.u.]")
    # plt.title("Summed XUV Spectra")
    # plt.grid(True)
    # plt.legend()
    # plt.tight_layout()
    # plt.show()

    # first count the number of different delay steps and blocks
    delay_steps = np.unique(identifiers[:, 1])
    blocks = np.unique(identifiers[:, 2])
    spds = len(blocks) - 1
    print(f"Found {len(delay_steps)} delay steps and {spds} on-off blocks in the identifiers.")

    # calculate the spss
    _, counts = np.unique(identifiers[:, 2], return_counts=True)
    values, value_counts = np.unique(counts, return_counts=True)

    # initialize the ATAS 2D array for the ON and OFF spectra
    atas_spectrum = np.zeros((len(delay_steps), len(xuv_energies_eV)), dtype=np.float64)

    # set up the ON and OFF spectra (use as SUMs)
    off_spectrum = np.zeros(len(xuv_energies_eV), dtype=np.float64)
    on_spectrum = np.zeros(len(xuv_energies_eV), dtype=np.float64)

    # NEW: counters for unequal counts
    on_count = 0
    off_count = 0

    # calculate the mean off spectrum (unchanged)
    mean_off_spectrum = np.mean(xuv_spectra[identifiers[:, 0] == DataLabel.OFF], axis=0)

    eps = 1e-12  # NEW: guard to avoid log(0)/division-by-zero

    # loop over the delay steps and blocks
    for delay_step in delay_steps:
        indices = np.where(identifiers[:, 1] == delay_step)[0]

        spds_counter = 0
        # NEW: reset accumulators per delay step
        on_spectrum.fill(0.0); off_spectrum.fill(0.0)
        on_count = 0; off_count = 0

        for idx in indices:
            # apply a shift ONCE (fixes the double-shift bug)
            idx = idx + shift
            if idx >= len(xuv_spectra) or idx < 0:
                continue

            # accumulate sums and counts
            if identifiers[idx, 0] == DataLabel.ON:
                on_spectrum += xuv_spectra[idx]  # <- no + shift here
                on_count += 1
            elif identifiers[idx, 0] == DataLabel.OFF:
                off_spectrum += xuv_spectra[idx]
                off_count += 1
            else:
                continue

            # if current is OFF and the next is not OFF, close and compute OD
            if (idx + 1 < len(xuv_spectra)
                    and identifiers[idx, 0] == DataLabel.OFF
                    and identifiers[idx + 1, 0] != DataLabel.OFF):

                if on_count > 0 and off_count > 0:
                    on_mean = on_spectrum / on_count
                    off_mean = off_spectrum / off_count

                    # OD = ln(ON/OFF), with guards for zeros/negatives
                    OD_values = np.log(
                        np.maximum(on_mean, eps) / np.maximum(off_mean, eps)
                    )

                    # accumulate into ATAS spectrum (keep your normalization by spds)
                    atas_spectrum[int(delay_step), :] += OD_values / spds

                # reset for the next pair within this delay step
                on_spectrum.fill(0.0); off_spectrum.fill(0.0)
                on_count = 0; off_count = 0

                spds_counter += 1

        # print(f"Finished delay step {delay_step} with {spds_counter} blocks.")

    # flip the ATAS spectrum if needed
    if flip:
        atas_spectrum = atas_spectrum * -1

    # calculate the timestep axis for the x-axis of the plot
    if delay_stepsize_mm > 0:
        timestep_fs = distance_mm_to_delay_fs(delay_stepsize_mm)
        delay_steps_fs = np.arange(len(delay_steps)) * timestep_fs
        # x_label = rf"Pump-Probe Delay $\tau$ [fs], $\Delta \tau \approx ${int(timestep_fs)} fs"
        x_label = rf"Pump-Probe Delay [fs]"
    else:
        delay_steps_fs = np.arange(len(delay_steps))
        x_label = "Delay Step"

    # Plot the ATAS spectrum
    plt.figure(figsize=(10, 6))
    X, Y = np.meshgrid(delay_steps_fs, xuv_energies_eV)
    vmax = np.abs(atas_spectrum).max()
    vmin = -vmax
    # cmap = plt.get_cmap('seismic')
    cmap = plt.get_cmap('bwr')  # Blue-White-Red colormap
    norm = plt.Normalize(vmin=vmin, vmax=vmax)

    # The pcm object is what we'll make interactive
    pcm = plt.pcolormesh(X, Y, atas_spectrum.T, shading='auto', cmap=cmap, norm=norm)
    # rasterize the pcolormesh for better performance and saving
    pcm.set_rasterized(True)

    plt.xlabel(x_label)
    plt.ylabel("XUV Energy [eV]")
    plt.colorbar(pcm, label=rf"Pump-Induced Signal [$\Delta$mOD]")
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, bbox_inches="tight")

    plt.show()


    # PLOT EXTRACTED LINES:
    # --- Find index closest to 28 eV ---
    # target_energy = 28.0  # eV
    # idx_28eV = np.argmin(np.abs(xuv_energies_eV - target_energy))

    # # --- Extract the line ---
    # line_at_28eV = atas_spectrum[:, idx_28eV]

    # # --- Plot the extracted line ---
    # plt.figure(figsize=(10, 4))
    # plt.plot(delay_steps_fs, line_at_28eV, label=f"{target_energy} eV")
    # plt.xlabel("Delay (fs)")
    # plt.ylabel(rf"Pump-Induced Signal [$\Delta$mOD]")
    # plt.title(f"ATAS Lineout at {target_energy} eV")
    # plt.legend()
    # plt.grid(True)
    # plt.tight_layout()
    # plt.show()

    # # extract the line at the specified time if requested
    # extracted_line, picked_time_fs, idx = extract_atas_line_at_time(
    #     atas_spectrum, delay_steps_fs, extract_line_fs
    # )

    # If a specific line is requested, the helper below can be used by callers separately.
    # We always return the full 2D ATAS spectrum for downstream processing (e.g., MATLAB).
    return atas_spectrum


def plot_on_off_clas_shift_sweep(
    xuv_spectra: np.ndarray,
    xuv_energies_eV: np.ndarray,
    identifiers: np.ndarray,
    delay_stepsize_mm: float = -1.0,
    flip: bool = False,
    use_background_correction: bool = True,
    save_path: Optional[str] = None,
) -> None:
    """
    Sweep the ON/OFF classical OD calculation over integer shifts in [-4, 4] using
    the *same logic* as in `plot_on_off_clas` (including the double-shift on spectra
    access), and plot a 3x3 grid of results.

    :param xuv_spectra: 2D array (n_samples, n_energy) of XUV spectra.
    :param xuv_energies_eV: 1D array (n_energy,) of XUV energies in eV.
    :param identifiers: 2D int array (n_samples, 3): [DataLabel, delay_step, block].
    :param delay_stepsize_mm: If > 0, converts delay steps to femtoseconds via `distance_mm_to_delay_fs`.
    :param flip: If True, multiplies OD values by -1 (sign flip).
    :param use_background_correction: If True, subtracts the 50th percentile per delay row (axis=1).
    :param save_path: If provided, saves the 3×3 figure at this path.
    :return: None
    """
    # --- Delay axis like in your original ---
    delay_steps = np.unique(identifiers[:, 1])
    blocks = np.unique(identifiers[:, 2])
    spds = len(blocks) - 1  # exactly like your code
    n_energy = xuv_energies_eV.shape[0]

    if delay_stepsize_mm > 0:
        timestep_fs = distance_mm_to_delay_fs(delay_stepsize_mm)
        delay_axis = np.arange(len(delay_steps)) * timestep_fs
        x_label = r"Pump-Probe Delay [fs]"
    else:
        delay_axis = np.arange(len(delay_steps))
        x_label = "Delay Step"

    eps = 1e-12  # numerical guard for log

    # === Helper that reproduces your original block logic verbatim (including double shift on spectra) ===
    def compute_atas_spectrum_for_shift(shift: int) -> np.ndarray:
        atas_spectrum = np.zeros((len(delay_steps), n_energy), dtype=np.float64)
        on_spectrum = np.zeros(n_energy, dtype=np.float64)
        off_spectrum = np.zeros(n_energy, dtype=np.float64)

        for dsi, delay_step in enumerate(delay_steps):
            indices = np.where(identifiers[:, 1] == delay_step)[0]

            # reset accumulators for each delay step like in your code
            on_spectrum[:] = 0.0
            off_spectrum[:] = 0.0
            spds_counter = 0

            for idx0 in indices:
                # apply a shift if needed (exactly as in your function)
                idx = idx0 + shift
                if idx < 0 or idx >= len(xuv_spectra):
                    continue

                # === Your original uses identifiers[idx, 0] to decide ON/OFF,
                #     but accumulates xuv_spectra[idx + shift] (double shift) ===
                x_idx = idx + shift
                if x_idx < 0 or x_idx >= len(xuv_spectra):
                    # safety guard to avoid IndexError at the edges for larger shifts
                    continue

                if identifiers[idx, 0] == DataLabel.ON:
                    on_spectrum += xuv_spectra[x_idx]
                elif identifiers[idx, 0] == DataLabel.OFF:
                    off_spectrum += xuv_spectra[x_idx]
                else:
                    continue

                # end-of-OFF-block detection (exactly like your code)
                if (idx + 1) < len(xuv_spectra) and identifiers[idx, 0] == DataLabel.OFF and identifiers[idx + 1, 0] != DataLabel.OFF:
                    ratio = (on_spectrum + eps) / (off_spectrum + eps)
                    OD_values = np.log(ratio)

                    atas_spectrum[int(delay_step), :] += OD_values / max(spds, 1)
                    # reset accumulators
                    on_spectrum[:] = 0.0
                    off_spectrum[:] = 0.0
                    spds_counter += 1

        if flip:
            atas_spectrum *= -1.0

        # mandatory 50th percentile unless disabled
        if use_background_correction:
            atas_spectrum = atas_spectrum - np.percentile(atas_spectrum, 50, axis=1, keepdims=True)

        return atas_spectrum

    # --- Compute all 9 shifts with exactly the same logic ---
    shifts = list(range(-4, 5))  # [-4, ..., +4]
    atas_list = [compute_atas_spectrum_for_shift(s) for s in shifts]

    # consistent color scaling across subplots
    all_vals = np.stack(atas_list, axis=0)
    vmax = float(np.nanmax(np.abs(all_vals)))
    vmin = -vmax
    cmap = plt.get_cmap('bwr')
    norm = plt.Normalize(vmin=vmin, vmax=vmax)

    # --- Plot 3x3 grid ---
    X, Y = np.meshgrid(delay_axis, xuv_energies_eV)
    fig, axs = plt.subplots(3, 3, figsize=(15, 12), sharex=True, sharey=True)
    fig.subplots_adjust(hspace=0.3, wspace=0.25)

    pcm_last = None
    for i in range(3):
        for j in range(3):
            ax = axs[i, j]
            s = shifts[i * 3 + j]
            pcm_last = ax.pcolormesh(X, Y, atas_list[i * 3 + j].T, shading='auto', cmap=cmap, norm=norm)
            ax.set_title(f"shift = {s}", fontsize=10)
            if i == 2:
                ax.set_xlabel(x_label)
            if j == 0:
                ax.set_ylabel("XUV Energy [eV]")

    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])
    fig.colorbar(pcm_last, cax=cbar_ax, label=rf"Pump-Induced Signal [$\Delta$mOD]")

    bc_note = "with 50th-percentile background" if use_background_correction else "without background correction"
    fig.suptitle(f"ATAS (ON/OFF, log ratio) shift sweep (−4…+4), {bc_note}", fontsize=14)
    plt.tight_layout(rect=[0, 0, 0.9, 0.95])

    if save_path:
        plt.savefig(save_path, bbox_inches="tight")
    plt.show()


def extract_atas_line_at_time(
    atas_spectrum: np.ndarray,
    delay_steps_fs: np.ndarray,
    extract_line_fs: float
) -> Tuple[np.ndarray, float, int]:
    """
    Extracts the 1D ATAS spectrum line closest to a given delay time.

    :param atas_spectrum: 2D array shaped (n_delays, n_energies).
    :param delay_steps_fs: 1D array of delays in fs (length n_delays).
    :param extract_line_fs: Target time in fs.
    :return: Tuple containing:
        - line_1d (np.ndarray): The extracted spectrum line.
        - picked_time_fs (float): The actual delay time used.
        - picked_index (int): The index of the extracted line.
    """
    delay_steps_fs = np.asarray(delay_steps_fs, dtype=float)
    idx = int(np.argmin(np.abs(delay_steps_fs - extract_line_fs)))
    return atas_spectrum[idx, :], float(delay_steps_fs[idx]), idx


####################################################################################################
# Functions for miscellaneous tasks ################################################################
####################################################################################################
def distance_mm_to_delay_fs(distance_mm: float) -> float:
    """
    Convert delay stage travel distance to optical delay in femtoseconds.

    :param distance_mm: Distance moved by the delay stage in millimeters.
    :return: Corresponding time delay in femtoseconds.
    """
    c_mm_per_ns = 299.792458  # speed of light in mm/ns
    delay_fs = (2 * distance_mm / c_mm_per_ns) * 1e6  # factor 2 for round-trip, ns -> fs
    return delay_fs

def delay_fs_to_distance_mm(delay_fs: float) -> float:
    """
    Convert optical delay in femtoseconds to delay stage travel distance in millimeters.

    :param delay_fs: Time delay in femtoseconds.
    :return: Corresponding distance moved by the delay stage in millimeters.
    """
    c_mm_per_ns = 299.792458  # speed of light in mm/ns
    distance_mm = (delay_fs / 1e6) * c_mm_per_ns / 2  # factor 2 for round-trip, fs -> ns
    return distance_mm
blob
mark :32
data 25353
# -*- coding: utf-8 -*-
"""
This script processes raw data from an Attosecond Transient Absorption
Spectroscopy (ATAS) experiment. It focuses on synchronising XUV spectra
with delay stage positions based on timestamps.

The main steps are:
1.  Load XUV spectra, XUV timestamps, and delay stage trigger timestamps.
2.  Identify synchronisation patterns within the XUV timestamps. These patterns
    correspond to the start of each delay scan.
3.  Segment the continuous XUV data into chunks, where each chunk represents
    one full scan of the delay stage.
4.  Within each chunk, label individual spectra as 'ON' (pump and probe beams
    are present), 'OFF' (only probe beam is present), or 'DISCARDED' based on
    the known experimental sequence.
5.  Generate and display the final ATAS trace (ΔA) by processing the labelled
    'ON' and 'OFF' spectra.
"""
import json
import os
import re
import sys
import warnings
from enum import IntEnum
from typing import Any, Dict, List, Optional, Tuple, cast

import matplotlib.pyplot as plt
import numpy as np

# Assuming the 'misc_functions' module is in the specified path and contains the necessary plotting functions.
sys.path.append('C:\\Users\\Moritz\\OneDrive - ETH Zurich\\Code for ThinkPad to test!')
from misc_functions import atas_on_off_clas, plot_on_off_clas_shift_sweep

# GLOBAL CONSTANTS #############################################################
SYNC_GAPS_PATTERN = [1, 4, 3, 3, 1]  # Always the same for all measurements
XUV_EV_CALIBRATION_FILE = "Spec.txt"  # Always the same for all measurements

# MEASUREMENT CONFIGURATION ####################################################
# Manually define these parameters for each measurement
XUV_DATE = "250903"                         # YYMMDD format
XUV_SEQUENCE = 3                            # X for YYMMDD_00X
DELAY_STAGE_DATE = XUV_DATE                 # YYMMDD format (same as XUV_DATE typically)
DELAY_STAGE_SEQUENCE = XUV_SEQUENCE         # X for YYMMDD_00X (can be different from XUV)
SYNC_PATTERN_TOLERANCE_US = 40000           # Tolerance for detecting sync pattern gaps
SYNC_PULSES_TO_SKIP = 3                     # Number of initial pulses to discard
PULSES_TO_SKIP_BEFORE_FIRST_SHUTTER = 5     # Number of pulses to skip before shutter
SHIFT = -1                                  # Manual shift for ON/OFF labelling


class Paths:
    """Defines the main directory paths for input data."""
    BASE_DIR = r"C:\Users\Moritz\Desktop\TESTDATA"
    SERVER_DIR = r"Z:\Attoline"  # Base server directory
    # The folder where all generated plots and data will be stored.
    OUTPUT_FOLDER = r"C:\Users\Moritz\Desktop\Pixis_data\output"


def find_xuv_files(yymmdd: str, sequence_num: int) -> Tuple[str, str]:
    """
    Find XUV .npy and .json files based on the new directory structure.
    
    Args:
        yymmdd: Date in YYMMDD format (e.g., "250903")
        sequence_num: Sequence number (e.g., 1 for _001)
        
    Returns:
        Tuple of (json_path, npy_path)
    """
    year = "20" + yymmdd[:2]  # Convert YY to YYYY
    sequence_str = f"{yymmdd}_{sequence_num:03d}"
    
    # Build the directory path
    data_dir = os.path.join(Paths.BASE_DIR, year, "STRA", yymmdd, sequence_str)
    
    json_path = os.path.join(data_dir, f"{sequence_str}.json")
    npy_path = os.path.join(data_dir, f"{sequence_str}.npy")
    
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"JSON file not found: {json_path}")
    if not os.path.exists(npy_path):
        raise FileNotFoundError(f"NPY file not found: {npy_path}")
        
    return json_path, npy_path


def find_delay_stage_file(yymmdd: str, sequence_num: int) -> str:
    """
    Find delay stage .npz file based on the new directory structure.
    
    Directory structure: server_dir/2025/DelayStage/2025/DelayStage/YYMMDD/YYMMDD_XXX/delay_YYMMDD_SXXX.npz
    
    Args:
        yymmdd: Date in YYMMDD format (e.g., "250903")
        sequence_num: Sequence number (e.g., 2 for S002)
        
    Returns:
        Full path to the delay stage .npz file
    """
    year = "20" + yymmdd[:2]  # Convert YY to YYYY
    sequence_str = f"{yymmdd}_{sequence_num:03d}"
    
    # Build the directory path: server_dir/2025/DelayStage/2025/DelayStage/250903/250903_002/
    data_dir = os.path.join(
        Paths.SERVER_DIR, 
        year, 
        "DelayStage", 
        year, 
        "DelayStage", 
        yymmdd, 
        sequence_str
    )
    
    # Build the filename: delay_250903_S002.npz
    filename = f"delay_{yymmdd}_S{sequence_num:03d}.npz"
    npz_path = os.path.join(data_dir, filename)
    
    if not os.path.exists(npz_path):
        raise FileNotFoundError(f"Delay stage NPZ file not found: {npz_path}")
        
    return npz_path


class Settings:
    """Contains global settings and flags for the script's execution."""
    # Plot an overview of all timestamps and detected sync points. Useful for debugging timing issues.
    PLOT_TIMING_OVERVIEW = False
    # Plot the final on/off spectra and the calculated ATAS trace.
    PLOT_ON_OFF_SPECTRUM = True


class DataLabel(IntEnum):
    """Enumeration for labelling each acquired data point."""
    OFF = 0  # Pump beam blocked
    ON = 1  # Pump and probe beams present
    TRAINING = 2  # Data not used for ON/OFF, potentially for ML
    DISCARDED = 3  # Data to be ignored entirely


def get_metadata(json_path: str) -> Dict:
    """Loads metadata from a specified JSON file.

    Args:
        json_path: Path to the metadata JSON file.

    Returns:
        A dictionary containing the metadata.
    """
    with open(json_path, "r") as f:
        metadata = json.load(f)
    return metadata


def load_xuv_spectra_and_timestamps(json_path: str) -> Tuple[np.ndarray, np.ndarray]:
    """Loads XUV spectra and their timestamps from a memory-mapped file.

    Args:
        json_path: Path to the metadata JSON file that describes the memmap file.

    Returns:
        A tuple containing:
        - A 2D NumPy array of spectra (N_frames, N_pixels).
        - A 1D NumPy array of timestamps in microseconds.
    """
    metadata = get_metadata(json_path)
    print('the json path is at ', json_path)
    memmap_path = os.path.join(os.path.dirname(json_path), metadata["memmap_file"])

    # Define the structured data type for the memory-mapped file
    dtype = np.dtype([
        ("spectrum", np.uint16, metadata["spectra_shape"][1]),
        ("timestamp_us", np.uint64)
    ])

    # Read the data and convert from memmap to standard numpy arrays
    mmap = np.memmap(memmap_path, dtype=dtype, mode="r")
    return np.array(mmap["spectrum"]), np.array(mmap["timestamp_us"])


def load_delay_stage_data(filename: str) -> Tuple[np.ndarray, int, int, int, Optional[float]]:
    """Reads delay stage data, extracting timestamps and experimental parameters from the .npz file.

    Args:
        filename: Path to the .npz file containing delay stage timestamps and parameters.

    Returns:
        A tuple containing:
        - timestamps_us (np.ndarray): Trigger timestamps in microseconds.
        - ppas (int): Pulses per acquisition step.
        - spss (int): Spectra per shutter step (duration of ON or OFF block).
        - spds (int): Steps per delay scan (number of ON/OFF pairs).
        - move_step_fs (float | None): Move step size in femtoseconds, if found.
    """
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File not found: {filename}")
    
    # Load data from the .npz file
    print('filename:', filename)
    
    try:
        npz_data = np.load(filename)
        available_keys = list(npz_data.keys())
        print(f"Available arrays in .npz file: {available_keys}")
        
        # Load timestamps
        timestamp_key = None
        for key in ['timestamps', 'timestamps_us', 'data', 'arr_0']:
            if key in available_keys:
                timestamp_key = key
                break
        
        if timestamp_key is None:
            raise ValueError(f"Could not find timestamp data in .npz file. Available keys: {available_keys}")
        
        timestamps_us = np.array(npz_data[timestamp_key])
        timestamps_us = timestamps_us[timestamps_us > 0]  # Filter out zero entries
        print(f"Using key '{timestamp_key}' for timestamp data")
        
        # Load parameters from .npz file variables
        ppas = int(npz_data['ppas']) if 'ppas' in available_keys else None
        spss = int(npz_data['spss']) if 'spss' in available_keys else None
        spds = int(npz_data['spds']) if 'spds' in available_keys else None
        move_step_fs = float(npz_data['move_step_fs']) if 'move_step_fs' in available_keys else None
        
        # Check that required parameters were found
        if ppas is None or spss is None or spds is None:
            # Fallback: try to parse from filename for backward compatibility
            print("Warning: Could not find ppas, spss, spds in .npz file, trying filename parsing...")
            match = re.search(r"ppas_(\d+)_spss_(\d+)_spds_(\d+)", filename)
            if not match:
                raise ValueError("Could not find ppas, spss, and spds in .npz file or filename.")
            ppas = int(match.group(1))
            spss = int(match.group(2))
            spds = int(match.group(3))
        
        print(f"Parameters: ppas={ppas}, spss={spss}, spds={spds}, move_step_fs={move_step_fs}")
        
        return timestamps_us, ppas, spss, spds, move_step_fs
        
    except Exception as e:
        raise ValueError(f"Error loading .npz file: {e}")
    finally:
        if 'npz_data' in locals():
            npz_data.close()


def find_sync_patterns(timestamps_us: np.ndarray, expected_gaps_us: List[int], tolerance_us: int) -> np.ndarray:
    """Detects synchronisation patterns in a timestamp array.

    A pattern is defined by a specific sequence of time gaps between consecutive timestamps.

    Args:
        timestamps_us: An array of timestamps in microseconds.
        expected_gaps_us: A list of the expected time gaps that form the sync pattern.
        tolerance_us: The allowed deviation in microseconds for each gap.

    Returns:
        An array of indices, where each index points to the start of a detected sync pattern.
    """
    sync_indices = []
    num_gaps = len(expected_gaps_us)
    i = 0
    # Iterate through the timestamps to find the pattern
    while i <= len(timestamps_us) - (num_gaps + 1):
        # Get a slice of timestamps that could form one pattern
        t_slice = timestamps_us[i:i + num_gaps + 1]
        gaps = np.diff(t_slice)

        # Check if all calculated gaps match the expected gaps within the tolerance
        if all(np.abs(g - e) < tolerance_us for g, e in zip(gaps, expected_gaps_us)):
            sync_indices.append(i)
            # Skip forward to prevent re-detecting overlapping patterns.
            # A cooldown of num_gaps is a safe choice.
            i += num_gaps
        else:
            i += 1
    return np.array(sync_indices)


def create_on_off_mask(chunk_length: int, spss: int, spds: int,
                       sync_pulses_to_skip: int, pulses_to_skip: int,
                       shift: int) -> np.ndarray:
    """Creates a mask to label each data point in a chunk as ON, OFF, TRAINING, or DISCARDED.

    Args:
        chunk_length: The total number of data points in the chunk.
        spss: Spectra per shutter step (duration of an ON or OFF block).
        spds: Steps per delay scan (number of ON/OFF pairs).
        sync_pulses_to_skip: Number of initial pulses to label as DISCARDED.
        pulses_to_skip: Number of subsequent pulses to label as TRAINING.
        shift: A manual timing shift to apply to the ON/OFF blocks.

    Returns:
        A 1D NumPy array of labels (see DataLabel enum) for the chunk.
    """
    # Start by labelling everything as TRAINING.
    mask = np.full(chunk_length, DataLabel.TRAINING, dtype=np.uint8)

    # Label initial pulses for synchronisation and stabilisation.
    mask[:sync_pulses_to_skip] = DataLabel.DISCARDED
    mask[sync_pulses_to_skip:pulses_to_skip] = DataLabel.TRAINING

    # Apply the ON/OFF pattern for each delay step.
    current_index = pulses_to_skip + shift
    for _ in range(spds):
        # ON block
        mask[current_index:current_index + spss] = DataLabel.ON
        current_index += spss
        # OFF block
        mask[current_index:current_index + spss] = DataLabel.OFF
        current_index += spss
    return mask

# def prepare_atas_inputs(selected_set_name: Optional[str] = None) -> Dict[str, object]:
#     """Prepare inputs for ATAS computation without plotting.

#     This function mirrors the loading, synchronization, chunking, and labeling
#     steps from main(), but returns the assembled arrays and metadata in a dict
#     so that MATLAB (MoritzReader.m) can compute and plot ATAS.

#     Args:
#         selected_set_name: Optional name of the training set to use (e.g., "TS0028").
#                            If None, uses TrainingSets.SELECTED.

#     Returns:
#         A dictionary with keys:
#             - 'final_spectra' (np.ndarray, float32) [N x E]
#             - 'final_identifiers' (np.ndarray, int32) [N x 3]
#             - 'xuv_energy_ev' (np.ndarray, float32) [E]
#             - 'delay_stepsize_mm' (float)
#             - 'ppas' (int), 'spss' (int), 'spds' (int)
#             - 'config_name' (str)
#     """
#     # Resolve configuration
#     config: Dict[str, Any]
#     if selected_set_name is None:
#         config = cast(Dict[str, Any], TrainingSets.SELECTED)
#     else:
#         # Allow matching by attribute name (e.g., "TS0028") or by config['Name']
#         found: Optional[Dict[str, Any]] = None
#         for attr in dir(TrainingSets):
#             if attr.startswith("TS"):
#                 cfg = getattr(TrainingSets, attr)
#                 if isinstance(cfg, dict) and (attr == selected_set_name or cfg.get("Name") == selected_set_name):
#                     found = cast(Dict[str, Any], cfg)
#                     break
#         if found is None:
#             available = [a for a in dir(TrainingSets) if a.startswith("TS")]
#             raise ValueError(f"Unknown training set '{selected_set_name}'. Available: {available}")
#         config = found

#     # Locate files
#     # xuv_data_file = [f for f in os.listdir(Paths.XUV_SPECTRA) if f.startswith(config["XUV_date"]) and f.endswith(".npy")]
#     # xuv_meta_file = [f for f in os.listdir(Paths.XUV_SPECTRA) if f.startswith(config["XUV_date"]) and f.endswith(".json")]
#     # delay_file = [f for f in os.listdir(Paths.DELAY_STAGE_TIMES) if f.startswith(config["DELAY_STAGE_date"]) and f.endswith(".npz")]
#     # if not (len(xuv_data_file) == 1 and len(xuv_meta_file) == 1 and len(delay_file) == 1):
#     #     raise FileNotFoundError("Could not find exactly one file for each data type (XUV data, XUV meta, Delay).")

#     # Load data
#     # xuv_spectra, xuv_timestamps_us = load_xuv_spectra_and_timestamps(os.path.join(Paths.XUV_SPECTRA, xuv_meta_file[0]))
#     # delay_timestamps_us, ppas, spss, spds, delay_stepsize_mm = load_delay_stage_data(os.path.join(Paths.DELAY_STAGE_TIMES, delay_file[0]))
#     # xuv_energy_ev = np.loadtxt(os.path.join(Paths.XUV_SPECTRA, config["XUV_eV_calibration_file"]))
    


#     # Synchronisation: detect start of scans via sync gaps pattern
#     # sync_gaps_us: List[int] = [int(round(float(element) * (ppas * (1 / 1030) * 1e6))) for element in config["SYNC_GAPS_PATTERN"]]
#     # sync_indices = find_sync_patterns(xuv_timestamps_us, sync_gaps_us, config["SYNC_PATTERN_TOLERANCE_US"]) + 2

#     # Segment into chunks (one per delay scan)
#     chunk_indices = np.split(np.arange(len(xuv_spectra)), sync_indices)[1:]
#     chunk_lengths = [len(c) for c in chunk_indices]
#     if len(chunk_lengths) == 0:
#         raise RuntimeError("No chunks detected from XUV sync patterns; check timestamps or pattern settings.")

#     # Build ON/OFF mask using first complete chunk length
#     chunk_length = chunk_lengths[0]
#     on_off_mask = create_on_off_mask(
#         chunk_length=chunk_length,
#         spss=spss,
#         spds=spds,
#         sync_pulses_to_skip=config["SYNC_PULSES_TO_SKIP"],
#         pulses_to_skip=config["PULSES_TO_SKIP_BEFORE_FIRST_SHUTTER"],
#         shift=config["SHIFT"],
#     )

#     final_spectra: List[np.ndarray] = []
#     # identifiers: (DataLabel, chunk_index, on_off_block_index)
#     final_identifiers: List[Tuple[int, int, int]] = []

#     for chunk_idx, indices in enumerate(chunk_indices):
#         if len(indices) < chunk_length:
#             # skip incomplete last chunk
#             continue
#         block_counter = -1
#         prev_label = None
#         for i in range(chunk_length):
#             label = on_off_mask[i]
#             if label == DataLabel.DISCARDED:
#                 continue
#             if label == DataLabel.ON and prev_label != DataLabel.ON:
#                 block_counter += 1
#             prev_label = label
#             spectrum_index = indices[i]
#             final_spectra.append(xuv_spectra[spectrum_index])
#             final_identifiers.append((int(label), int(chunk_idx), int(block_counter if label != DataLabel.TRAINING else -1)))

#     # Convert to arrays (types friendly for MATLAB)
#     final_spectra_arr = np.asarray(final_spectra, dtype=np.float32)
#     final_identifiers_arr = np.asarray(final_identifiers, dtype=np.int32)
#     xuv_energy_ev_arr = np.asarray(xuv_energy_ev, dtype=np.float32)

#     # Default delay step if missing
#     if not delay_stepsize_mm:
#         delay_stepsize_mm = 0.05

#     return {
#         "final_spectra": final_spectra_arr,
#         "final_identifiers": final_identifiers_arr,
#         "xuv_energy_ev": xuv_energy_ev_arr,
#         "delay_stepsize_mm": float(delay_stepsize_mm),
#         "ppas": int(ppas),
#         "spss": int(spss),
#         "spds": int(spds),
#         "config_name": str(config["Name"]),
#     }

def main():
    """Main function to run the data processing pipeline."""
    
    # --- 1. Setup and Data Loading ---
    print("\n--- LOADING DATA ---")
    if not os.path.exists(Paths.OUTPUT_FOLDER):
        os.makedirs(Paths.OUTPUT_FOLDER)
        print(f"Created output directory: {Paths.OUTPUT_FOLDER}")

    print(f"XUV Date: {XUV_DATE}, Sequence: {XUV_SEQUENCE}")
    print(f"Delay Stage Date: {DELAY_STAGE_DATE}, Sequence: {DELAY_STAGE_SEQUENCE}")
    
    # Find XUV files using the new directory structure
    xuv_json_path, xuv_npy_path = find_xuv_files(XUV_DATE, XUV_SEQUENCE)
    print(f"XUV JSON: {xuv_json_path}")
    print(f"XUV NPY: {xuv_npy_path}")

    # Find delay stage file using the new directory structure
    delay_file_path = find_delay_stage_file(DELAY_STAGE_DATE, DELAY_STAGE_SEQUENCE)
    print(f"Delay stage file: {delay_file_path}")

    # Load data from files
    xuv_spectra, xuv_timestamps_us = load_xuv_spectra_and_timestamps(xuv_json_path)
    print(load_delay_stage_data(delay_file_path))
    delay_timestamps_us, ppas, spss, spds, move_step_fs = load_delay_stage_data(delay_file_path)
    
    # Load energy calibration file from the workspace directory (always Spec.txt)
    spec_file_path = os.path.join(os.path.dirname(__file__), XUV_EV_CALIBRATION_FILE)
    xuv_energy_ev = np.loadtxt(spec_file_path)
    print("\nLOADED DATA SHAPES:")
    print(f"\tXUV Spectra: {xuv_spectra.shape}")
    print(f"\tXUV Timestamps: {xuv_timestamps_us.shape}")
    print(f"\tDelay Timestamps: {delay_timestamps_us.shape}")
    print(f"\tSPSS: {spss}, SPDS: {spds}, Move Step: {move_step_fs} fs")
    if move_step_fs is not None:
        print(f"\tMove step in fs: {move_step_fs:.2f} fs")
    print(f"\tPPAS: {ppas}")
    # --- 2. Synchronisation and Chunking ---
    print("\n--- SYNCHRONISING DATA ---")
    # Find sync patterns in the XUV data, which mark the beginning of each delay scan.
    # An offset of +2 is added based on empirical observation of the acquisition timing.
    sync_gaps_us = [element * (ppas * (1 / 1030) * 1e6) for element in SYNC_GAPS_PATTERN]
    sync_indices = find_sync_patterns(xuv_timestamps_us, sync_gaps_us, SYNC_PATTERN_TOLERANCE_US) + 2

    expected_scans = len(delay_timestamps_us) // 2
    print(f"Expected {expected_scans} scans based on delay stage triggers.")
    print(f"Found {len(sync_indices)} sync patterns in XUV timestamps.")
    if len(sync_indices) != expected_scans:
        warnings.warn("Mismatch between number of sync patterns and expected delay scans.")

    if Settings.PLOT_TIMING_OVERVIEW:
        plt.figure(figsize=(15, 5))
        plt.vlines(xuv_timestamps_us * 1e-6, 0, 0.8, color='blue', alpha=0.3, label='XUV Timestamps')
        plt.scatter(xuv_timestamps_us[sync_indices] * 1e-6, [1.0] * len(sync_indices), color='cyan', marker='^', s=60, label='Detected XUV Sync')
        plt.vlines(delay_timestamps_us * 1e-6, 0, 1.2, color='orange', alpha=1, label='Delay Stage Triggers')
        plt.xlabel("Time [s]")
        plt.title("Timestamp Overview")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(Paths.OUTPUT_FOLDER, "sync_sequences_overview.png"), dpi=300)
        plt.show()

        # plot xuv spectra summed over the energies:
    # xuv_summed = np.sum(xuv_spectra, axis=1)
    # plt.figure()
    # plt.plot(xuv_timestamps_us, xuv_summed)
    # plt.xlabel("Time [us]")
    # plt.ylabel("Summed XUV Signal")
    # plt.title("XUV Spectra Summed Over Energies")
    # plt.grid()
    # plt.show()
    # Split the continuous data stream into chunks based on the sync indices.
    # Each chunk corresponds to one full delay scan.
    chunk_indices = np.split(np.arange(len(xuv_spectra)), sync_indices)[1:]
    # All chunks should ideally have the same length for consistent processing.
    chunk_lengths = [len(c) for c in chunk_indices]
    if len(set(chunk_lengths[:-1])) > 1: # Check all but the last chunk
        warnings.warn(f"Chunk lengths are not uniform: {chunk_lengths}")

    # --- 3. Labelling and Data Restructuring ---
    print("\n--- LABELLING AND RESTRUCTURING DATA ---")
    if not Settings.PLOT_ON_OFF_SPECTRUM:
        print("Skipping plotting as per settings.")
        return

    # Use the first complete chunk's length to create the ON/OFF mask.
    chunk_length = chunk_lengths[0]
    on_off_mask = create_on_off_mask(
        chunk_length=chunk_length,
        spss=spss,
        spds=spds,
        sync_pulses_to_skip=SYNC_PULSES_TO_SKIP,
        pulses_to_skip=PULSES_TO_SKIP_BEFORE_FIRST_SHUTTER,
        shift=SHIFT
    )

    # Prepare lists to hold the restructured data for plotting.
    final_spectra = []
    # Identifier format: (DataLabel, chunk_index, on_off_block_index)
    final_identifiers = []

    # Iterate through each chunk (delay scan)
    for chunk_idx, indices in enumerate(chunk_indices):
        if len(indices) < chunk_length:
            print(f"Skipping incomplete chunk {chunk_idx} with length {len(indices)}.")
            continue

        block_counter = -1
        prev_label = None
        # Iterate through each spectrum within the chunk
        for i in range(chunk_length):
            label = on_off_mask[i]
            if label == DataLabel.DISCARDED:
                continue

            # Increment the ON/OFF block counter at the start of each new ON block.
            if label == DataLabel.ON and prev_label != DataLabel.ON:
                block_counter += 1
            prev_label = label

            spectrum_index = indices[i]
            final_spectra.append(xuv_spectra[spectrum_index])
            
            identifier = (label, chunk_idx, block_counter if label != DataLabel.TRAINING else -1)
            final_identifiers.append(identifier)

    # Convert lists to NumPy arrays for efficient processing.
    final_spectra = np.array(final_spectra, dtype=np.float32)
    final_identifiers = np.array(final_identifiers, dtype=np.int32)

    # --- 4. Plotting Results ---
    print("\n--- PLOTTING RESULTS ---")
    
    # Convert move_step_fs to delay_stepsize_mm for plotting functions
    # If move_step_fs is not available, use a default value
    if move_step_fs is not None:
        # Convert fs to mm: 1 fs = c * 1e-15 / 2 meters (round trip)
        # c = 3e8 m/s, so 1 fs = 1.5e-7 m = 0.15 mm
        delay_stepsize_mm = move_step_fs * 0.15e-3  # Convert fs to mm
        print(f"Using move step: {move_step_fs} fs = {delay_stepsize_mm:.6f} mm")
    else:
        delay_stepsize_mm = 0.05  # Default value if not found
        print(f"Using default delay step size: {delay_stepsize_mm} mm")

    # Apply a mask to focus on a relevant energy range.
    energy_mask = xuv_energy_ev < 86
    atas_on_off_clas(
        xuv_spectra=final_spectra[:, energy_mask],
        xuv_energies_eV=xuv_energy_ev[energy_mask],
        identifiers=final_identifiers,
        delay_stepsize_mm=delay_stepsize_mm,
        shift=0  # The shift is already applied in the mask, so this is for plotting only.
    )

    plot_on_off_clas_shift_sweep(
        xuv_spectra=final_spectra,
        xuv_energies_eV=xuv_energy_ev,
        identifiers=final_identifiers,
        delay_stepsize_mm=delay_stepsize_mm,
        use_background_correction=False
    )
    print("\n--- PROCESSING COMPLETE ---")

if __name__ == "__main__":
    main()
blob
mark :33
data 15940
import numpy as np
import time
import matplotlib
# Suggest a backend compatible with animations. 'QtAgg' is a robust choice.
matplotlib.use('QtAgg') 
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import matplotlib.ticker as mticker
from matplotlib.animation import FuncAnimation
from pylablib.devices import PrincetonInstruments
from collections import deque
from matplotlib.widgets import Button, TextBox
import os
import datetime

# --- Configuration ---
# A class to hold all settings for easy modification.
class Settings:
    """Holds static configuration parameters for the script."""
    # Exposure time in milliseconds. Note: In trigger mode, the camera waits for a trigger,
    # so this value is less critical than the trigger rate itself.
    EXP_TIME_MS = 1  
    # Binning settings (horizontal, vertical). Here, we bin all vertical pixels into one line.
    BINNING = (1, 400)
    # Expected shape of the spectrum data (rows, columns).
    SPECTRA_SHAPE = (1, 1340)
    # Path to the energy calibration file. Assumes it's in the same directory as the script.
    ENERGY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Spec.txt")

# --- Main Application Logic ---
def main():
    """Initializes the camera, sets up the plot, and runs the live display."""
    
    # --- 1. Initialization and Setup ---
    
    # Define the base directory for saving files.
    base_dir = os.path.join(os.path.expanduser("~"), "XUV_data") #C:\Users\Moritz\XUV_data
    base_dir = r'Z:\Attoline\TEST'
    
    # Load the energy axis from the calibration file.
    try:
        energy_eV = np.loadtxt(Settings.ENERGY_FILE)
    except Exception as e:
        print(f"Error: Failed to load energy axis from '{Settings.ENERGY_FILE}'. {e}")
        return

    # Validate that the energy axis matches the expected spectrum shape.
    if energy_eV.shape[0] != Settings.SPECTRA_SHAPE[1]:
        print(f"Error: Energy axis length ({energy_eV.shape[0]}) does not match spectrum length ({Settings.SPECTRA_SHAPE[1]}).")
        return

    # Initialize the Region of Interest (ROI) for standard deviation to the full spectrum.
    roi_indices = np.arange(energy_eV.shape[0])
    
    # Create a deque (a fast, double-ended queue) to buffer recent spectra for calculations.
    spectrum_buffer_2s = deque() 

    # Connect to the Princeton Instruments camera.
    try:
        print("Available cameras:", PrincetonInstruments.list_cameras())
        cam = PrincetonInstruments.PicamCamera('2105050003') # Replace with your camera's serial number if different.
        print("Camera connected successfully.")
    except Exception as e:
        print(f"Error: Could not connect to the camera. {e}")
        return

    # Configure camera acquisition settings.
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    cam.set_attribute_value("Clean Until Trigger", False)
    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    cam.set_attribute_value("Shutter Closing Delay", 0)
    time.sleep(0.2) # A brief pause to ensure settings are applied.
    
    # Start the acquisition sequence.
    cam.setup_acquisition(mode="sequence", nframes=1000)
    cam.start_acquisition()
    print("Starting live display... (Close the plot window to stop)")

    # --- 2. Plot and Widget Layout ---

    # Create the main figure and a GridSpec layout for flexible subplot arrangement.
    fig = plt.figure(figsize=(12, 8))
    gs = GridSpec(2, 2, figure=fig, height_ratios=[1, 4], width_ratios=[14, 1])

    # Define the subplots within the grid.
    ax_spec = fig.add_subplot(gs[1, 0])      # Main spectrum plot (bottom left)
    ax_max_trace = fig.add_subplot(gs[0, 0])  # Max value trace plot (top left)
    ax_std_gauge = fig.add_subplot(gs[:, 1])   # Standard deviation gauge (right column)
    
    # --- 3. Plot Artists and Styling ---

    # Artists for the main spectrum plot.
    line, = ax_spec.plot(energy_eV, np.zeros_like(energy_eV), zorder=2, color='#0072BD', label="Live Spectrum")
    ref_line, = ax_spec.plot(energy_eV, np.zeros_like(energy_eV), color='#D95319', linestyle='--', linewidth=1, label="Kept Spectrum", zorder=1)
    ref_line.set_visible(False) # Hide the reference line initially.

    # Artists for the max value trace plot.
    max_vals_buffer = deque([0] * 200, maxlen=200) # Buffer for the last 200 max values.
    max_line, = ax_max_trace.plot(list(max_vals_buffer), color='#A2142F')
    
    # Artist for the standard deviation gauge (a single vertical bar).
    bar_container = ax_std_gauge.bar(0, 0, color='#33A02C', width=1.0)
    std_dev_bar_patch = bar_container[0]

    # Artist for showing save status. Attached to an axes for blitting compatibility.
    save_status_text = ax_max_trace.text(0.75, 1.4, '', ha='center', va='bottom', fontsize=10, color='#33A02C', transform=ax_max_trace.transAxes)

    # --- Styling for the Spectrum Plot (ax_spec) ---
    ax_spec.set_title("Use up/down arrow keys to adjust Y-axis", loc='left')
    ax_spec.set_xlabel("Energy (eV)")
    ax_spec.set_ylabel("Counts")
    ax_spec.grid(True)
    initial_ylim = np.max(cam.read_newest_image().ravel().astype(np.uint16)) * 2
    ax_spec.set_ylim(0, initial_ylim)
    xmin, xmax = 20, 75 # Default X-axis limits.
    ax_spec.set_xlim(xmin, xmax)
    ax_spec.hlines(y=65535, xmin=xmin, xmax=xmax, colors='#7E2F8E', linestyles='--', linewidth=1, label="Saturation (16-bit)")
    ax_spec.legend()

    # --- Styling for the Max Trace Plot (ax_max_trace) ---
    ax_max_trace.set_title("Max Count Trace", fontsize=9)
    ax_max_trace.set_ylabel("Max", fontsize=8)
    ax_max_trace.set_xticks([]) # Hide x-axis ticks.
    ax_max_trace.tick_params(axis='y', labelsize=8)
    ax_max_trace.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    ax_max_trace.set_ylim(0, 65535)

    # --- Styling for the Standard Deviation Gauge (ax_std_gauge) ---
    ax_std_gauge.set_title("Avg. Norm. Std\n(2s window, %)", fontsize=9)
    ax_std_gauge.set_ylim(0, 5) # Set a fixed vertical scale in percent.
    ax_std_gauge.set_xlim(-0.5, 0.5) # Center the bar.
    ax_std_gauge.set_xticks([]) # Hide x-axis ticks.
    ax_std_gauge.yaxis.tick_right() # Move ticks and labels to the right.
    ax_std_gauge.yaxis.set_label_position("right")
    ax_std_gauge.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=100))
    ax_std_gauge.tick_params(labelsize=8)
    
    # --- 4. Interactive Widget Handlers ---

    def on_key(event):
        """Handles key presses to adjust the Y-axis of the spectrum plot."""
        nonlocal animation # Use nonlocal to modify the animation object defined in the outer scope.
        if animation is None: return
        
        animation.event_source.stop() # Pause animation to prevent redraw conflicts.
        current_ylim = ax_spec.get_ylim()
        if event.key == 'up':
            new_max = current_ylim[1] * 1.2
        elif event.key == 'down':
            new_max = current_ylim[1] / 1.2
        else:
            animation.event_source.start() # Resume for unhandled keys.
            return
        
        ax_spec.set_ylim(0, max(1000, new_max)) # Apply new limit, with a minimum floor.
        fig.canvas.draw_idle() # Redraw the canvas.
        animation.event_source.start() # Resume animation.

    def on_keep_clicked(event):
        """Averages the spectra from the last 2 seconds and keeps it as a reference."""
        if not spectrum_buffer_2s:
            print("No spectra in the buffer to average.")
            return

        # Extract spectra from the buffer
        spectra_to_average = np.array([item[1] for item in spectrum_buffer_2s])
        
        # Calculate the mean spectrum
        averaged_spectrum = np.mean(spectra_to_average, axis=0)
        
        ref_line.set_ydata(averaged_spectrum)
        ref_line.set_visible(True)
        print("Kept the average of the last 2 seconds.")

    def on_save_clicked(event):
        """Saves the 'kept' spectrum to an npz file."""
        if not ref_line.get_visible():
            save_status_text.set_text("No 'kept' spectrum to save. Click 'Keep' first.")
            fig.canvas.draw_idle()
            print("No 'kept' spectrum to save. Click 'Keep' first.")
            return

        kept_spectrum = ref_line.get_ydata()
        
        # --- File Path and Naming Logic ---
        now = datetime.datetime.now()
        year_str = now.strftime("%Y")
        date_str_long = now.strftime("%y%m%d")
        
        # Construct the directory path
        save_dir = os.path.join(base_dir, year_str, "XUV", date_str_long)
        os.makedirs(save_dir, exist_ok=True)
        
        # Find the next available file number for the current day
        file_index = 1
        while True:
            filename = f"XUV_{date_str_long}_{file_index:04d}.npz"
            filepath = os.path.join(save_dir, filename)
            if not os.path.exists(filepath):
                break
            file_index += 1
            
        # --- Save the Data ---
        try:
            np.savez_compressed(filepath, energy_eV=energy_eV, counts=kept_spectrum)
            status_msg = f"Saved to: {filepath}"
            print(status_msg)
            save_status_text.set_text(status_msg)
        except Exception as e:
            status_msg = f"Error saving file: {e}"
            print(status_msg)
            save_status_text.set_text(status_msg)
        
        fig.canvas.draw_idle()

    def on_roi_submit(text):
        """Updates the ROI for the standard deviation calculation from text boxes."""
        nonlocal roi_indices
        try:
            min_val = float(text_box_min.text)
            max_val = float(text_box_max.text)
        except ValueError:
            print("Invalid ROI input. Please enter numbers.")
            return

        if min_val >= max_val:
            print("Min ROI must be less than Max ROI.")
            return
        
        # Find the indices of the energy axis that fall within the specified range.
        roi_indices = np.where((energy_eV >= min_val) & (energy_eV <= max_val))[0]
        print(f"Std Dev ROI set to {min_val:.1f}-{max_val:.1f} eV.")

    def on_std_full_clicked(event):
        """Resets the standard deviation ROI to the full spectrum."""
        nonlocal roi_indices
        roi_indices = np.arange(energy_eV.shape[0])
        # Update text boxes to reflect the full range.
        text_box_min.set_val(f"{energy_eV[0]:.1f}")
        text_box_max.set_val(f"{energy_eV[-1]:.1f}")
        print("Std Dev ROI reset to full spectrum.")

    # --- Create and place widgets on the figure ---
    fig.canvas.mpl_connect('key_press_event', on_key)

    keep_button_ax = fig.add_axes([0.01, 0.92, 0.08, 0.06])
    keep_button = Button(keep_button_ax, "Keep", color='#D95319', hovercolor='#FF7F0E')
    keep_button.on_clicked(on_keep_clicked)

    save_button_ax = fig.add_axes([0.01, 0.84, 0.08, 0.06])
    save_button = Button(save_button_ax, "Save", color='#1F77B4', hovercolor='#4DBEEE')
    save_button.on_clicked(on_save_clicked)

    ax_roi_min = fig.add_axes([0.10, 0.92, 0.1, 0.06])
    text_box_min = TextBox(ax_roi_min, 'Min (eV)', initial=f"{xmin:.0f}", textalignment="right")
    text_box_min.label.set_horizontalalignment('left')
    text_box_min.on_submit(on_roi_submit)

    ax_roi_max = fig.add_axes([0.21, 0.92, 0.1, 0.06])
    text_box_max = TextBox(ax_roi_max, 'Max (eV)', initial=f"{xmax:.0f}", textalignment="right")
    text_box_max.label.set_horizontalalignment('left')
    text_box_max.on_submit(on_roi_submit)
    
    std_full_ax = fig.add_axes([0.32, 0.92, 0.1, 0.06])
    std_full_button = Button(std_full_ax, "Std Dev (Full)", color='yellow', hovercolor='orange')
    std_full_button.on_clicked(on_std_full_clicked)

    # Manually trigger the first ROI calculation with initial values from the text boxes.
    on_roi_submit(None)

    # --- 5. Animation Core ---
    def update(frame):
        """This function is called repeatedly to update the plot data."""
        current_time = time.time()
        
        # Read the latest image from the camera.
        data = cam.read_newest_image()
        if data is None:
            return line, max_line, ref_line, std_dev_bar_patch, save_status_text # Return unchanged artists if no data.

        spectrum = data.ravel().astype(np.uint16)
        if spectrum.shape[0] != energy_eV.shape[0]:
            print(f"Warning: Unexpected spectrum shape received: {spectrum.shape}")
            return line, max_line, ref_line, std_dev_bar_patch, save_status_text

        # --- Update Spectrum Plot ---
        line.set_ydata(spectrum)
        
        # --- Update Max Trace Plot ---
        max_vals_buffer.append(np.max(spectrum))
        max_line.set_ydata(list(max_vals_buffer))
        max_line.set_xdata(np.arange(len(max_vals_buffer)))
        # Dynamically adjust the y-axis of the max trace plot.
        ax_max_trace.set_ylim(0, max(max(max_vals_buffer) * 1.1, 1000))
        
        # --- Update Standard Deviation Gauge ---
        # 1. Add the new spectrum to the 2-second buffer and remove old ones.
        spectrum_buffer_2s.append((current_time, spectrum))
        while spectrum_buffer_2s and (current_time - spectrum_buffer_2s[0][0] > 2):
            spectrum_buffer_2s.popleft()
        #print(rf"Sensor Temperature Set Point: {cam.get_attribute_value('Sensor Temperature Set Point')} K")
        #print(rf"Sensor Temperature Reading: {cam.get_attribute_value('Sensor Temperature Reading')} K")  
        # 2. Calculate the average normalized standard deviation.
        avg_norm_std_percent = 0.0
        if len(spectrum_buffer_2s) >= 2: # Need at least two points to calculate std dev.
            # Stack spectra from the buffer into a 2D array (time x energy).
            spectra_over_time = np.array([item[1] for item in spectrum_buffer_2s])
            
            # Calculate standard deviation and mean for each energy bin (column-wise).
            std_per_energy = np.std(spectra_over_time, axis=0)
            mean_per_energy = np.mean(spectra_over_time, axis=0)
            
            # Calculate normalized std dev (std/mean), avoiding division by zero.
            normalized_std = np.divide(std_per_energy, mean_per_energy, 
                                      out=np.zeros_like(std_per_energy), 
                                      where=mean_per_energy != 0)
            
            # Average the normalized std dev over the user-defined ROI.
            if roi_indices.size > 0:
                avg_norm_std = np.mean(normalized_std[roi_indices])
                avg_norm_std_percent = avg_norm_std * 100 # Convert to percent for display.

        # 3. Update the height of the gauge bar.
        std_dev_bar_patch.set_height(avg_norm_std_percent)

        # Return a tuple of all artists that were modified for blitting.
        return line, max_line, ref_line, std_dev_bar_patch, save_status_text

    # Create and start the animation.
    animation = FuncAnimation(fig, update, interval=1, blit=True, cache_frame_data=False)
    plt.show()

    # --- 6. Cleanup ---
    print("Plot window closed. Disconnecting camera...")
    if cam.acquisition_in_progress():
        cam.stop_acquisition()
    cam.clear_acquisition()
    cam.close()
    print("Camera disconnected.")

# --- Script Entry Point ---
if __name__ == "__main__":
    main()
blob
mark :34
data 13004
import numpy as np
import os
import time
import json
from datetime import datetime
from pynput import keyboard
from pylablib.devices import PrincetonInstruments
import gc
import h5py
import threading
import shutil

# Global stop flag
stop_loop = False
copy_thread_stop_event = threading.Event()

# PATHS ########################################################################
class Paths:
    BASE_DIR = r"C:\Users\Moritz\Desktop\ATAS local"
    SERVER_DIR = r"Z:\Attoline" # <--- CHANGE THIS to your server path


# DIRECTORY AND FILE MANAGEMENT ###############################################
def create_data_directory_and_paths():
    """
    Creates the directory structure: base_dir/YYYY/STRA_new/YYMMDD/YYMMDD_XXX/
    and server_dir/YYYY/STRA_new/YYMMDD/YYMMDD_XXX/
    Returns the local directory path, server directory path, and base filename.
    Automatically increments XXX if directory already exists.
    """
    now = datetime.now()
    year = now.strftime("%Y")
    date_str = now.strftime("%y%m%d")
    
    # Create the base directory structure for local and server
    local_year_dir = os.path.join(Paths.BASE_DIR, year)
    local_STRA_new_dir = os.path.join(local_year_dir, "STRA_new")
    local_date_dir = os.path.join(local_STRA_new_dir, date_str)
    
    server_year_dir = os.path.join(Paths.SERVER_DIR, year)
    server_STRA_new_dir = os.path.join(server_year_dir, "STRA_new")
    server_date_dir = os.path.join(server_STRA_new_dir, date_str)

    # Create local directories if they don't exist
    os.makedirs(local_date_dir, exist_ok=True)
    
    # Find the next available sequence number
    sequence_num = 1
    while True:
        sequence_str = f"{date_str}_{sequence_num:03d}"
        final_local_dir = os.path.join(local_date_dir, sequence_str)
        
        if not os.path.exists(final_local_dir):
            os.makedirs(final_local_dir, exist_ok=True)
            break
        
        sequence_num += 1
        
        # Safety check to prevent infinite loop
        if sequence_num > 999:
            raise ValueError("Too many acquisitions for this date (>999)")

    final_server_dir = os.path.join(server_date_dir, sequence_str)
    # The copy thread will create the server directory.

    base_filename = sequence_str
    return final_local_dir, final_server_dir, base_filename

# COPY THREAD #################################################################
def copy_to_server_thread(local_path, server_path):
    """
    Continuously tries to copy a file to the server until the stop event is set.
    """
    while not copy_thread_stop_event.is_set():
        try:
            # Check if local file exists and has content before copying
            if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
                # Check if server file needs updating
                if not os.path.exists(server_path) or os.path.getsize(server_path) != os.path.getsize(local_path):
                    print(f"Copying {local_path} to {server_path}...")
                    server_dir = os.path.dirname(server_path)
                    os.makedirs(server_dir, exist_ok=True)
                    shutil.copy2(local_path, server_path)
                    print("Copy successful.")
        except Exception as e:
            print(f"Error copying to server: {e}. Retrying in 10 seconds...")
        
        # Wait for 10 seconds or until stop event is set
        copy_thread_stop_event.wait(10)
    
    # Final copy attempt after loop ends
    try:
        print(f"Final copy attempt for {local_path} to {server_path}...")
        server_dir = os.path.dirname(server_path)
        os.makedirs(server_dir, exist_ok=True)
        shutil.copy2(local_path, server_path)
        print("Final copy successful.")
    except Exception as e:
        print(f"Final copy attempt failed: {e}")


# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 25
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    NUMBER_OF_IMAGES = int(300e3)

    ACQUISITION_TIME_VIOLATION_THRESHOLD_MS = EXP_TIME_MS*1.8


# MAIN FUNCTION ################################################################
def main():
    global stop_loop

    # Start the keyboard listener
    listener = keyboard.Listener(on_press=on_press)
    listener.start()

    # Create directory structure and get file paths
    local_data_dir, server_data_dir, base_filename = create_data_directory_and_paths()
    print(f"Local data directory: {local_data_dir}")
    print(f"Server data directory: {server_data_dir}")
    print(f"Base filename: {base_filename}")

    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(f"Timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Save camera attributes BEFORE acquisition
    all_attrs_at_start = cam.get_all_attribute_values()

    # Backup original attributes
    original_roi = cam.get_roi()
    original_trigger_det = cam.get_attribute_value("Trigger Determination")
    original_trigger_resp = cam.get_attribute_value("Trigger Response")
    original_shutter_delay = cam.get_attribute_value("Shutter Closing Delay")
    original_delay_res = cam.get_attribute_value("Shutter Delay Resolution")
    original_shutter_mode = cam.get_attribute_value("Shutter Timing Mode")

    # Setup camera for acquisition
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    print(f"[SET] Exposure Time: {cam.get_attribute_value('Exposure Time')} ms")

    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    print_roi(cam)

    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    print(f"[SET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    print(f"[SET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    cam.set_attribute_value("Clean Until Trigger", False)
    print(f"[SET] Clean Until Trigger: {cam.get_attribute_value('Clean Until Trigger')}")

    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    print(f"[SET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

    cam.set_attribute_value("Shutter Closing Delay", 0)
    print(f"[SET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

    time.sleep(0.2)

    # Save attributes after setup
    all_attrs_measurement = cam.get_all_attribute_values()

    # Set up acquisition
    cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES)

    # HDF5 setup
    h5_path = os.path.join(local_data_dir, f"{base_filename}.h5")
    h5_server_path = os.path.join(server_data_dir, f"{base_filename}.h5")
    
    h5file = h5py.File(h5_path, 'w')
    spectra_ds = h5file.create_dataset(
        "spectra", 
        (0, Settings.SPECTRA_SHAPE[1]), 
        maxshape=(None, Settings.SPECTRA_SHAPE[1]), 
        dtype='uint16', 
        chunks=(100, Settings.SPECTRA_SHAPE[1])
    )
    timestamps_ds = h5file.create_dataset(
        "timestamps_us", 
        (0,), 
        maxshape=(None,), 
        dtype='uint64', 
        chunks=(100,)
    )

    # Save metadata
    start_time_unix = time.time()
    metadata = {
        "timestamp": timestamp,
        "exp_time_ms": Settings.EXP_TIME_MS,
        "binning": Settings.BINNING,
        "spectra_shape": Settings.SPECTRA_SHAPE,
        "number_of_images_planned": Settings.NUMBER_OF_IMAGES,
        "start_time_unix": start_time_unix,
        "hdf5_file": os.path.basename(h5_path),
        "camera_attributes_at_start": [list(item) for item in all_attrs_at_start.items()],
        "camera_attributes_measurement": [list(item) for item in all_attrs_measurement.items()]
    }
    metadata_path = os.path.join(local_data_dir, f"{base_filename}.json")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=4)

    # Start the copy thread
    copier_thread = threading.Thread(target=copy_to_server_thread, args=(h5_path, h5_server_path))
    copier_thread.start()

    # Start acquisition
    print("Starting acquisition... (Press 'Esc' to stop early)")
    cam.start_acquisition()

    violations = []
    t_prev = time.time()
    i = 0
    # run main acquisition loop ########################################################
    try:
        while i < Settings.NUMBER_OF_IMAGES:
            data = cam.read_oldest_image()

            if data is None:
                time.sleep(Settings.EXP_TIME_MS / 1000 / 20)
                continue

            t_now = time.time()
            timestamp_us = int(t_now * 1e6)
            # Efficient max value extraction
            max_val = data.max()

            # Append data to HDF5 datasets
            spectra_ds.resize(i + 1, axis=0)
            spectra_ds[i, :] = data.ravel().astype(np.uint16)
            timestamps_ds.resize(i + 1, axis=0)
            timestamps_ds[i] = timestamp_us

            if i % 100 == 0:
                h5file.flush()
                print(f"Image {i} flushed to HDF5.")

            dt = t_now - t_prev

            # print status:
            print(rf"Image no {i} acquired in {dt:.4f} s, max value: {max_val}")

            if dt > Settings.ACQUISITION_TIME_VIOLATION_THRESHOLD_MS / 1000:
                print(f"ACQUISITION TIME VIOLATION at {i}: {dt:.4f}s --------------------------------------")
                violations.append(i)

            t_prev = t_now

            if stop_loop:
                print("User interrupted acquisition with 'Esc'.")
                break
            
            i += 1

    finally:
        # Stop acquisition
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()

        # Close HDF5 file
        if 'h5file' in locals() and h5file.id:
            h5file.close()
            print("HDF5 file closed.")

        # Signal the copy thread to stop and wait for it
        print("Signaling copy thread to stop...")
        copy_thread_stop_event.set()
        copier_thread.join()
        print("Copy thread finished.")

        # Reset original camera settings
        print("\nResetting camera to original settings:")
        cam.set_roi(*original_roi)
        print_roi(cam)

        cam.set_attribute_value("Trigger Determination", original_trigger_det)
        print(f"[RESET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

        cam.set_attribute_value("Trigger Response", original_trigger_resp)
        print(f"[RESET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

        cam.set_attribute_value("Shutter Closing Delay", original_shutter_delay)
        print(f"[RESET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

        cam.set_attribute_value("Shutter Delay Resolution", original_delay_res)
        print(f"[RESET] Shutter Delay Resolution: {cam.get_attribute_value('Shutter Delay Resolution')}")

        cam.set_attribute_value("Shutter Timing Mode", original_shutter_mode)
        print(f"[RESET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

        cam.close()
        print("Camera connection closed.")

        # Finalize metadata
        metadata["images_acquired"] = i
        metadata["violations"] = violations
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=4)

        print(f"Acquisition complete. Images acquired: {i}, Number of Violations: {len(violations)}")
        #print(rf"Violations at: {violations}")


# PRINT ROI HELPER ############################################################
def print_roi(cam) -> None:
    roi = cam.get_roi()
    print("ROI settings:")
    print(f"  Horizontal start:  {roi[0]}")
    print(f"  Horizontal end:    {roi[1]}")
    print(f"  Vertical start:    {roi[2]}")
    print(f"  Vertical end:      {roi[3]}")
    print(f"  Horizontal binning:{roi[4]}")
    print(f"  Vertical binning:  {roi[5]}")


# PYNPUT CALLBACK #############################################################
def on_press(key):
    global stop_loop
    try:
        if key == keyboard.Key.esc:
            print("Detected 'Esc' key, breaking the loop.")
            stop_loop = True
            return False  # Stop the listener
    except AttributeError:
        pass


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()

blob
mark :35
data 11762
import numpy as np
import os
import time
import json
from datetime import datetime
from pynput import keyboard
from pylablib.devices import PrincetonInstruments
import gc

# Global stop flag
stop_loop = False

# PATHS ########################################################################
class Paths:
    BASE_DIR = r"C:\Users\Moritz\Desktop\TESTDATA"


# DIRECTORY AND FILE MANAGEMENT ###############################################
def create_data_directory_and_paths():
    """
    Creates the directory structure: base_dir/YYYY/STRA/YYMMDD/YYMMDD_XXX/
    Returns the directory path and base filename (without extension).
    Automatically increments XXX if directory already exists.
    """
    now = datetime.now()
    year = now.strftime("%Y")
    date_str = now.strftime("%y%m%d")
    
    # Create the base directory structure
    year_dir = os.path.join(Paths.BASE_DIR, year)
    stra_dir = os.path.join(year_dir, "STRA")
    date_dir = os.path.join(stra_dir, date_str)
    
    # Create directories if they don't exist
    os.makedirs(date_dir, exist_ok=True)
    
    # Find the next available sequence number
    sequence_num = 1
    while True:
        sequence_str = f"{date_str}_{sequence_num:03d}"
        final_dir = os.path.join(date_dir, sequence_str)
        
        if not os.path.exists(final_dir):
            os.makedirs(final_dir, exist_ok=True)
            break
        
        sequence_num += 1
        
        # Safety check to prevent infinite loop
        if sequence_num > 999:
            raise ValueError("Too many acquisitions for this date (>999)")
    
    base_filename = sequence_str
    return final_dir, base_filename

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 25
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    NUMBER_OF_IMAGES = int(300e3)

    ACQUISITION_TIME_VIOLATION_THRESHOLD_MS = EXP_TIME_MS*1.8


# MAIN FUNCTION ################################################################
def main():
    global stop_loop

    # Start the keyboard listener
    listener = keyboard.Listener(on_press=on_press)
    listener.start()

    # Create directory structure and get file paths
    data_dir, base_filename = create_data_directory_and_paths()
    print(f"Data directory: {data_dir}")
    print(f"Base filename: {base_filename}")

    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(f"Timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Save camera attributes BEFORE acquisition
    all_attrs_at_start = cam.get_all_attribute_values()

    # Backup original attributes
    original_roi = cam.get_roi()
    original_trigger_det = cam.get_attribute_value("Trigger Determination")
    original_trigger_resp = cam.get_attribute_value("Trigger Response")
    original_shutter_delay = cam.get_attribute_value("Shutter Closing Delay")
    original_delay_res = cam.get_attribute_value("Shutter Delay Resolution")
    original_shutter_mode = cam.get_attribute_value("Shutter Timing Mode")

    # Setup camera for acquisition
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    print(f"[SET] Exposure Time: {cam.get_attribute_value('Exposure Time')} ms")

    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    print_roi(cam)

    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    print(f"[SET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    print(f"[SET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    cam.set_attribute_value("Clean Until Trigger", False)
    print(f"[SET] Clean Until Trigger: {cam.get_attribute_value('Clean Until Trigger')}")

    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    print(f"[SET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

    cam.set_attribute_value("Shutter Closing Delay", 0)
    print(f"[SET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

    time.sleep(0.2)

    # Save attributes after setup
    all_attrs_measurement = cam.get_all_attribute_values()

    # Set up acquisition
    cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES)

    # Structured dtype for mmap
    dtype = np.dtype([
        ("spectrum", np.uint16, Settings.SPECTRA_SHAPE[1]),
        ("timestamp_us", np.uint64)
    ])
    mmap_path = os.path.join(data_dir, f"{base_filename}.npy")
    mmap = np.memmap(mmap_path, dtype=dtype, mode="w+", shape=(Settings.NUMBER_OF_IMAGES,))
    mmap[:] = 0
    mmap.flush()

    # Save metadata
    start_time_unix = time.time()
    metadata = {
        "timestamp": timestamp,
        "exp_time_ms": Settings.EXP_TIME_MS,
        "binning": Settings.BINNING,
        "spectra_shape": Settings.SPECTRA_SHAPE,
        "number_of_images_planned": Settings.NUMBER_OF_IMAGES,
        "start_time_unix": start_time_unix,
        "memmap_file": os.path.basename(mmap_path),
        "dtype": {
            "spectrum": "uint16",
            "timestamp_us": "uint64"
        },
        "camera_attributes_at_start": [list(item) for item in all_attrs_at_start.items()],
        "camera_attributes_measurement": [list(item) for item in all_attrs_measurement.items()]
    }
    metadata_path = os.path.join(data_dir, f"{base_filename}.json")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=4)

    # Start acquisition
    print("Starting acquisition... (Press 'Esc' to stop early)")
    cam.start_acquisition()

    violations = []
    t_prev = time.time()
    i = 0
    # run main acquisition loop ########################################################
    try:
        while i < Settings.NUMBER_OF_IMAGES:
            data = cam.read_oldest_image()

            if data is None:
                time.sleep(Settings.EXP_TIME_MS / 1000 / 20)
                continue

            t_now = time.time()
            timestamp_us = int(t_now * 1e6)
            # Efficient max value extraction
            max_val = data.max()

            mmap[i] = (data.ravel().astype(np.uint16), timestamp_us)

            if i % 100 == 0:
                mmap.flush()
                print(f"Image {i} flushed.")

            dt = t_now - t_prev

            # print status:
            print(rf"Image no {i} acquired in {dt:.4f} s, max value: {max_val}")

            if dt > Settings.ACQUISITION_TIME_VIOLATION_THRESHOLD_MS / 1000:
                print(f"ACQUISITION TIME VIOLATION at {i}: {dt:.4f}s --------------------------------------")
                violations.append(i)

            t_prev = t_now

            if stop_loop:
                print("User interrupted acquisition with 'Esc'.")
                break
            
            i += 1

    finally:
        # Final flush
        mmap.flush()
        del mmap
        gc.collect()

        # remove zeros from memmap
        print("Cleaning memmap...")
        load_and_clean_memmap(mmap_path, Settings.SPECTRA_SHAPE[1])

        # Stop acquisition
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()

        # Reset original camera settings
        print("\nResetting camera to original settings:")
        cam.set_roi(*original_roi)
        print_roi(cam)

        cam.set_attribute_value("Trigger Determination", original_trigger_det)
        print(f"[RESET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

        cam.set_attribute_value("Trigger Response", original_trigger_resp)
        print(f"[RESET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

        cam.set_attribute_value("Shutter Closing Delay", original_shutter_delay)
        print(f"[RESET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

        cam.set_attribute_value("Shutter Delay Resolution", original_delay_res)
        print(f"[RESET] Shutter Delay Resolution: {cam.get_attribute_value('Shutter Delay Resolution')}")

        cam.set_attribute_value("Shutter Timing Mode", original_shutter_mode)
        print(f"[RESET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

        cam.close()
        print("Camera connection closed.")

        # Finalize metadata
        metadata["images_acquired"] = i
        metadata["violations"] = violations
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=4)

        print(f"Acquisition complete. Images acquired: {i}, Number of Violations: {len(violations)}")
        print(rf"Violations at: {violations}")


# PRINT ROI HELPER ############################################################
def print_roi(cam) -> None:
    roi = cam.get_roi()
    print("ROI settings:")
    print(f"  Horizontal start:  {roi[0]}")
    print(f"  Horizontal end:    {roi[1]}")
    print(f"  Vertical start:    {roi[2]}")
    print(f"  Vertical end:      {roi[3]}")
    print(f"  Horizontal binning:{roi[4]}")
    print(f"  Vertical binning:  {roi[5]}")


# PYNPUT CALLBACK #############################################################
def on_press(key):
    global stop_loop
    try:
        if key == keyboard.Key.esc:
            print("Detected 'Esc' key, breaking the loop.")
            stop_loop = True
            return False  # Stop the listener
    except AttributeError:
        pass

# CLEAR ZEROS FROM MEMMAP #######################################################
def load_and_clean_memmap(file_path: str, spectrum_length: int) -> None:
    """
    Loads the memmap, removes all-zero rows, and overwrites the original file safely via a temp file.

    :param file_path: Path to the .npy file.
    :param spectrum_length: Length of the spectrum.
    """
    import numpy as np
    import os
    import tempfile
    import gc

    print(f"Loading memmap from: {file_path}")

    dtype = np.dtype([
        ("intensities", np.uint16, spectrum_length),
        ("timestamp_us", np.uint64)
    ])

    # Step 1: Open and filter data
    mmap = np.memmap(file_path, dtype=dtype, mode="r")
    nonzero_mask = ~(
        (mmap["timestamp_us"] == 0) &
        (np.all(mmap["intensities"] == 0, axis=1))
    )
    cleaned_data = mmap[nonzero_mask].copy()  # Load into RAM
    print(f"Original rows: {len(mmap)}, Non-zero rows: {len(cleaned_data)}")

    # Step 2: Fully release original mmap (important on Windows)
    if hasattr(mmap, '_mmap'):
        mmap._mmap.close()
    del mmap
    gc.collect()

    # Step 3: Write to a temporary file
    temp_fd, temp_path = tempfile.mkstemp(dir=os.path.dirname(file_path))
    os.close(temp_fd)

    cleaned_mmap = np.memmap(temp_path, dtype=dtype, mode="w+", shape=(len(cleaned_data),))
    cleaned_mmap[:] = cleaned_data
    cleaned_mmap.flush()

    if hasattr(cleaned_mmap, '_mmap'):
        cleaned_mmap._mmap.close()
    del cleaned_mmap
    gc.collect()

    # Step 4: Atomically replace original file
    os.replace(temp_path, file_path)

    print(f"Cleaned memmap saved to: {file_path}")

# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()

blob
mark :36
data 11760
import numpy as np
import os
import time
import json
from datetime import datetime
from pynput import keyboard
from pylablib.devices import PrincetonInstruments
import gc

# Global stop flag
stop_loop = False

# PATHS ########################################################################
class Paths:
    BASE_DIR = r"C:\Users\Moritz\Desktop\TESTDATA"


# DIRECTORY AND FILE MANAGEMENT ###############################################
def create_data_directory_and_paths():
    """
    Creates the directory structure: base_dir/YYYY/STRA/YYMMDD/YYMMDD_XXX/
    Returns the directory path and base filename (without extension).
    Automatically increments XXX if directory already exists.
    """
    now = datetime.now()
    year = now.strftime("%Y")
    date_str = now.strftime("%y%m%d")
    
    # Create the base directory structure
    year_dir = os.path.join(Paths.BASE_DIR, year)
    stra_dir = os.path.join(year_dir, "STRA")
    date_dir = os.path.join(stra_dir, date_str)
    
    # Create directories if they don't exist
    os.makedirs(date_dir, exist_ok=True)
    
    # Find the next available sequence number
    sequence_num = 1
    while True:
        sequence_str = f"{date_str}_{sequence_num:03d}"
        final_dir = os.path.join(date_dir, sequence_str)
        
        if not os.path.exists(final_dir):
            os.makedirs(final_dir, exist_ok=True)
            break
        
        sequence_num += 1
        
        # Safety check to prevent infinite loop
        if sequence_num > 999:
            raise ValueError("Too many acquisitions for this date (>999)")
    
    base_filename = sequence_str
    return final_dir, base_filename

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 20
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    NUMBER_OF_IMAGES = int(300e3)
    ACQUISITION_TIME_VIOLATION_THRESHOLD_MS = EXP_TIME_MS*1.8


# MAIN FUNCTION ################################################################
def main():
    global stop_loop

    # Start the keyboard listener
    listener = keyboard.Listener(on_press=on_press)
    listener.start()

    # Create directory structure and get file paths
    data_dir, base_filename = create_data_directory_and_paths()
    print(f"Data directory: {data_dir}")
    print(f"Base filename: {base_filename}")

    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(f"Timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Save camera attributes BEFORE acquisition
    all_attrs_at_start = cam.get_all_attribute_values()

    # Backup original attributes
    original_roi = cam.get_roi()
    original_trigger_det = cam.get_attribute_value("Trigger Determination")
    original_trigger_resp = cam.get_attribute_value("Trigger Response")
    original_shutter_delay = cam.get_attribute_value("Shutter Closing Delay")
    original_delay_res = cam.get_attribute_value("Shutter Delay Resolution")
    original_shutter_mode = cam.get_attribute_value("Shutter Timing Mode")

    # Setup camera for acquisition
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    print(f"[SET] Exposure Time: {cam.get_attribute_value('Exposure Time')} ms")

    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    print_roi(cam)

    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    print(f"[SET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    print(f"[SET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    cam.set_attribute_value("Clean Until Trigger", False)
    print(f"[SET] Clean Until Trigger: {cam.get_attribute_value('Clean Until Trigger')}")

    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    print(f"[SET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

    cam.set_attribute_value("Shutter Closing Delay", 0)
    print(f"[SET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

    time.sleep(0.2)

    # Save attributes after setup
    all_attrs_measurement = cam.get_all_attribute_values()

    # Set up acquisition
    cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES)

    # Structured dtype for mmap
    dtype = np.dtype([
        ("spectrum", np.uint16, Settings.SPECTRA_SHAPE[1]),
        ("timestamp_us", np.uint64)
    ])
    mmap_path = os.path.join(data_dir, f"{base_filename}.npy")
    mmap = np.memmap(mmap_path, dtype=dtype, mode="w+", shape=(Settings.NUMBER_OF_IMAGES,))
    mmap[:] = 0
    mmap.flush()

    # Save metadata
    start_time_unix = time.time()
    metadata = {
        "timestamp": timestamp,
        "exp_time_ms": Settings.EXP_TIME_MS,
        "binning": Settings.BINNING,
        "spectra_shape": Settings.SPECTRA_SHAPE,
        "number_of_images_planned": Settings.NUMBER_OF_IMAGES,
        "start_time_unix": start_time_unix,
        "memmap_file": os.path.basename(mmap_path),
        "dtype": {
            "spectrum": "uint16",
            "timestamp_us": "uint64"
        },
        "camera_attributes_at_start": [list(item) for item in all_attrs_at_start.items()],
        "camera_attributes_measurement": [list(item) for item in all_attrs_measurement.items()]
    }
    metadata_path = os.path.join(data_dir, f"{base_filename}.json")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=4)

    # Start acquisition
    print("Starting acquisition... (Press 'Esc' to stop early)")
    cam.start_acquisition()

    violations = []
    t_prev = time.time()
    i = 0
    # run main acquisition loop ########################################################
    try:
        while i < Settings.NUMBER_OF_IMAGES:
            data = cam.read_oldest_image()

            if data is None:
                time.sleep(Settings.EXP_TIME_MS / 1000 / 20)
                continue

            t_now = time.time()
            timestamp_us = int(t_now * 1e6)
            # Efficient max value extraction
            max_val = data.max()

            mmap[i] = (data.ravel().astype(np.uint16), timestamp_us)

            if i % 100 == 0:
                mmap.flush()
                print(f"Image {i} flushed.")

            dt = t_now - t_prev

            # print status:
            print(rf"Image no {i} acquired in {dt:.4f} s, max value: {max_val}")

            if dt > Settings.ACQUISITION_TIME_VIOLATION_THRESHOLD_MS / 1000:
                print(f"ACQUISITION TIME VIOLATION at {i}: {dt:.4f}s --------------------------------------")
                violations.append(i)

            t_prev = t_now

            if stop_loop:
                print("User interrupted acquisition with 'Esc'.")
                break
            
            i += 1

    finally:
        # Final flush
        mmap.flush()
        del mmap
        gc.collect()

        # remove zeros from memmap
        print("Cleaning memmap...")
        load_and_clean_memmap(mmap_path, Settings.SPECTRA_SHAPE[1])

        # Stop acquisition
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()

        # Reset original camera settings
        print("\nResetting camera to original settings:")
        cam.set_roi(*original_roi)
        print_roi(cam)

        cam.set_attribute_value("Trigger Determination", original_trigger_det)
        print(f"[RESET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

        cam.set_attribute_value("Trigger Response", original_trigger_resp)
        print(f"[RESET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

        cam.set_attribute_value("Shutter Closing Delay", original_shutter_delay)
        print(f"[RESET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

        cam.set_attribute_value("Shutter Delay Resolution", original_delay_res)
        print(f"[RESET] Shutter Delay Resolution: {cam.get_attribute_value('Shutter Delay Resolution')}")

        cam.set_attribute_value("Shutter Timing Mode", original_shutter_mode)
        print(f"[RESET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

        cam.close()
        print("Camera connection closed.")

        # Finalize metadata
        metadata["images_acquired"] = i
        metadata["violations"] = violations
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=4)

        print(f"Acquisition complete. Images acquired: {i}, Number of Violations: {len(violations)}")
        print(rf"Violations at: {violations}")


# PRINT ROI HELPER ############################################################
def print_roi(cam) -> None:
    roi = cam.get_roi()
    print("ROI settings:")
    print(f"  Horizontal start:  {roi[0]}")
    print(f"  Horizontal end:    {roi[1]}")
    print(f"  Vertical start:    {roi[2]}")
    print(f"  Vertical end:      {roi[3]}")
    print(f"  Horizontal binning:{roi[4]}")
    print(f"  Vertical binning:  {roi[5]}")


# PYNPUT CALLBACK #############################################################
def on_press(key):
    global stop_loop
    try:
        if key == keyboard.Key.esc:
            print("Detected 'Esc' key, breaking the loop.")
            stop_loop = True
            return False  # Stop the listener
    except AttributeError:
        pass

# CLEAR ZEROS FROM MEMMAP #######################################################
def load_and_clean_memmap(file_path: str, spectrum_length: int) -> None:
    """
    Loads the memmap, removes all-zero rows, and overwrites the original file safely via a temp file.

    :param file_path: Path to the .npy file.
    :param spectrum_length: Length of the spectrum.
    """
    import numpy as np
    import os
    import tempfile
    import gc

    print(f"Loading memmap from: {file_path}")

    dtype = np.dtype([
        ("intensities", np.uint16, spectrum_length),
        ("timestamp_us", np.uint64)
    ])

    # Step 1: Open and filter data
    mmap = np.memmap(file_path, dtype=dtype, mode="r")
    nonzero_mask = ~(
        (mmap["timestamp_us"] == 0) &
        (np.all(mmap["intensities"] == 0, axis=1))
    )
    cleaned_data = mmap[nonzero_mask].copy()  # Load into RAM
    print(f"Original rows: {len(mmap)}, Non-zero rows: {len(cleaned_data)}")

    # Step 2: Fully release original mmap (important on Windows)
    if hasattr(mmap, '_mmap'):
        mmap._mmap.close()
    del mmap
    gc.collect()

    # Step 3: Write to a temporary file
    temp_fd, temp_path = tempfile.mkstemp(dir=os.path.dirname(file_path))
    os.close(temp_fd)

    cleaned_mmap = np.memmap(temp_path, dtype=dtype, mode="w+", shape=(len(cleaned_data),))
    cleaned_mmap[:] = cleaned_data
    cleaned_mmap.flush()

    if hasattr(cleaned_mmap, '_mmap'):
        cleaned_mmap._mmap.close()
    del cleaned_mmap
    gc.collect()

    # Step 4: Atomically replace original file
    os.replace(temp_path, file_path)

    print(f"Cleaned memmap saved to: {file_path}")

# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()

blob
mark :37
data 11820
import numpy as np
import os
import time
import json
from datetime import datetime
from pynput import keyboard
from pylablib.devices import PrincetonInstruments
import gc

# Global stop flag
stop_loop = False

# PATHS ########################################################################
class Paths:
    BASE_DIR = r"C:\Users\Moritz\Desktop\TESTDATA"
#Empty statement to indicate this version also works with v2

# DIRECTORY AND FILE MANAGEMENT ###############################################
def create_data_directory_and_paths():
    """
    Creates the directory structure: base_dir/YYYY/STRA/YYMMDD/YYMMDD_XXX/
    Returns the directory path and base filename (without extension).
    Automatically increments XXX if directory already exists.
    """
    now = datetime.now()
    year = now.strftime("%Y")
    date_str = now.strftime("%y%m%d")
    
    # Create the base directory structure
    year_dir = os.path.join(Paths.BASE_DIR, year)
    stra_dir = os.path.join(year_dir, "STRA")
    date_dir = os.path.join(stra_dir, date_str)
    
    # Create directories if they don't exist
    os.makedirs(date_dir, exist_ok=True)
    
    # Find the next available sequence number
    sequence_num = 1
    while True:
        sequence_str = f"{date_str}_{sequence_num:03d}"
        final_dir = os.path.join(date_dir, sequence_str)
        
        if not os.path.exists(final_dir):
            os.makedirs(final_dir, exist_ok=True)
            break
        
        sequence_num += 1
        
        # Safety check to prevent infinite loop
        if sequence_num > 999:
            raise ValueError("Too many acquisitions for this date (>999)")
    
    base_filename = sequence_str
    return final_dir, base_filename

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 20
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    NUMBER_OF_IMAGES = int(300e3)
    ACQUISITION_TIME_VIOLATION_THRESHOLD_MS = EXP_TIME_MS*1.8


# MAIN FUNCTION ################################################################
def main():
    global stop_loop

    # Start the keyboard listener
    listener = keyboard.Listener(on_press=on_press)
    listener.start()

    # Create directory structure and get file paths
    data_dir, base_filename = create_data_directory_and_paths()
    print(f"Data directory: {data_dir}")
    print(f"Base filename: {base_filename}")

    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(f"Timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Save camera attributes BEFORE acquisition
    all_attrs_at_start = cam.get_all_attribute_values()

    # Backup original attributes
    original_roi = cam.get_roi()
    original_trigger_det = cam.get_attribute_value("Trigger Determination")
    original_trigger_resp = cam.get_attribute_value("Trigger Response")
    original_shutter_delay = cam.get_attribute_value("Shutter Closing Delay")
    original_delay_res = cam.get_attribute_value("Shutter Delay Resolution")
    original_shutter_mode = cam.get_attribute_value("Shutter Timing Mode")

    # Setup camera for acquisition
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    print(f"[SET] Exposure Time: {cam.get_attribute_value('Exposure Time')} ms")

    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    print_roi(cam)

    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    print(f"[SET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    print(f"[SET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    cam.set_attribute_value("Clean Until Trigger", False)
    print(f"[SET] Clean Until Trigger: {cam.get_attribute_value('Clean Until Trigger')}")

    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    print(f"[SET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

    cam.set_attribute_value("Shutter Closing Delay", 0)
    print(f"[SET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

    time.sleep(0.2)

    # Save attributes after setup
    all_attrs_measurement = cam.get_all_attribute_values()

    # Set up acquisition
    cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES)

    # Structured dtype for mmap
    dtype = np.dtype([
        ("spectrum", np.uint16, Settings.SPECTRA_SHAPE[1]),
        ("timestamp_us", np.uint64)
    ])
    mmap_path = os.path.join(data_dir, f"{base_filename}.npy")
    mmap = np.memmap(mmap_path, dtype=dtype, mode="w+", shape=(Settings.NUMBER_OF_IMAGES,))
    mmap[:] = 0
    mmap.flush()

    # Save metadata
    start_time_unix = time.time()
    metadata = {
        "timestamp": timestamp,
        "exp_time_ms": Settings.EXP_TIME_MS,
        "binning": Settings.BINNING,
        "spectra_shape": Settings.SPECTRA_SHAPE,
        "number_of_images_planned": Settings.NUMBER_OF_IMAGES,
        "start_time_unix": start_time_unix,
        "memmap_file": os.path.basename(mmap_path),
        "dtype": {
            "spectrum": "uint16",
            "timestamp_us": "uint64"
        },
        "camera_attributes_at_start": [list(item) for item in all_attrs_at_start.items()],
        "camera_attributes_measurement": [list(item) for item in all_attrs_measurement.items()]
    }
    metadata_path = os.path.join(data_dir, f"{base_filename}.json")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=4)

    # Start acquisition
    print("Starting acquisition... (Press 'Esc' to stop early)")
    cam.start_acquisition()

    violations = []
    t_prev = time.time()
    i = 0
    # run main acquisition loop ########################################################
    try:
        while i < Settings.NUMBER_OF_IMAGES:
            data = cam.read_oldest_image()

            if data is None:
                time.sleep(Settings.EXP_TIME_MS / 1000 / 20)
                continue

            t_now = time.time()
            timestamp_us = int(t_now * 1e6)
            # Efficient max value extraction
            max_val = data.max()

            mmap[i] = (data.ravel().astype(np.uint16), timestamp_us)

            if i % 100 == 0:
                mmap.flush()
                print(f"Image {i} flushed.")

            dt = t_now - t_prev

            # print status:
            print(rf"Image no {i} acquired in {dt:.4f} s, max value: {max_val}")

            if dt > Settings.ACQUISITION_TIME_VIOLATION_THRESHOLD_MS / 1000:
                print(f"ACQUISITION TIME VIOLATION at {i}: {dt:.4f}s --------------------------------------")
                violations.append(i)

            t_prev = t_now

            if stop_loop:
                print("User interrupted acquisition with 'Esc'.")
                break
            
            i += 1

    finally:
        # Final flush
        mmap.flush()
        del mmap
        gc.collect()

        # remove zeros from memmap
        print("Cleaning memmap...")
        load_and_clean_memmap(mmap_path, Settings.SPECTRA_SHAPE[1])

        # Stop acquisition
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()

        # Reset original camera settings
        print("\nResetting camera to original settings:")
        cam.set_roi(*original_roi)
        print_roi(cam)

        cam.set_attribute_value("Trigger Determination", original_trigger_det)
        print(f"[RESET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

        cam.set_attribute_value("Trigger Response", original_trigger_resp)
        print(f"[RESET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

        cam.set_attribute_value("Shutter Closing Delay", original_shutter_delay)
        print(f"[RESET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

        cam.set_attribute_value("Shutter Delay Resolution", original_delay_res)
        print(f"[RESET] Shutter Delay Resolution: {cam.get_attribute_value('Shutter Delay Resolution')}")

        cam.set_attribute_value("Shutter Timing Mode", original_shutter_mode)
        print(f"[RESET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

        cam.close()
        print("Camera connection closed.")

        # Finalize metadata
        metadata["images_acquired"] = i
        metadata["violations"] = violations
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=4)

        print(f"Acquisition complete. Images acquired: {i}, Number of Violations: {len(violations)}")
        print(rf"Violations at: {violations}")


# PRINT ROI HELPER ############################################################
def print_roi(cam) -> None:
    roi = cam.get_roi()
    print("ROI settings:")
    print(f"  Horizontal start:  {roi[0]}")
    print(f"  Horizontal end:    {roi[1]}")
    print(f"  Vertical start:    {roi[2]}")
    print(f"  Vertical end:      {roi[3]}")
    print(f"  Horizontal binning:{roi[4]}")
    print(f"  Vertical binning:  {roi[5]}")


# PYNPUT CALLBACK #############################################################
def on_press(key):
    global stop_loop
    try:
        if key == keyboard.Key.esc:
            print("Detected 'Esc' key, breaking the loop.")
            stop_loop = True
            return False  # Stop the listener
    except AttributeError:
        pass

# CLEAR ZEROS FROM MEMMAP #######################################################
def load_and_clean_memmap(file_path: str, spectrum_length: int) -> None:
    """
    Loads the memmap, removes all-zero rows, and overwrites the original file safely via a temp file.

    :param file_path: Path to the .npy file.
    :param spectrum_length: Length of the spectrum.
    """
    import numpy as np
    import os
    import tempfile
    import gc

    print(f"Loading memmap from: {file_path}")

    dtype = np.dtype([
        ("intensities", np.uint16, spectrum_length),
        ("timestamp_us", np.uint64)
    ])

    # Step 1: Open and filter data
    mmap = np.memmap(file_path, dtype=dtype, mode="r")
    nonzero_mask = ~(
        (mmap["timestamp_us"] == 0) &
        (np.all(mmap["intensities"] == 0, axis=1))
    )
    cleaned_data = mmap[nonzero_mask].copy()  # Load into RAM
    print(f"Original rows: {len(mmap)}, Non-zero rows: {len(cleaned_data)}")

    # Step 2: Fully release original mmap (important on Windows)
    if hasattr(mmap, '_mmap'):
        mmap._mmap.close()
    del mmap
    gc.collect()

    # Step 3: Write to a temporary file
    temp_fd, temp_path = tempfile.mkstemp(dir=os.path.dirname(file_path))
    os.close(temp_fd)

    cleaned_mmap = np.memmap(temp_path, dtype=dtype, mode="w+", shape=(len(cleaned_data),))
    cleaned_mmap[:] = cleaned_data
    cleaned_mmap.flush()

    if hasattr(cleaned_mmap, '_mmap'):
        cleaned_mmap._mmap.close()
    del cleaned_mmap
    gc.collect()

    # Step 4: Atomically replace original file
    os.replace(temp_path, file_path)

    print(f"Cleaned memmap saved to: {file_path}")

# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()

blob
mark :38
data 10654
import numpy as np
import os
import time
import json
from datetime import datetime
from pynput import keyboard
from pylablib.devices import PrincetonInstruments
import gc
import h5py

# Global stop flag
stop_loop = False

# PATHS ########################################################################
class Paths:
    BASE_DIR = r"C:\Users\Moritz\Desktop\TESTDATA"
#Empty statement to indicate this version also works with v2

# DIRECTORY AND FILE MANAGEMENT ###############################################
def create_hdf5_filepath():
    """
    Creates the directory structure: base_dir/YYYY/STRA/YYMMDD/
    and returns the full path for a new HDF5 file: YYMMDD_XXX.hdf5.
    Automatically increments XXX if the file already exists.
    """
    now = datetime.now()
    year = now.strftime("%Y")
    date_str = now.strftime("%y%m%d")
    
    # Create the base directory structure
    year_dir = os.path.join(Paths.BASE_DIR, year)
    stra_dir = os.path.join(year_dir, "STRA")
    date_dir = os.path.join(stra_dir, date_str)
    
    # Create directories if they don't exist
    os.makedirs(date_dir, exist_ok=True)
    
    # Find the next available sequence number
    sequence_num = 1
    while True:
        sequence_str = f"{date_str}_{sequence_num:03d}"
        hdf5_filename = f"{sequence_str}.hdf5"
        final_path = os.path.join(date_dir, hdf5_filename)
        
        if not os.path.exists(final_path):
            break
        
        sequence_num += 1
        
        # Safety check to prevent infinite loop
        if sequence_num > 999:
            raise ValueError("Too many acquisitions for this date (>999)")
            
    return final_path

def save_dict_to_hdf5_attrs(group, metadata_dict):
    """
    Saves dictionary items as attributes to an HDF5 group.
    Complex types like dicts or lists are serialized to JSON strings.
    """
    for key, value in metadata_dict.items():
        if isinstance(value, (dict, list, tuple)):
            group.attrs[key] = json.dumps(value)
        else:
            group.attrs[key] = value

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 20
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    NUMBER_OF_IMAGES = int(300e3)
    ACQUISITION_TIME_VIOLATION_THRESHOLD_MS = EXP_TIME_MS * 1.8


# MAIN FUNCTION ################################################################
def main():
    global stop_loop

    # Start the keyboard listener
    listener = keyboard.Listener(on_press=on_press)
    listener.start()

    # Create directory structure and get HDF5 file path
    hdf5_path = create_hdf5_filepath()
    print(f"Data will be saved to: {hdf5_path}")

    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(f"Timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Save camera attributes BEFORE acquisition
    all_attrs_at_start = cam.get_all_attribute_values()

    # Backup original attributes
    original_roi = cam.get_roi()
    original_trigger_det = cam.get_attribute_value("Trigger Determination")
    original_trigger_resp = cam.get_attribute_value("Trigger Response")
    original_shutter_delay = cam.get_attribute_value("Shutter Closing Delay")
    original_delay_res = cam.get_attribute_value("Shutter Delay Resolution")
    original_shutter_mode = cam.get_attribute_value("Shutter Timing Mode")

    # Setup camera for acquisition
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    print(f"[SET] Exposure Time: {cam.get_attribute_value('Exposure Time')} ms")

    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    print_roi(cam)

    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    print(f"[SET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    print(f"[SET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    cam.set_attribute_value("Clean Until Trigger", False)
    print(f"[SET] Clean Until Trigger: {cam.get_attribute_value('Clean Until Trigger')}")

    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    print(f"[SET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

    cam.set_attribute_value("Shutter Closing Delay", 0)
    print(f"[SET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

    time.sleep(0.2)

    # Save attributes after setup
    all_attrs_measurement = cam.get_all_attribute_values()

    # Set up acquisition
    cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES)

    # Structured dtype for HDF5 dataset
    dtype = np.dtype([
        ("spectrum", np.uint16, Settings.SPECTRA_SHAPE[1]),
        ("timestamp_us", np.uint64)
    ])

    # Metadata dictionary
    start_time_unix = time.time()
    metadata = {
        "timestamp": timestamp,
        "exp_time_ms": Settings.EXP_TIME_MS,
        "binning": list(Settings.BINNING),
        "spectra_shape": list(Settings.SPECTRA_SHAPE),
        "number_of_images_planned": Settings.NUMBER_OF_IMAGES,
        "start_time_unix": start_time_unix,
        "dtype_spectrum": "uint16",
        "dtype_timestamp_us": "uint64",
        "camera_attributes_at_start": [list(item) for item in all_attrs_at_start.items()],
        "camera_attributes_measurement": [list(item) for item in all_attrs_measurement.items()]
    }

    i = 0
    violations = []

    # Use a try...finally block to ensure resources are closed properly
    try:
        with h5py.File(hdf5_path, 'w') as f:
            # Create a resizable dataset for the spectra and timestamps
            dset = f.create_dataset(
                "data",
                shape=(0,),
                maxshape=(Settings.NUMBER_OF_IMAGES,),
                dtype=dtype,
                chunks=(100,)  # Chunking is important for performance
            )
            
            # Create a group for metadata and save it as attributes
            meta_group = f.create_group("metadata")
            save_dict_to_hdf5_attrs(meta_group, metadata)
            
            # Start acquisition
            print("Starting acquisition... (Press 'Esc' to stop early)")
            cam.start_acquisition()
            t_prev = time.time()
            
            # Run main acquisition loop
            while i < Settings.NUMBER_OF_IMAGES:
                data = cam.read_oldest_image()

                if data is None:
                    time.sleep(Settings.EXP_TIME_MS / 1000 / 20)
                    continue

                t_now = time.time()
                timestamp_us = int(t_now * 1e6)
                max_val = data.max()

                # Resize dataset and write new data
                dset.resize((i + 1,))
                dset[i] = (data.ravel().astype(np.uint16), timestamp_us)
                
                # Periodically flush data to disk
                if i % 100 == 0:
                    f.flush()
                    print(f"Image {i} flushed.")

                dt = t_now - t_prev
                print(rf"Image no {i} acquired in {dt:.4f} s, max value: {max_val}")

                if dt > Settings.ACQUISITION_TIME_VIOLATION_THRESHOLD_MS / 1000:
                    print(f"ACQUISITION TIME VIOLATION at {i}: {dt:.4f}s --------------------------------------")
                    violations.append(i)

                t_prev = t_now

                if stop_loop:
                    print("User interrupted acquisition with 'Esc'.")
                    break
                
                i += 1
            
            # Final metadata update
            meta_group.attrs["images_acquired"] = i
            meta_group.attrs["violations"] = json.dumps(violations)
            print("Final metadata saved.")

    finally:
        # Stop acquisition
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()

        # Reset original camera settings
        print("\nResetting camera to original settings:")
        cam.set_roi(*original_roi)
        print_roi(cam)

        cam.set_attribute_value("Trigger Determination", original_trigger_det)
        print(f"[RESET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

        cam.set_attribute_value("Trigger Response", original_trigger_resp)
        print(f"[RESET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

        cam.set_attribute_value("Shutter Closing Delay", original_shutter_delay)
        print(f"[RESET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

        cam.set_attribute_value("Shutter Delay Resolution", original_delay_res)
        print(f"[RESET] Shutter Delay Resolution: {cam.get_attribute_value('Shutter Delay Resolution')}")

        cam.set_attribute_value("Shutter Timing Mode", original_shutter_mode)
        print(f"[RESET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

        cam.close()
        print("Camera connection closed.")

        gc.collect()

        print(f"Acquisition complete. Images acquired: {i}, Number of Violations: {len(violations)}")
        print(rf"Violations at: {violations}")


# PRINT ROI HELPER ############################################################
def print_roi(cam) -> None:
    roi = cam.get_roi()
    print("ROI settings:")
    print(f"  Horizontal start:   {roi[0]}")
    print(f"  Horizontal end:     {roi[1]}")
    print(f"  Vertical start:     {roi[2]}")
    print(f"  Vertical end:       {roi[3]}")
    print(f"  Horizontal binning: {roi[4]}")
    print(f"  Vertical binning:   {roi[5]}")


# PYNPUT CALLBACK #############################################################
def on_press(key):
    global stop_loop
    try:
        if key == keyboard.Key.esc:
            print("Detected 'Esc' key, breaking the loop.")
            stop_loop = True
            return False  # Stop the listener
    except AttributeError:
        pass


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()
blob
mark :39
data 10660
import numpy as np
import os
import time
import json
from datetime import datetime
from pynput import keyboard
from pylablib.devices import PrincetonInstruments
import gc
import h5py

# Global stop flag
stop_loop = False

# PATHS ########################################################################
class Paths:
    BASE_DIR = r"C:\Users\Moritz\Desktop\TESTDATA"
#Empty statement to indicate this version also works with v2

# DIRECTORY AND FILE MANAGEMENT ###############################################
def create_hdf5_filepath():
    """
    Creates the directory structure: base_dir/YYYY/STRA/YYMMDD/
    and returns the full path for a new HDF5 file: YYMMDD_XXX.hdf5.
    Automatically increments XXX if the file already exists.
    """
    now = datetime.now()
    year = now.strftime("%Y")
    date_str = now.strftime("%y%m%d")
    
    # Create the base directory structure
    year_dir = os.path.join(Paths.BASE_DIR, year)
    stra_dir = os.path.join(year_dir, "STRA")
    date_dir = os.path.join(stra_dir, date_str)
    
    # Create directories if they don't exist
    os.makedirs(date_dir, exist_ok=True)
    
    # Find the next available sequence number
    sequence_num = 1
    while True:
        sequence_str = f"{date_str}_{sequence_num:03d}"
        hdf5_filename = f"{sequence_str}.hdf5"
        final_path = os.path.join(date_dir, sequence_str, hdf5_filename)

        if not os.path.exists(final_path):
            break
        
        sequence_num += 1
        
        # Safety check to prevent infinite loop
        if sequence_num > 999:
            raise ValueError("Too many acquisitions for this date (>999)")
            
    return final_path

def save_dict_to_hdf5_attrs(group, metadata_dict):
    """
    Saves dictionary items as attributes to an HDF5 group.
    Complex types like dicts or lists are serialized to JSON strings.
    """
    for key, value in metadata_dict.items():
        if isinstance(value, (dict, list, tuple)):
            group.attrs[key] = json.dumps(value)
        else:
            group.attrs[key] = value

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 20
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    NUMBER_OF_IMAGES = int(300e3)
    ACQUISITION_TIME_VIOLATION_THRESHOLD_MS = EXP_TIME_MS * 1.8


# MAIN FUNCTION ################################################################
def main():
    global stop_loop

    # Start the keyboard listener
    listener = keyboard.Listener(on_press=on_press)
    listener.start()

    # Create directory structure and get HDF5 file path
    hdf5_path = create_hdf5_filepath()
    print(f"Data will be saved to: {hdf5_path}")

    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(f"Timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Save camera attributes BEFORE acquisition
    all_attrs_at_start = cam.get_all_attribute_values()

    # Backup original attributes
    original_roi = cam.get_roi()
    original_trigger_det = cam.get_attribute_value("Trigger Determination")
    original_trigger_resp = cam.get_attribute_value("Trigger Response")
    original_shutter_delay = cam.get_attribute_value("Shutter Closing Delay")
    original_delay_res = cam.get_attribute_value("Shutter Delay Resolution")
    original_shutter_mode = cam.get_attribute_value("Shutter Timing Mode")

    # Setup camera for acquisition
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    print(f"[SET] Exposure Time: {cam.get_attribute_value('Exposure Time')} ms")

    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    print_roi(cam)

    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    print(f"[SET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    print(f"[SET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    cam.set_attribute_value("Clean Until Trigger", False)
    print(f"[SET] Clean Until Trigger: {cam.get_attribute_value('Clean Until Trigger')}")

    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    print(f"[SET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

    cam.set_attribute_value("Shutter Closing Delay", 0)
    print(f"[SET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

    time.sleep(0.2)

    # Save attributes after setup
    all_attrs_measurement = cam.get_all_attribute_values()

    # Set up acquisition
    cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES)

    # Structured dtype for HDF5 dataset
    dtype = np.dtype([
        ("spectrum", np.uint16, Settings.SPECTRA_SHAPE[1]),
        ("timestamp_us", np.uint64)
    ])

    # Metadata dictionary
    start_time_unix = time.time()
    metadata = {
        "timestamp": timestamp,
        "exp_time_ms": Settings.EXP_TIME_MS,
        "binning": list(Settings.BINNING),
        "spectra_shape": list(Settings.SPECTRA_SHAPE),
        "number_of_images_planned": Settings.NUMBER_OF_IMAGES,
        "start_time_unix": start_time_unix,
        "dtype_spectrum": "uint16",
        "dtype_timestamp_us": "uint64",
        "camera_attributes_at_start": [list(item) for item in all_attrs_at_start.items()],
        "camera_attributes_measurement": [list(item) for item in all_attrs_measurement.items()]
    }

    i = 0
    violations = []

    # Use a try...finally block to ensure resources are closed properly
    try:
        with h5py.File(hdf5_path, 'w') as f:
            # Create a resizable dataset for the spectra and timestamps
            dset = f.create_dataset(
                "data",
                shape=(0,),
                maxshape=(Settings.NUMBER_OF_IMAGES,),
                dtype=dtype,
                chunks=(100,)  # Chunking is important for performance
            )
            
            # Create a group for metadata and save it as attributes
            meta_group = f.create_group("metadata")
            save_dict_to_hdf5_attrs(meta_group, metadata)
            
            # Start acquisition
            print("Starting acquisition... (Press 'Esc' to stop early)")
            cam.start_acquisition()
            t_prev = time.time()
            
            # Run main acquisition loop
            while i < Settings.NUMBER_OF_IMAGES:
                data = cam.read_oldest_image()

                if data is None:
                    time.sleep(Settings.EXP_TIME_MS / 1000 / 20)
                    continue

                t_now = time.time()
                timestamp_us = int(t_now * 1e6)
                max_val = data.max()

                # Resize dataset and write new data
                dset.resize((i + 1,))
                dset[i] = (data.ravel().astype(np.uint16), timestamp_us)
                
                # Periodically flush data to disk
                if i % 100 == 0:
                    f.flush()
                    print(f"Image {i} flushed.")

                dt = t_now - t_prev
                print(rf"Image no {i} acquired in {dt:.4f} s, max value: {max_val}")

                if dt > Settings.ACQUISITION_TIME_VIOLATION_THRESHOLD_MS / 1000:
                    print(f"ACQUISITION TIME VIOLATION at {i}: {dt:.4f}s --------------------------------------")
                    violations.append(i)

                t_prev = t_now

                if stop_loop:
                    print("User interrupted acquisition with 'Esc'.")
                    break
                
                i += 1
            
            # Final metadata update
            meta_group.attrs["images_acquired"] = i
            meta_group.attrs["violations"] = json.dumps(violations)
            print("Final metadata saved.")

    finally:
        # Stop acquisition
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()

        # Reset original camera settings
        print("\nResetting camera to original settings:")
        cam.set_roi(*original_roi)
        print_roi(cam)

        cam.set_attribute_value("Trigger Determination", original_trigger_det)
        print(f"[RESET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

        cam.set_attribute_value("Trigger Response", original_trigger_resp)
        print(f"[RESET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

        cam.set_attribute_value("Shutter Closing Delay", original_shutter_delay)
        print(f"[RESET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

        cam.set_attribute_value("Shutter Delay Resolution", original_delay_res)
        print(f"[RESET] Shutter Delay Resolution: {cam.get_attribute_value('Shutter Delay Resolution')}")

        cam.set_attribute_value("Shutter Timing Mode", original_shutter_mode)
        print(f"[RESET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

        cam.close()
        print("Camera connection closed.")

        gc.collect()

        print(f"Acquisition complete. Images acquired: {i}, Number of Violations: {len(violations)}")
        print(rf"Violations at: {violations}")


# PRINT ROI HELPER ############################################################
def print_roi(cam) -> None:
    roi = cam.get_roi()
    print("ROI settings:")
    print(f"  Horizontal start:   {roi[0]}")
    print(f"  Horizontal end:     {roi[1]}")
    print(f"  Vertical start:     {roi[2]}")
    print(f"  Vertical end:       {roi[3]}")
    print(f"  Horizontal binning: {roi[4]}")
    print(f"  Vertical binning:   {roi[5]}")


# PYNPUT CALLBACK #############################################################
def on_press(key):
    global stop_loop
    try:
        if key == keyboard.Key.esc:
            print("Detected 'Esc' key, breaking the loop.")
            stop_loop = True
            return False  # Stop the listener
    except AttributeError:
        pass


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()
blob
mark :40
data 11085
import numpy as np
import os
import time
import json
from datetime import datetime
from pynput import keyboard
from pylablib.devices import PrincetonInstruments
import gc
import h5py

# Global stop flag
stop_loop = False

# PATHS ########################################################################
class Paths:
    BASE_DIR = r"Z:\Attoline"
#Empty statement to indicate this version also works with v2

# DIRECTORY AND FILE MANAGEMENT ###############################################
def create_hdf5_filepath():
    """
    Creates the directory structure: base_dir/YYYY/STRA_new/YYMMDD/
    and returns the full path for a new HDF5 file: YYMMDD_XXX.hdf5.
    Automatically increments XXX if the file already exists.
    """
    now = datetime.now()
    year = now.strftime("%Y")
    date_str = now.strftime("%y%m%d")
    
    # Create the base directory structure
    year_dir = os.path.join(Paths.BASE_DIR, year)
    stra_dir = os.path.join(year_dir, "STRA_new")
    date_dir = os.path.join(stra_dir, date_str)
    sequence_num = 1
    # Find the next available sequence number
    while True:
        sequence_str = f"{date_str}_{sequence_num:03d}"
        sequence_dir = os.path.join(date_dir, sequence_str)
        hdf5_filename = f"{sequence_str}.hdf5"
        final_path = os.path.join(sequence_dir, hdf5_filename)

        if not os.path.exists(final_path):
            # Ensure the directory for this sequence exists
            os.makedirs(sequence_dir, exist_ok=True)
            break

        sequence_num += 1

        # Safety check to prevent infinite loop
        if sequence_num > 999:
            raise ValueError("Too many acquisitions for this date (>999)")

    return final_path

def save_dict_to_hdf5_attrs(group, metadata_dict):
    """
    Saves dictionary items as attributes to an HDF5 group.
    Complex types like dicts or lists are serialized to JSON strings.
    """
    for key, value in metadata_dict.items():
        if isinstance(value, (dict, list, tuple)):
            group.attrs[key] = json.dumps(value)
        else:
            group.attrs[key] = value

# SETTINGS #####################################################################
class Settings:
    EXP_TIME_MS = 20
    BINNING = (1, 400)
    SPECTRA_SHAPE = (1, 1340)
    NUMBER_OF_IMAGES = int(300e3)
    ACQUISITION_TIME_VIOLATION_THRESHOLD_MS = EXP_TIME_MS * 1.8


# MAIN FUNCTION ################################################################
def main():
    global stop_loop

    # Start the keyboard listener
    listener = keyboard.Listener(on_press=on_press)
    listener.start()


    # Create directory structure and get HDF5 file path
    hdf5_path = create_hdf5_filepath()
    print(f"Data will be saved to: {hdf5_path}")
    # Ensure parent directory exists before opening file
    os.makedirs(os.path.dirname(hdf5_path), exist_ok=True)

    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    print(f"Timestamp: {timestamp}")

    # check if camera is connected:
    print("Connected devices:")
    print(PrincetonInstruments.list_cameras())  # Should list the connected camera

    cam = PrincetonInstruments.PicamCamera('2105050003')
    print("Camera connected.")

    # Save camera attributes BEFORE acquisition
    all_attrs_at_start = cam.get_all_attribute_values()

    # Backup original attributes
    original_roi = cam.get_roi()
    original_trigger_det = cam.get_attribute_value("Trigger Determination")
    original_trigger_resp = cam.get_attribute_value("Trigger Response")
    original_shutter_delay = cam.get_attribute_value("Shutter Closing Delay")
    original_delay_res = cam.get_attribute_value("Shutter Delay Resolution")
    original_shutter_mode = cam.get_attribute_value("Shutter Timing Mode")

    # Setup camera for acquisition
    cam.set_attribute_value("Exposure Time", Settings.EXP_TIME_MS)
    print(f"[SET] Exposure Time: {cam.get_attribute_value('Exposure Time')} ms")

    cam.set_roi(hbin=Settings.BINNING[0], vbin=Settings.BINNING[1])
    print_roi(cam)

    cam.set_attribute_value("Trigger Determination", "Positive Polarity")
    print(f"[SET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

    cam.set_attribute_value("Trigger Response", "Readout Per Trigger")
    print(f"[SET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

    cam.set_attribute_value("Clean Until Trigger", False)
    print(f"[SET] Clean Until Trigger: {cam.get_attribute_value('Clean Until Trigger')}")

    cam.set_attribute_value("Shutter Timing Mode", "Always Open")
    print(f"[SET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

    cam.set_attribute_value("Shutter Closing Delay", 0)
    print(f"[SET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

    time.sleep(0.2)

    # Save attributes after setup
    all_attrs_measurement = cam.get_all_attribute_values()

    # Set up acquisition
    cam.setup_acquisition(mode="sequence", nframes=Settings.NUMBER_OF_IMAGES)

    # Structured dtype for HDF5 dataset
    dtype = np.dtype([
        ("spectrum", np.uint16, Settings.SPECTRA_SHAPE[1]),
        ("timestamp_us", np.uint64)
    ])

    # Metadata dictionary
    start_time_unix = time.time()
    metadata = {
        "timestamp": timestamp,
        "exp_time_ms": Settings.EXP_TIME_MS,
        "binning": list(Settings.BINNING),
        "spectra_shape": list(Settings.SPECTRA_SHAPE),
        "number_of_images_planned": Settings.NUMBER_OF_IMAGES,
        "start_time_unix": start_time_unix,
        "dtype_spectrum": "uint16",
        "dtype_timestamp_us": "uint64",
        "camera_attributes_at_start": [list(item) for item in all_attrs_at_start.items()],
        "camera_attributes_measurement": [list(item) for item in all_attrs_measurement.items()]
    }

    i = 0
    violations = []

    # Use a try...finally block to ensure resources are closed properly
    try:
        with h5py.File(hdf5_path, 'w') as f:
            # Create a resizable dataset for the spectra and timestamps
            dset = f.create_dataset(
                "data",
                shape=(0,),
                maxshape=(Settings.NUMBER_OF_IMAGES,),
                dtype=dtype,
                chunks=(100,)  # Chunking is important for performance
            )
            
            # Create a group for metadata and save it as attributes
            meta_group = f.create_group("metadata")
            save_dict_to_hdf5_attrs(meta_group, metadata)
            #f.swmr_mode = True
            #print("Writer entered SWMR mode. Acquisition starting.")
            # Start acquisition
            print("Starting acquisition... (Press 'Esc' to stop early)")
            cam.start_acquisition()
            t_prev = time.time()
            
            # Run main acquisition loop
            while i < Settings.NUMBER_OF_IMAGES:
                data = cam.read_oldest_image()

                if data is None:
                    time.sleep(Settings.EXP_TIME_MS / 1000 / 20)
                    continue

                t_now = time.time()
                timestamp_us = int(t_now * 1e6)
                max_val = data.max()

                # Resize dataset and write new data
                dset.resize((i + 1,))
                dset[i] = (data.ravel().astype(np.uint16), timestamp_us)
                
                # Periodically flush data to disk
                if i % 100 == 0:
                    f.flush()
                    print(f"Image {i} flushed.")

                dt = t_now - t_prev
                if i % 40 == 0:
                    try:
                        print(rf"Image no {i} acquired in {dt:.4f} s, max value: {max_val}")
                    except Exception as e:
                        print(f"Error occurred while logging image {i}: {e}")

                if dt > Settings.ACQUISITION_TIME_VIOLATION_THRESHOLD_MS / 1000:
                    print(f"ACQUISITION TIME VIOLATION at {i}: {dt:.4f}s --------------------------------------")
                    violations.append(i)

                t_prev = t_now

                if stop_loop:
                    print("User interrupted acquisition with 'Esc'.")
                    break
                
                i += 1
            
            # Final metadata update
            meta_group.attrs["images_acquired"] = i
            meta_group.attrs["violations"] = json.dumps(violations)
            print("Final metadata saved.")

    finally:
        # Stop acquisition
        if cam.acquisition_in_progress():
            cam.stop_acquisition()
        cam.clear_acquisition()

        # Reset original camera settings
        print("\nResetting camera to original settings:")
        cam.set_roi(*original_roi)
        print_roi(cam)

        cam.set_attribute_value("Trigger Determination", original_trigger_det)
        print(f"[RESET] Trigger Determination: {cam.get_attribute_value('Trigger Determination')}")

        cam.set_attribute_value("Trigger Response", original_trigger_resp)
        print(f"[RESET] Trigger Response: {cam.get_attribute_value('Trigger Response')}")

        cam.set_attribute_value("Shutter Closing Delay", original_shutter_delay)
        print(f"[RESET] Shutter Closing Delay: {cam.get_attribute_value('Shutter Closing Delay')}")

        cam.set_attribute_value("Shutter Delay Resolution", original_delay_res)
        print(f"[RESET] Shutter Delay Resolution: {cam.get_attribute_value('Shutter Delay Resolution')}")

        cam.set_attribute_value("Shutter Timing Mode", original_shutter_mode)
        print(f"[RESET] Shutter Timing Mode: {cam.get_attribute_value('Shutter Timing Mode')}")

        cam.close()
        print("Camera connection closed.")

        gc.collect()

        print(f"Acquisition complete. Images acquired: {i}, Number of Violations: {len(violations)}")
        print(rf"Violations at: {violations}")


# PRINT ROI HELPER ############################################################
def print_roi(cam) -> None:
    roi = cam.get_roi()
    print("ROI settings:")
    print(f"  Horizontal start:   {roi[0]}")
    print(f"  Horizontal end:     {roi[1]}")
    print(f"  Vertical start:     {roi[2]}")
    print(f"  Vertical end:       {roi[3]}")
    print(f"  Horizontal binning: {roi[4]}")
    print(f"  Vertical binning:   {roi[5]}")


# PYNPUT CALLBACK #############################################################
def on_press(key):
    global stop_loop
    try:
        if key == keyboard.Key.esc:
            print("Detected 'Esc' key, breaking the loop.")
            stop_loop = True
            return False  # Stop the listener
    except AttributeError:
        pass


# RUN SCRIPT ##################################################################
if __name__ == "__main__":
    main()
commit refs/heads/trunk
mark :41
committer Moritz <Moritz> 1756719598 +0000
data 22
initial empty check-in
deleteall

commit refs/heads/trunk
mark :42
committer Moritz <Moritz> 1756719936 +0000
data 33
First commit of pixis_code folder
from :41
M 100644 :5 Notes.md
M 100644 :7 README.md
M 100644 :11 TODO.md
M 100644 :1 main.py
M 100644 :2 main_backup.py
M 100644 :3 main_teledyne.py
M 100644 :4 memmap_clear_zeros.py
M 100644 :6 plot_single_acqusition.py
M 100644 :8 saving_performance_test.py
M 100644 :9 simple_acq_working.py
M 100644 :10 spectrum_live.py
M 100644 :12 visualize_data.py
M 100644 :13 with_shutter_working.py

commit refs/heads/trunk
mark :43
committer Moritz <Moritz> 1756720592 +0000
data 36
changed energy_file in spectrum_live
from :42
M 100644 :14 spectrum_live.py

commit refs/heads/trunk
mark :44
committer Moritz <Moritz> 1756812239 +0000
data 14
added spec.txt
from :43
M 100644 :15 Spec.txt

commit refs/heads/trunk
mark :45
committer Moritz <Moritz> 1756812461 +0000
data 51
Spec.txt is found as long as it is in the same path
from :44
M 100644 :16 spectrum_live.py

commit refs/heads/trunk
mark :46
committer Moritz <Moritz> 1756814977 +0000
data 84
Changes plotting to blit which is faster but some issues with showing multiple lines
from :45
M 100644 :17 marceltest.py

commit refs/heads/trunk
mark :47
committer Moritz <Moritz> 1756815283 +0000
data 38
Faster plotting and only single lines.
from :46
M 100644 :18 spectrum_live.py

commit refs/heads/trunk
mark :48
committer Moritz <Moritz> 1756818886 +0000
data 40
Changed colours, initial limits and xlim
from :47
M 100644 :19 spectrum_live.py

commit refs/heads/trunk
mark :49
committer Moritz <Moritz> 1756822277 +0000
data 94
Three plots now, std max and counts, horizontal line at saturation. Std limits not correct yet
from :48
M 100644 :20 spectrum_live.py

commit refs/heads/trunk
mark :50
committer Moritz <Moritz> 1756824002 +0000
data 38
gauge plot for the std over 2 seconds.
from :49
M 100644 :21 spectrum_live.py

commit refs/heads/trunk
mark :51
committer Moritz <Moritz> 1756829466 +0000
data 79
changed the std calculation to be normalised for each energy before averaging. 
from :50
M 100644 :22 spectrum_live.py

commit refs/heads/trunk
mark :52
committer Moritz <Moritz> 1756831111 +0000
data 30
Added GUI button for std range
from :51
M 100644 :23 spectrum_live.py

commit refs/heads/trunk
mark :53
committer Moritz <Moritz> 1756897029 +0000
data 54
Changed the textbox size and alignment for readability
from :52
M 100644 :24 spectrum_live.py

commit refs/heads/trunk
mark :54
committer Moritz <Moritz> 1756897717 +0000
data 28
Added comments for clarity. 
from :53
M 100644 :25 spectrum_live.py

commit refs/heads/trunk
mark :55
committer Moritz <Moritz> 1756905336 +0000
data 61
added two (for now commented) lines that show the tempeature 
from :54
M 100644 :26 spectrum_live.py

commit refs/heads/trunk
mark :56
committer Moritz <Moritz> 1756909996 +0000
data 85
Updated the path settings, changed atas_prep to use real data with new paths to test.
from :55
M 100644 :28 atas_preperation.py
M 100644 :27 main.py
M 100644 :29 misc_functions.py

commit refs/heads/trunk
mark :57
committer Moritz <Moritz> 1756974693 +0000
data 131
verified raspberry code and atas code by taking real data and converting it to new format (variables not saved in filename anymore)
from :56
M 100644 :30 atas_preperation.py
M 100644 :31 misc_functions.py

commit refs/heads/trunk
mark :58
committer Moritz <Moritz> 1756978380 +0000
data 38
removed artifact memmap hardcoded line
from :57
M 100644 :32 atas_preperation.py

commit refs/heads/trunk
mark :59
committer Moritz <Moritz> 1756985444 +0000
data 39
Added save option and nicer colours lol
from :58
M 100644 :33 spectrum_live.py

commit refs/heads/trunk
mark :60
committer Moritz <Moritz> 1757006253 +0000
data 18
h5 but doesnt work
from :59
M 100644 :34 main.py

commit refs/heads/trunk
mark :61
committer Moritz <Moritz> 1757007231 +0000
data 7
cooked 
from :60
M 100644 :35 main.py

commit refs/heads/trunk
mark :62
committer Moritz <Moritz> 1757079244 +0000
data 99
v1 verified working together with moritz. Basically the original code with better folder structure.
from :61
M 100644 :36 main.py

commit refs/heads/trunk
mark :63
committer Moritz <Moritz> 1757084410 +0000
data 100
v2 Raspberry now saves as h5 (hdf5). Does not require change in this script, hence the comment line.
from :62
M 100644 :37 main.py

commit refs/heads/trunk
mark :64
committer Moritz <Moritz> 1757087120 +0000
data 46
v3 Saves as hdf5, getting rid of json and npy.
from :63
M 100644 :38 main.py

commit refs/heads/trunk
mark :65
committer Moritz <Moritz> 1757087575 +0000
data 58
v3.1 Updated such that the hdf5 have each their own folder
from :64
M 100644 :39 main.py

commit refs/heads/trunk
mark :66
committer Moritz <Moritz> 1757094982 +0000
data 100
v3.2 removed 0 entries from timestamps. Print less images. Try to allow read/write at the same time.
from :65
M 100644 :40 main.py

